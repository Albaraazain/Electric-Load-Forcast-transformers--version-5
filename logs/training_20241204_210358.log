2024-12-04 21:03:58,418 - __main__ - INFO - CUDA Environment:
2024-12-04 21:03:58,419 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 21:03:58,419 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 21:03:58,998 - __main__ - INFO - Starting training...
2024-12-04 21:03:58,998 - training.trainer - INFO - 
================================================================================
2024-12-04 21:03:58,998 - training.trainer - INFO - Starting Training
2024-12-04 21:03:58,998 - training.trainer - INFO - ================================================================================
2024-12-04 21:04:08,383 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:04:08,383 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:04:08,398 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:04:08,398 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:04:08,807 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:04:08,807 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:04:12,397 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.3063 | Learning Rate: 0.000002
================================================================================
2024-12-04 21:04:15,772 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.1243 | Learning Rate: 0.000003
================================================================================
2024-12-04 21:04:19,674 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0645 | Learning Rate: 0.000005
================================================================================
2024-12-04 21:04:23,497 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0320 | Learning Rate: 0.000006
================================================================================
2024-12-04 21:04:27,648 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.1215 | Learning Rate: 0.000008
================================================================================
2024-12-04 21:04:32,436 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0427 | Learning Rate: 0.000010
================================================================================
2024-12-04 21:04:35,769 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9894 | Learning Rate: 0.000011
================================================================================
2024-12-04 21:04:38,907 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9557 | Learning Rate: 0.000013
================================================================================
2024-12-04 21:04:42,122 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0272 | Learning Rate: 0.000014
================================================================================
2024-12-04 21:04:45,266 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0239 | Learning Rate: 0.000016
================================================================================
2024-12-04 21:04:48,419 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9471 | Learning Rate: 0.000017
================================================================================
2024-12-04 21:04:51,586 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9813 | Learning Rate: 0.000019
================================================================================
2024-12-04 21:04:54,885 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0392 | Learning Rate: 0.000021
================================================================================
2024-12-04 21:04:58,184 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 1.0067 | Learning Rate: 0.000022
================================================================================
2024-12-04 21:05:01,438 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9723 | Learning Rate: 0.000024
================================================================================
2024-12-04 21:05:06,373 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9906 | Learning Rate: 0.000025
================================================================================
2024-12-04 21:05:11,389 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 1.0332 | Learning Rate: 0.000027
================================================================================
2024-12-04 21:05:15,599 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9412 | Learning Rate: 0.000029
================================================================================
2024-12-04 21:05:19,096 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9648 | Learning Rate: 0.000030
================================================================================
2024-12-04 21:05:23,932 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9473 | Learning Rate: 0.000032
================================================================================
2024-12-04 21:05:32,818 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9760 | Learning Rate: 0.000033
================================================================================
2024-12-04 21:06:09,741 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0232
Val Loss: 0.8893
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 21:06:09,741 - training.trainer - INFO - New best model saved! (Val Loss: 0.8893)
2024-12-04 21:06:09,796 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 21:06:22,990 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:06:22,992 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:06:22,993 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:06:22,993 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:06:23,178 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:06:23,178 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:06:29,658 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.0593 | Learning Rate: 0.000035
================================================================================
2024-12-04 21:06:37,165 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9530 | Learning Rate: 0.000037
================================================================================
2024-12-04 21:06:43,465 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9493 | Learning Rate: 0.000038
================================================================================
2024-12-04 21:06:49,344 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9275 | Learning Rate: 0.000040
================================================================================
2024-12-04 21:06:53,943 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9648 | Learning Rate: 0.000041
================================================================================
2024-12-04 21:06:58,664 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9160 | Learning Rate: 0.000043
================================================================================
2024-12-04 21:07:03,302 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9828 | Learning Rate: 0.000044
================================================================================
2024-12-04 21:07:08,543 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9821 | Learning Rate: 0.000046
================================================================================
2024-12-04 21:07:13,174 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9824 | Learning Rate: 0.000048
================================================================================
2024-12-04 21:07:17,676 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0033 | Learning Rate: 0.000049
================================================================================
2024-12-04 21:07:22,068 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9367 | Learning Rate: 0.000051
================================================================================
2024-12-04 21:07:26,504 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.0614 | Learning Rate: 0.000052
================================================================================
2024-12-04 21:07:30,989 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9746 | Learning Rate: 0.000054
================================================================================
2024-12-04 21:07:35,497 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9902 | Learning Rate: 0.000056
================================================================================
2024-12-04 21:07:39,922 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0079 | Learning Rate: 0.000057
================================================================================
2024-12-04 21:07:44,412 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9280 | Learning Rate: 0.000059
================================================================================
2024-12-04 21:07:48,998 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9293 | Learning Rate: 0.000060
================================================================================
2024-12-04 21:07:54,310 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8881 | Learning Rate: 0.000062
================================================================================
2024-12-04 21:07:59,429 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9264 | Learning Rate: 0.000063
================================================================================
2024-12-04 21:08:04,521 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8737 | Learning Rate: 0.000065
================================================================================
2024-12-04 21:08:07,859 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9480 | Learning Rate: 0.000067
================================================================================
2024-12-04 21:08:49,111 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9612
Val Loss: 0.8790
Best Val Loss: 0.8893
Learning Rate: 0.000067
================================================================================
2024-12-04 21:08:49,111 - training.trainer - INFO - New best model saved! (Val Loss: 0.8790)
2024-12-04 21:08:49,198 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 21:09:02,260 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:09:02,260 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:09:02,260 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:09:02,261 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:09:02,443 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:09:02,443 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:09:05,824 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9938 | Learning Rate: 0.000068
================================================================================
2024-12-04 21:09:09,137 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9622 | Learning Rate: 0.000070
================================================================================
2024-12-04 21:09:12,670 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9526 | Learning Rate: 0.000071
================================================================================
2024-12-04 21:09:16,126 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9357 | Learning Rate: 0.000073
================================================================================
2024-12-04 21:09:19,456 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9763 | Learning Rate: 0.000075
================================================================================
2024-12-04 21:09:22,739 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9251 | Learning Rate: 0.000076
================================================================================
2024-12-04 21:09:26,403 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9477 | Learning Rate: 0.000078
================================================================================
2024-12-04 21:09:30,058 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8898 | Learning Rate: 0.000079
================================================================================
2024-12-04 21:09:34,768 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9468 | Learning Rate: 0.000081
================================================================================
2024-12-04 21:09:39,847 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9121 | Learning Rate: 0.000083
================================================================================
2024-12-04 21:09:46,198 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 1.0098 | Learning Rate: 0.000084
================================================================================
2024-12-04 21:09:52,410 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9355 | Learning Rate: 0.000086
================================================================================
2024-12-04 21:09:58,138 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9612 | Learning Rate: 0.000087
================================================================================
2024-12-04 21:10:03,009 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8937 | Learning Rate: 0.000089
================================================================================
2024-12-04 21:10:07,943 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9448 | Learning Rate: 0.000090
================================================================================
2024-12-04 21:10:12,930 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9119 | Learning Rate: 0.000092
================================================================================
2024-12-04 21:10:17,810 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9064 | Learning Rate: 0.000094
================================================================================
2024-12-04 21:10:22,371 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9611 | Learning Rate: 0.000095
================================================================================
2024-12-04 21:10:27,100 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9237 | Learning Rate: 0.000097
================================================================================
2024-12-04 21:10:31,872 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9210 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:10:37,006 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9801 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:10:58,763 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9424
Val Loss: 0.8835
Best Val Loss: 0.8790
Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:08,282 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:11:08,282 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:11:08,283 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:11:08,283 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:11:08,461 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:11:08,461 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:11:11,920 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7897 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:15,310 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.0129 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:19,754 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9459 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:23,332 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9754 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:26,563 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9191 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:29,821 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9234 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:33,048 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9307 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:36,292 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9586 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:39,579 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0051 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:42,982 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9075 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:46,473 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9580 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:49,883 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8986 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:53,120 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8998 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:56,372 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9176 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:11:59,670 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8591 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:02,905 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8752 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:06,129 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9994 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:09,417 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9303 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:12,793 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9676 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:16,242 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8739 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:20,444 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8734 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:44,680 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9248
Val Loss: 0.8955
Best Val Loss: 0.8790
Learning Rate: 0.000100
================================================================================
2024-12-04 21:12:54,243 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:12:54,244 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:12:54,244 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:12:54,244 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:12:54,421 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:12:54,421 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:12:58,009 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9045 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:01,466 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9524 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:05,036 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8941 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:08,474 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9481 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:11,714 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8378 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:14,887 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9066 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:18,206 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8917 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:22,104 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9051 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:25,791 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8282 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:29,737 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9237 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:34,400 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8956 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:39,183 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9480 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:44,068 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9498 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:48,806 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8801 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:54,067 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9834 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:13:59,810 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8590 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:05,236 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9289 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:09,295 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8706 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:13,223 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 1.0062 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:16,460 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8729 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:19,889 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8788 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:40,828 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.9079
Val Loss: 0.9345
Best Val Loss: 0.8790
Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:50,103 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:14:50,106 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:14:50,106 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:14:50,106 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:14:50,290 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:14:50,290 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:14:53,828 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8689 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:14:58,444 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9522 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:02,732 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9186 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:06,238 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8162 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:09,556 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9214 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:13,271 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9691 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:16,511 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8500 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:19,724 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9293 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:22,973 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9120 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:26,096 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8583 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:29,437 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8953 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:32,836 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8314 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:36,258 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8923 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:39,493 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8322 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:43,576 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8717 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:47,170 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8377 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:50,503 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9322 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:53,849 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9068 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:15:57,454 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8830 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:01,340 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9307 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:05,140 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8710 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:28,053 - training.trainer - INFO - 
============================== Epoch 5 Summary ==============================
Train Loss: 0.8895
Val Loss: 0.8982
Best Val Loss: 0.8790
Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:37,454 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:16:37,454 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:16:37,470 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:16:37,470 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:16:37,645 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:16:37,645 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:16:40,803 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8785 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:44,116 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8804 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:47,871 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9064 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:52,105 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8638 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:56,258 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8118 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:16:59,843 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8140 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:03,366 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8626 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:06,846 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8599 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:10,340 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8961 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:13,647 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9299 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:17,218 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8566 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:20,632 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8441 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:24,287 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8792 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:27,757 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9148 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:31,118 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8544 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:34,877 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8878 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:38,257 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9447 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:41,652 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8679 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:45,165 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8275 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:48,675 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8472 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:17:51,965 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8580 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:12,260 - training.trainer - INFO - 
============================== Epoch 6 Summary ==============================
Train Loss: 0.8707
Val Loss: 0.9253
Best Val Loss: 0.8790
Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:22,322 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:18:22,322 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:18:22,323 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:18:22,323 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:18:22,503 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:18:22,503 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:18:25,829 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8548 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:29,504 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8576 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:33,447 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8867 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:37,529 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9083 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:41,004 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8023 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:44,228 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8516 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:47,473 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8141 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:50,683 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8603 | Learning Rate: 0.000100
================================================================================
2024-12-04 21:18:53,894 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8413 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:18:57,435 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8413 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:00,759 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8615 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:04,269 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9354 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:07,538 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8247 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:11,224 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8382 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:15,171 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8466 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:18,501 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8392 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:22,648 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8090 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:27,147 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8603 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:31,347 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8884 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:34,718 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8141 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:37,885 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8358 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:19:58,156 - training.trainer - INFO - 
============================== Epoch 7 Summary ==============================
Train Loss: 0.8510
Val Loss: 0.9258
Best Val Loss: 0.8790
Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:07,889 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:20:07,892 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:20:07,892 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:20:07,892 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:20:08,069 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:20:08,069 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:20:11,717 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9148 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:15,823 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8989 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:19,297 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8946 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:22,813 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8317 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:26,309 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8300 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:29,580 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7906 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:33,269 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7932 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:36,683 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8580 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:40,149 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8553 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:43,686 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8265 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:47,369 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7434 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:50,863 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8699 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:54,838 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7180 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:20:58,795 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9145 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:02,133 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8830 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:05,669 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7757 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:08,964 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8613 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:12,308 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8015 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:16,029 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8169 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:20,541 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8237 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:25,284 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7968 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:50,450 - training.trainer - INFO - 
============================== Epoch 8 Summary ==============================
Train Loss: 0.8333
Val Loss: 0.9655
Best Val Loss: 0.8790
Learning Rate: 0.000099
================================================================================
2024-12-04 21:21:59,738 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:21:59,738 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:21:59,738 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:21:59,738 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:21:59,919 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:21:59,919 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:22:03,518 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8578 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:07,010 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.7924 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:10,248 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8318 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:17,425 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8483 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:21,039 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7413 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:24,299 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7801 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:27,769 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7896 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:31,630 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8536 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:34,952 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8836 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:38,980 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7841 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:42,944 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8335 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:46,680 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8353 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:50,736 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8074 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:54,406 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8861 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:22:58,078 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7793 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:01,462 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7887 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:04,860 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8729 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:08,036 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8048 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:11,237 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7671 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:14,435 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7813 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:17,646 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7956 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:37,658 - training.trainer - INFO - 
============================== Epoch 9 Summary ==============================
Train Loss: 0.8150
Val Loss: 0.9364
Best Val Loss: 0.8790
Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:47,210 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:23:47,219 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:23:47,219 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:23:47,219 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:23:47,395 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:23:47,395 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:23:50,875 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8386 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:54,154 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.7879 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:23:57,273 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8421 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:00,438 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7574 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:03,639 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7941 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:06,799 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7824 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:10,019 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8173 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:13,257 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7496 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:16,686 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8093 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:19,873 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8333 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:23,487 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8078 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:27,292 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8366 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:31,265 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.6882 | Learning Rate: 0.000099
================================================================================
2024-12-04 21:24:35,036 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8228 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:24:38,772 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8002 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:24:42,510 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7466 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:24:46,223 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7977 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:24:50,066 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8013 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:24:54,607 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7830 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:24:59,240 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7850 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:03,339 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8428 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:23,836 - training.trainer - INFO - 
============================== Epoch 10 Summary ==============================
Train Loss: 0.7964
Val Loss: 0.9842
Best Val Loss: 0.8790
Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:33,356 - training.trainer - INFO - 
Batch shapes:
2024-12-04 21:25:33,356 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 21:25:33,356 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 21:25:33,356 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 21:25:33,532 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 21:25:33,532 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 21:25:36,807 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7618 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:39,999 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8114 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:43,270 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8656 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:46,409 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7437 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:49,792 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7350 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:53,084 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7994 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:56,336 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7541 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:25:59,664 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8206 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:02,829 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7792 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:06,015 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7469 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:09,508 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7467 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:12,758 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7854 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:16,217 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7987 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:19,524 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7880 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:23,046 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7692 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:26,307 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8046 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:29,693 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8297 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:35,251 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7327 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:39,685 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7423 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:43,298 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8254 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:26:47,121 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7725 | Learning Rate: 0.000098
================================================================================
2024-12-04 21:27:08,272 - training.trainer - INFO - 
============================== Epoch 11 Summary ==============================
Train Loss: 0.7816
Val Loss: 1.0103
Best Val Loss: 0.8790
Learning Rate: 0.000098
================================================================================
2024-12-04 21:27:08,313 - training.trainer - INFO - 
============================== Early Stopping ==============================
No improvement for 10 epochs
Best Val Loss: 0.8790
================================================================================
2024-12-04 21:27:08,314 - training.trainer - INFO - 
================================================================================
2024-12-04 21:27:08,315 - training.trainer - INFO - Training Completed!
2024-12-04 21:27:08,315 - training.trainer - INFO - ================================================================================
2024-12-04 21:27:08,315 - __main__ - INFO - Evaluating model...
