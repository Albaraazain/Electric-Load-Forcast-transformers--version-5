2024-12-04 22:24:34,582 - __main__ - INFO - CUDA Environment:
2024-12-04 22:24:34,583 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 22:24:34,583 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 22:24:35,119 - __main__ - INFO - Starting training...
2024-12-04 22:24:35,119 - training.trainer - INFO - 
================================================================================
2024-12-04 22:24:35,119 - training.trainer - INFO - Starting Training
2024-12-04 22:24:35,119 - training.trainer - INFO - ================================================================================
2024-12-04 22:24:44,766 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:24:44,766 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:24:44,767 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:24:44,767 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:24:45,048 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:24:45,048 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:24:49,319 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.3266 | Learning Rate: 0.000002
================================================================================
2024-12-04 22:24:53,990 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.1171 | Learning Rate: 0.000003
================================================================================
2024-12-04 22:24:58,567 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.1555 | Learning Rate: 0.000005
================================================================================
2024-12-04 22:25:02,236 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0768 | Learning Rate: 0.000006
================================================================================
2024-12-04 22:25:06,126 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.0258 | Learning Rate: 0.000008
================================================================================
2024-12-04 22:25:10,034 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9979 | Learning Rate: 0.000010
================================================================================
2024-12-04 22:25:13,923 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0392 | Learning Rate: 0.000011
================================================================================
2024-12-04 22:25:17,633 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0276 | Learning Rate: 0.000013
================================================================================
2024-12-04 22:25:21,386 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0877 | Learning Rate: 0.000014
================================================================================
2024-12-04 22:25:24,768 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0251 | Learning Rate: 0.000016
================================================================================
2024-12-04 22:25:28,246 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9884 | Learning Rate: 0.000017
================================================================================
2024-12-04 22:25:31,635 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.0570 | Learning Rate: 0.000019
================================================================================
2024-12-04 22:25:35,195 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0074 | Learning Rate: 0.000021
================================================================================
2024-12-04 22:25:38,585 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9886 | Learning Rate: 0.000022
================================================================================
2024-12-04 22:25:42,194 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9089 | Learning Rate: 0.000024
================================================================================
2024-12-04 22:25:45,507 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9050 | Learning Rate: 0.000025
================================================================================
2024-12-04 22:25:48,897 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9442 | Learning Rate: 0.000027
================================================================================
2024-12-04 22:25:52,434 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 1.0174 | Learning Rate: 0.000029
================================================================================
2024-12-04 22:25:55,837 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 1.0014 | Learning Rate: 0.000030
================================================================================
2024-12-04 22:25:59,392 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8770 | Learning Rate: 0.000032
================================================================================
2024-12-04 22:26:03,134 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9323 | Learning Rate: 0.000033
================================================================================
2024-12-04 22:26:22,735 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0241
Val Loss: 0.8864
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 22:26:22,735 - training.trainer - INFO - New best model saved! (Val Loss: 0.8864)
2024-12-04 22:26:22,804 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 22:26:33,039 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:26:33,039 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:26:33,039 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:26:33,039 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:26:33,228 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:26:33,228 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:26:36,492 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9705 | Learning Rate: 0.000035
================================================================================
2024-12-04 22:26:39,764 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9330 | Learning Rate: 0.000037
================================================================================
2024-12-04 22:26:42,950 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9518 | Learning Rate: 0.000038
================================================================================
2024-12-04 22:26:46,212 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9523 | Learning Rate: 0.000040
================================================================================
2024-12-04 22:26:49,704 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9556 | Learning Rate: 0.000041
================================================================================
2024-12-04 22:26:53,268 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9589 | Learning Rate: 0.000043
================================================================================
2024-12-04 22:26:56,483 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9481 | Learning Rate: 0.000044
================================================================================
2024-12-04 22:26:59,611 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9319 | Learning Rate: 0.000046
================================================================================
2024-12-04 22:27:02,959 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9738 | Learning Rate: 0.000048
================================================================================
2024-12-04 22:27:06,078 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9944 | Learning Rate: 0.000049
================================================================================
2024-12-04 22:27:09,150 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8734 | Learning Rate: 0.000051
================================================================================
2024-12-04 22:27:12,274 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9120 | Learning Rate: 0.000052
================================================================================
2024-12-04 22:27:15,458 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0652 | Learning Rate: 0.000054
================================================================================
2024-12-04 22:27:18,503 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9994 | Learning Rate: 0.000056
================================================================================
2024-12-04 22:27:21,929 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9011 | Learning Rate: 0.000057
================================================================================
2024-12-04 22:27:24,982 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9684 | Learning Rate: 0.000059
================================================================================
2024-12-04 22:27:28,146 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9644 | Learning Rate: 0.000060
================================================================================
2024-12-04 22:27:31,212 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9360 | Learning Rate: 0.000062
================================================================================
2024-12-04 22:27:34,349 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9641 | Learning Rate: 0.000063
================================================================================
2024-12-04 22:27:37,828 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9946 | Learning Rate: 0.000065
================================================================================
2024-12-04 22:27:41,911 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 1.0134 | Learning Rate: 0.000067
================================================================================
2024-12-04 22:28:02,419 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9601
Val Loss: 0.8794
Best Val Loss: 0.8864
Learning Rate: 0.000067
================================================================================
2024-12-04 22:28:02,420 - training.trainer - INFO - New best model saved! (Val Loss: 0.8794)
2024-12-04 22:28:02,517 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 22:28:12,425 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:28:12,425 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:28:12,425 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:28:12,425 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:28:12,621 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:28:12,622 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:28:16,537 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9397 | Learning Rate: 0.000068
================================================================================
2024-12-04 22:28:19,754 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9774 | Learning Rate: 0.000070
================================================================================
2024-12-04 22:28:22,923 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9455 | Learning Rate: 0.000071
================================================================================
2024-12-04 22:28:26,048 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0154 | Learning Rate: 0.000073
================================================================================
2024-12-04 22:28:29,237 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9698 | Learning Rate: 0.000075
================================================================================
2024-12-04 22:28:32,443 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8950 | Learning Rate: 0.000076
================================================================================
2024-12-04 22:28:36,264 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0165 | Learning Rate: 0.000078
================================================================================
2024-12-04 22:28:41,011 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0066 | Learning Rate: 0.000079
================================================================================
2024-12-04 22:28:45,070 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9175 | Learning Rate: 0.000081
================================================================================
2024-12-04 22:28:49,205 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9856 | Learning Rate: 0.000083
================================================================================
2024-12-04 22:28:53,131 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8708 | Learning Rate: 0.000084
================================================================================
2024-12-04 22:28:57,571 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8507 | Learning Rate: 0.000086
================================================================================
2024-12-04 22:29:01,828 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9110 | Learning Rate: 0.000087
================================================================================
2024-12-04 22:29:06,166 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9997 | Learning Rate: 0.000089
================================================================================
2024-12-04 22:29:10,810 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9449 | Learning Rate: 0.000090
================================================================================
2024-12-04 22:29:15,388 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8646 | Learning Rate: 0.000092
================================================================================
2024-12-04 22:29:22,086 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9675 | Learning Rate: 0.000094
================================================================================
2024-12-04 22:29:29,455 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8620 | Learning Rate: 0.000095
================================================================================
2024-12-04 22:29:37,003 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9312 | Learning Rate: 0.000097
================================================================================
2024-12-04 22:29:41,519 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9756 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:29:45,592 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9059 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:09,744 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9406
Val Loss: 0.9108
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:23,224 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:30:23,227 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:30:23,227 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:30:23,228 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:30:23,425 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:30:23,425 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:30:28,015 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9287 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:32,223 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9237 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:36,382 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0363 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:40,804 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9932 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:44,866 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9412 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:49,095 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9362 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:53,190 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9692 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:57,508 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8748 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:02,625 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8743 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:07,478 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9276 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:11,131 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9026 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:14,614 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9176 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:18,240 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9301 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:21,716 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8894 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:25,256 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8958 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:28,621 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9557 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:32,048 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9409 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:35,858 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8987 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:39,588 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8377 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:43,501 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8714 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:46,870 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8749 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:05,004 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9200
Val Loss: 0.8930
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:14,649 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:32:14,649 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:32:14,650 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:32:14,650 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:32:14,834 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:32:14,834 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:32:19,643 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8653 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:24,850 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9112 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:30,166 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9566 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:35,484 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8974 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:40,277 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8972 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:44,310 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8025 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:47,947 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9281 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:51,735 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8766 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:55,303 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8607 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:58,638 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9807 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:02,058 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9282 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:05,381 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9073 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:08,709 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9407 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:12,146 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8838 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:16,633 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8678 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:22,092 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9201 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:27,281 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8589 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:32,565 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8813 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:38,077 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9800 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:43,848 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9145 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:49,049 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8886 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:20,527 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.9023
Val Loss: 0.8953
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:30,224 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:34:30,224 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:34:30,225 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:34:30,225 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:34:30,393 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:34:30,393 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:34:33,753 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8941 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:36,906 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8869 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:39,952 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8940 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:43,046 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8270 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:46,041 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8417 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:49,029 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8650 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:52,084 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9594 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:55,253 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8637 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:34:58,263 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8846 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:01,263 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8609 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:04,238 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8602 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:07,254 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8994 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:10,240 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8927 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:13,270 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9553 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:16,233 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8547 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:19,223 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8962 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:22,218 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8989 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:25,201 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8839 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:28,155 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9572 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:31,227 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8800 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:34,501 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8307 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:35:53,050 - training.trainer - INFO - 
============================== Epoch 5 Summary ==============================
Train Loss: 0.8851
Val Loss: 0.8993
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:03,098 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:36:03,098 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:36:03,099 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:36:03,099 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:36:03,268 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:36:03,268 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:36:07,413 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8106 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:11,262 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8717 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:14,767 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8480 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:17,935 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9106 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:20,993 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8789 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:24,103 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9185 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:27,216 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8541 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:30,343 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8617 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:33,493 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8436 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:36,995 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8307 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:40,541 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9060 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:43,952 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8920 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:47,331 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8368 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:50,615 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8713 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:53,963 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8201 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:36:57,372 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8676 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:00,783 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8500 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:04,065 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9743 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:07,411 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8185 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:10,689 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8633 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:14,001 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8505 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:31,176 - training.trainer - INFO - 
============================== Epoch 6 Summary ==============================
Train Loss: 0.8657
Val Loss: 0.9452
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:40,206 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:37:40,206 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:37:40,206 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:37:40,206 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:37:40,386 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:37:40,386 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:37:43,909 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9040 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:47,221 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8668 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:50,500 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8599 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:53,844 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8540 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:37:57,133 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8234 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:38:00,455 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7696 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:38:03,778 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8447 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:38:07,073 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8474 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:38:10,291 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7784 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:13,728 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8840 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:16,961 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8001 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:20,230 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7947 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:23,449 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7938 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:26,679 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8248 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:29,858 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9147 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:33,157 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9078 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:36,330 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9010 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:39,606 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9360 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:42,903 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8398 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:46,136 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8637 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:38:49,378 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8294 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:06,254 - training.trainer - INFO - 
============================== Epoch 7 Summary ==============================
Train Loss: 0.8494
Val Loss: 0.9810
Best Val Loss: 0.8794
Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:15,250 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:39:15,250 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:39:15,250 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:39:15,250 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:39:15,441 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:39:15,441 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:39:19,370 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8671 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:22,616 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8075 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:25,700 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8515 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:28,911 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8090 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:32,495 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8636 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:36,157 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7753 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:39,639 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8377 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:43,108 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8919 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:47,164 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8266 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:50,610 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8770 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:53,999 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7904 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:39:57,595 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8049 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:01,151 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8673 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:04,751 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8333 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:08,867 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8521 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:13,030 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8561 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:17,161 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7746 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:21,177 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8065 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:25,001 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8454 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:28,815 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8543 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:32,724 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7436 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:40:51,100 - training.trainer - INFO - 
============================== Epoch 8 Summary ==============================
Train Loss: 0.8303
Val Loss: 0.9401
Best Val Loss: 0.8794
Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:00,375 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:41:00,376 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:41:00,376 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:41:00,376 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:41:00,558 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:41:00,558 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:41:04,749 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8179 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:08,786 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8336 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:12,786 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8136 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:16,908 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7687 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:20,927 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8373 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:24,990 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8520 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:28,862 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7636 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:32,774 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8847 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:36,694 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7971 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:40,633 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7967 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:44,626 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7920 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:48,688 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8078 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:52,690 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8076 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:41:56,535 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7930 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:00,505 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7828 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:04,306 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7820 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:08,137 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9131 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:12,127 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7573 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:16,013 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8256 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:19,863 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8233 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:24,198 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8168 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:44,952 - training.trainer - INFO - 
============================== Epoch 9 Summary ==============================
Train Loss: 0.8127
Val Loss: 1.0079
Best Val Loss: 0.8794
Learning Rate: 0.000099
================================================================================
2024-12-04 22:42:55,818 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:42:55,818 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:42:55,818 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:42:55,818 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:42:56,010 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:42:56,010 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:43:00,529 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8077 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:05,091 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8032 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:09,184 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8594 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:13,496 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8646 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:17,810 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7735 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:22,436 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8309 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:27,167 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8187 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:31,188 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7314 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:35,212 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7629 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:39,037 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8104 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:42,921 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7861 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:47,136 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8083 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:51,126 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8057 | Learning Rate: 0.000099
================================================================================
2024-12-04 22:43:55,228 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7883 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:43:59,666 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7923 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:03,819 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7566 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:07,817 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7807 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:11,685 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7250 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:16,649 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7814 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:20,603 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7841 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:24,368 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8008 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:45,033 - training.trainer - INFO - 
============================== Epoch 10 Summary ==============================
Train Loss: 0.7939
Val Loss: 1.0084
Best Val Loss: 0.8794
Learning Rate: 0.000098
================================================================================
2024-12-04 22:44:54,856 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:44:54,860 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:44:54,860 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:44:54,860 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:44:55,052 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:44:55,052 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:44:59,112 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7455 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:04,064 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8030 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:08,390 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8034 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:12,718 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7907 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:16,873 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7511 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:20,965 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7708 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:25,137 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7310 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:29,305 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7459 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:33,224 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7966 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:37,690 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7580 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:42,593 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7847 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:46,461 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8037 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:50,299 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7935 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:54,271 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7869 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:45:58,339 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7773 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:02,213 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8045 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:06,294 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7841 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:10,372 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7173 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:14,561 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7750 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:18,671 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8127 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:22,629 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8008 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:40,533 - training.trainer - INFO - 
============================== Epoch 11 Summary ==============================
Train Loss: 0.7779
Val Loss: 1.0068
Best Val Loss: 0.8794
Learning Rate: 0.000098
================================================================================
2024-12-04 22:46:40,558 - training.trainer - INFO - 
============================== Early Stopping ==============================
No improvement for 10 epochs
Best Val Loss: 0.8794
================================================================================
2024-12-04 22:46:40,562 - training.trainer - INFO - 
================================================================================
2024-12-04 22:46:40,562 - training.trainer - INFO - Training Completed!
2024-12-04 22:46:40,562 - training.trainer - INFO - ================================================================================
2024-12-04 22:46:40,562 - __main__ - INFO - Evaluating model...
2024-12-04 22:47:00,848 - __main__ - INFO - Test Metrics:
2024-12-04 22:47:00,851 - __main__ - INFO - mse: 0.8050
2024-12-04 22:47:00,851 - __main__ - INFO - rmse: 0.8972
2024-12-04 22:47:00,851 - __main__ - INFO - mae: 0.5167
2024-12-04 22:47:00,851 - __main__ - INFO - mape: 319.4664
2024-12-04 22:47:00,851 - __main__ - INFO - smape: 112.5786
