2024-12-04 22:24:34,582 - __main__ - INFO - CUDA Environment:
2024-12-04 22:24:34,583 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 22:24:34,583 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 22:24:35,119 - __main__ - INFO - Starting training...
2024-12-04 22:24:35,119 - training.trainer - INFO - 
================================================================================
2024-12-04 22:24:35,119 - training.trainer - INFO - Starting Training
2024-12-04 22:24:35,119 - training.trainer - INFO - ================================================================================
2024-12-04 22:24:44,766 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:24:44,766 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:24:44,767 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:24:44,767 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:24:45,048 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:24:45,048 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:24:49,319 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.3266 | Learning Rate: 0.000002
================================================================================
2024-12-04 22:24:53,990 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.1171 | Learning Rate: 0.000003
================================================================================
2024-12-04 22:24:58,567 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.1555 | Learning Rate: 0.000005
================================================================================
2024-12-04 22:25:02,236 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0768 | Learning Rate: 0.000006
================================================================================
2024-12-04 22:25:06,126 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.0258 | Learning Rate: 0.000008
================================================================================
2024-12-04 22:25:10,034 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9979 | Learning Rate: 0.000010
================================================================================
2024-12-04 22:25:13,923 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0392 | Learning Rate: 0.000011
================================================================================
2024-12-04 22:25:17,633 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0276 | Learning Rate: 0.000013
================================================================================
2024-12-04 22:25:21,386 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0877 | Learning Rate: 0.000014
================================================================================
2024-12-04 22:25:24,768 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0251 | Learning Rate: 0.000016
================================================================================
2024-12-04 22:25:28,246 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9884 | Learning Rate: 0.000017
================================================================================
2024-12-04 22:25:31,635 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.0570 | Learning Rate: 0.000019
================================================================================
2024-12-04 22:25:35,195 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0074 | Learning Rate: 0.000021
================================================================================
2024-12-04 22:25:38,585 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9886 | Learning Rate: 0.000022
================================================================================
2024-12-04 22:25:42,194 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9089 | Learning Rate: 0.000024
================================================================================
2024-12-04 22:25:45,507 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9050 | Learning Rate: 0.000025
================================================================================
2024-12-04 22:25:48,897 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9442 | Learning Rate: 0.000027
================================================================================
2024-12-04 22:25:52,434 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 1.0174 | Learning Rate: 0.000029
================================================================================
2024-12-04 22:25:55,837 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 1.0014 | Learning Rate: 0.000030
================================================================================
2024-12-04 22:25:59,392 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8770 | Learning Rate: 0.000032
================================================================================
2024-12-04 22:26:03,134 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9323 | Learning Rate: 0.000033
================================================================================
2024-12-04 22:26:22,735 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0241
Val Loss: 0.8864
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 22:26:22,735 - training.trainer - INFO - New best model saved! (Val Loss: 0.8864)
2024-12-04 22:26:22,804 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 22:26:33,039 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:26:33,039 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:26:33,039 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:26:33,039 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:26:33,228 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:26:33,228 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:26:36,492 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9705 | Learning Rate: 0.000035
================================================================================
2024-12-04 22:26:39,764 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9330 | Learning Rate: 0.000037
================================================================================
2024-12-04 22:26:42,950 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9518 | Learning Rate: 0.000038
================================================================================
2024-12-04 22:26:46,212 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9523 | Learning Rate: 0.000040
================================================================================
2024-12-04 22:26:49,704 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9556 | Learning Rate: 0.000041
================================================================================
2024-12-04 22:26:53,268 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9589 | Learning Rate: 0.000043
================================================================================
2024-12-04 22:26:56,483 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9481 | Learning Rate: 0.000044
================================================================================
2024-12-04 22:26:59,611 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9319 | Learning Rate: 0.000046
================================================================================
2024-12-04 22:27:02,959 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9738 | Learning Rate: 0.000048
================================================================================
2024-12-04 22:27:06,078 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9944 | Learning Rate: 0.000049
================================================================================
2024-12-04 22:27:09,150 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8734 | Learning Rate: 0.000051
================================================================================
2024-12-04 22:27:12,274 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9120 | Learning Rate: 0.000052
================================================================================
2024-12-04 22:27:15,458 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0652 | Learning Rate: 0.000054
================================================================================
2024-12-04 22:27:18,503 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9994 | Learning Rate: 0.000056
================================================================================
2024-12-04 22:27:21,929 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9011 | Learning Rate: 0.000057
================================================================================
2024-12-04 22:27:24,982 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9684 | Learning Rate: 0.000059
================================================================================
2024-12-04 22:27:28,146 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9644 | Learning Rate: 0.000060
================================================================================
2024-12-04 22:27:31,212 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9360 | Learning Rate: 0.000062
================================================================================
2024-12-04 22:27:34,349 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9641 | Learning Rate: 0.000063
================================================================================
2024-12-04 22:27:37,828 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9946 | Learning Rate: 0.000065
================================================================================
2024-12-04 22:27:41,911 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 1.0134 | Learning Rate: 0.000067
================================================================================
2024-12-04 22:28:02,419 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9601
Val Loss: 0.8794
Best Val Loss: 0.8864
Learning Rate: 0.000067
================================================================================
2024-12-04 22:28:02,420 - training.trainer - INFO - New best model saved! (Val Loss: 0.8794)
2024-12-04 22:28:02,517 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 22:28:12,425 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:28:12,425 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:28:12,425 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:28:12,425 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:28:12,621 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:28:12,622 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:28:16,537 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9397 | Learning Rate: 0.000068
================================================================================
2024-12-04 22:28:19,754 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9774 | Learning Rate: 0.000070
================================================================================
2024-12-04 22:28:22,923 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9455 | Learning Rate: 0.000071
================================================================================
2024-12-04 22:28:26,048 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0154 | Learning Rate: 0.000073
================================================================================
2024-12-04 22:28:29,237 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9698 | Learning Rate: 0.000075
================================================================================
2024-12-04 22:28:32,443 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8950 | Learning Rate: 0.000076
================================================================================
2024-12-04 22:28:36,264 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0165 | Learning Rate: 0.000078
================================================================================
2024-12-04 22:28:41,011 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0066 | Learning Rate: 0.000079
================================================================================
2024-12-04 22:28:45,070 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9175 | Learning Rate: 0.000081
================================================================================
2024-12-04 22:28:49,205 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9856 | Learning Rate: 0.000083
================================================================================
2024-12-04 22:28:53,131 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8708 | Learning Rate: 0.000084
================================================================================
2024-12-04 22:28:57,571 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8507 | Learning Rate: 0.000086
================================================================================
2024-12-04 22:29:01,828 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9110 | Learning Rate: 0.000087
================================================================================
2024-12-04 22:29:06,166 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9997 | Learning Rate: 0.000089
================================================================================
2024-12-04 22:29:10,810 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9449 | Learning Rate: 0.000090
================================================================================
2024-12-04 22:29:15,388 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8646 | Learning Rate: 0.000092
================================================================================
2024-12-04 22:29:22,086 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9675 | Learning Rate: 0.000094
================================================================================
2024-12-04 22:29:29,455 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8620 | Learning Rate: 0.000095
================================================================================
2024-12-04 22:29:37,003 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9312 | Learning Rate: 0.000097
================================================================================
2024-12-04 22:29:41,519 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9756 | Learning Rate: 0.000098
================================================================================
2024-12-04 22:29:45,592 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9059 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:09,744 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9406
Val Loss: 0.9108
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:23,224 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:30:23,227 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:30:23,227 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:30:23,228 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:30:23,425 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:30:23,425 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:30:28,015 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9287 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:32,223 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9237 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:36,382 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0363 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:40,804 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9932 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:44,866 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9412 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:49,095 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9362 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:53,190 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9692 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:30:57,508 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8748 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:02,625 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8743 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:07,478 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9276 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:11,131 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9026 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:14,614 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9176 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:18,240 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9301 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:21,716 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8894 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:25,256 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8958 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:28,621 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9557 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:32,048 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9409 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:35,858 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8987 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:39,588 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8377 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:43,501 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8714 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:31:46,870 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8749 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:05,004 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9200
Val Loss: 0.8930
Best Val Loss: 0.8794
Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:14,649 - training.trainer - INFO - 
Batch shapes:
2024-12-04 22:32:14,649 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 22:32:14,650 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 22:32:14,650 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 22:32:14,834 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 22:32:14,834 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 22:32:19,643 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8653 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:24,850 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9112 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:30,166 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9566 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:35,484 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8974 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:40,277 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8972 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:44,310 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8025 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:47,947 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9281 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:51,735 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8766 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:55,303 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8607 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:32:58,638 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9807 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:02,058 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9282 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:05,381 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9073 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:08,709 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9407 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:12,146 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8838 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:16,633 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8678 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:22,092 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9201 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:27,281 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8589 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:32,565 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8813 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:38,077 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9800 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:43,848 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9145 | Learning Rate: 0.000100
================================================================================
2024-12-04 22:33:49,049 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8886 | Learning Rate: 0.000100
================================================================================
