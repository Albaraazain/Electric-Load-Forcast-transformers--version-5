2024-12-04 23:05:38,760 - __main__ - INFO - CUDA Environment:
2024-12-04 23:05:38,760 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 23:05:38,760 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 23:05:39,320 - __main__ - INFO - Starting training...
2024-12-04 23:05:39,320 - training.trainer - INFO - 
================================================================================
2024-12-04 23:05:39,320 - training.trainer - INFO - Starting Training
2024-12-04 23:05:39,320 - training.trainer - INFO - ================================================================================
2024-12-04 23:05:50,024 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:05:50,024 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:05:50,025 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:05:50,025 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:05:50,450 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:05:50,450 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:05:54,538 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.3395 | Learning Rate: 0.000002
================================================================================
2024-12-04 23:05:58,269 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.1937 | Learning Rate: 0.000003
================================================================================
2024-12-04 23:06:02,911 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0880 | Learning Rate: 0.000005
================================================================================
2024-12-04 23:06:08,272 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0128 | Learning Rate: 0.000006
================================================================================
2024-12-04 23:06:13,140 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9793 | Learning Rate: 0.000008
================================================================================
2024-12-04 23:06:18,405 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0195 | Learning Rate: 0.000010
================================================================================
2024-12-04 23:06:23,480 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9916 | Learning Rate: 0.000011
================================================================================
2024-12-04 23:06:28,720 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0145 | Learning Rate: 0.000013
================================================================================
2024-12-04 23:06:34,232 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0849 | Learning Rate: 0.000014
================================================================================
2024-12-04 23:06:39,920 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9830 | Learning Rate: 0.000016
================================================================================
2024-12-04 23:06:45,055 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9749 | Learning Rate: 0.000017
================================================================================
2024-12-04 23:06:49,945 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.0574 | Learning Rate: 0.000019
================================================================================
2024-12-04 23:06:54,650 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9960 | Learning Rate: 0.000021
================================================================================
2024-12-04 23:06:59,250 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9529 | Learning Rate: 0.000022
================================================================================
2024-12-04 23:07:03,690 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9263 | Learning Rate: 0.000024
================================================================================
2024-12-04 23:07:08,844 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 1.0268 | Learning Rate: 0.000025
================================================================================
2024-12-04 23:07:13,460 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 1.0093 | Learning Rate: 0.000027
================================================================================
2024-12-04 23:07:18,344 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 1.0114 | Learning Rate: 0.000029
================================================================================
2024-12-04 23:07:22,625 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9200 | Learning Rate: 0.000030
================================================================================
2024-12-04 23:07:27,363 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9464 | Learning Rate: 0.000032
================================================================================
2024-12-04 23:07:32,473 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 1.0532 | Learning Rate: 0.000033
================================================================================
2024-12-04 23:07:54,818 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0277
Val Loss: 0.8917
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 23:07:54,818 - training.trainer - INFO - New best model saved! (Val Loss: 0.8917)
2024-12-04 23:07:54,882 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:08:04,366 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:08:04,366 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:08:04,366 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:08:04,366 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:08:04,562 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:08:04,562 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:08:08,510 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9101 | Learning Rate: 0.000035
================================================================================
2024-12-04 23:08:12,080 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.0016 | Learning Rate: 0.000037
================================================================================
2024-12-04 23:08:15,488 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9639 | Learning Rate: 0.000038
================================================================================
2024-12-04 23:08:19,031 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9714 | Learning Rate: 0.000040
================================================================================
2024-12-04 23:08:22,610 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9966 | Learning Rate: 0.000041
================================================================================
2024-12-04 23:08:26,145 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0359 | Learning Rate: 0.000043
================================================================================
2024-12-04 23:08:29,386 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9306 | Learning Rate: 0.000044
================================================================================
2024-12-04 23:08:33,310 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9537 | Learning Rate: 0.000046
================================================================================
2024-12-04 23:08:37,722 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9888 | Learning Rate: 0.000048
================================================================================
2024-12-04 23:08:42,195 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9307 | Learning Rate: 0.000049
================================================================================
2024-12-04 23:08:46,150 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9643 | Learning Rate: 0.000051
================================================================================
2024-12-04 23:08:49,929 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.0524 | Learning Rate: 0.000052
================================================================================
2024-12-04 23:08:53,395 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8946 | Learning Rate: 0.000054
================================================================================
2024-12-04 23:08:56,959 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9885 | Learning Rate: 0.000056
================================================================================
2024-12-04 23:09:00,366 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9696 | Learning Rate: 0.000057
================================================================================
2024-12-04 23:09:03,727 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9333 | Learning Rate: 0.000059
================================================================================
2024-12-04 23:09:07,019 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8626 | Learning Rate: 0.000060
================================================================================
2024-12-04 23:09:10,583 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 1.0407 | Learning Rate: 0.000062
================================================================================
2024-12-04 23:09:14,336 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9363 | Learning Rate: 0.000063
================================================================================
2024-12-04 23:09:18,000 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8776 | Learning Rate: 0.000065
================================================================================
2024-12-04 23:09:21,312 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9556 | Learning Rate: 0.000067
================================================================================
2024-12-04 23:09:42,690 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9599
Val Loss: 0.8784
Best Val Loss: 0.8917
Learning Rate: 0.000067
================================================================================
2024-12-04 23:09:42,690 - training.trainer - INFO - New best model saved! (Val Loss: 0.8784)
2024-12-04 23:09:42,749 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:09:52,267 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:09:52,270 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:09:52,271 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:09:52,271 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:09:52,440 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:09:52,440 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:09:55,943 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.0181 | Learning Rate: 0.000068
================================================================================
2024-12-04 23:09:59,399 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9754 | Learning Rate: 0.000070
================================================================================
2024-12-04 23:10:02,800 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8729 | Learning Rate: 0.000071
================================================================================
2024-12-04 23:10:06,150 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8957 | Learning Rate: 0.000073
================================================================================
2024-12-04 23:10:09,557 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9759 | Learning Rate: 0.000075
================================================================================
2024-12-04 23:10:13,146 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9069 | Learning Rate: 0.000076
================================================================================
2024-12-04 23:10:16,770 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9328 | Learning Rate: 0.000078
================================================================================
2024-12-04 23:10:20,143 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9380 | Learning Rate: 0.000079
================================================================================
2024-12-04 23:10:23,681 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9491 | Learning Rate: 0.000081
================================================================================
2024-12-04 23:10:28,164 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9531 | Learning Rate: 0.000083
================================================================================
2024-12-04 23:10:32,760 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 1.0078 | Learning Rate: 0.000084
================================================================================
2024-12-04 23:10:36,892 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9494 | Learning Rate: 0.000086
================================================================================
2024-12-04 23:10:40,970 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9049 | Learning Rate: 0.000087
================================================================================
2024-12-04 23:10:45,788 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9185 | Learning Rate: 0.000089
================================================================================
2024-12-04 23:10:50,321 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0109 | Learning Rate: 0.000090
================================================================================
2024-12-04 23:10:56,235 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9260 | Learning Rate: 0.000092
================================================================================
2024-12-04 23:11:00,338 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9360 | Learning Rate: 0.000094
================================================================================
2024-12-04 23:11:03,358 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9817 | Learning Rate: 0.000095
================================================================================
2024-12-04 23:11:06,431 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9099 | Learning Rate: 0.000097
================================================================================
2024-12-04 23:11:09,490 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8856 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:11:12,663 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9245 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:11:32,650 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9416
Val Loss: 0.9037
Best Val Loss: 0.8784
Learning Rate: 0.000100
================================================================================
2024-12-04 23:11:42,520 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:11:42,520 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:11:42,520 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:11:42,520 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:11:42,702 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:11:42,702 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:11:45,830 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9425 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:11:49,030 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9887 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:11:52,330 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9192 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:11:55,563 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9196 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:11:59,156 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9500 | Learning Rate: 0.000100
================================================================================
