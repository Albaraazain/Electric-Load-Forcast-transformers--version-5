2024-12-04 23:28:02,945 - __main__ - INFO - CUDA Environment:
2024-12-04 23:28:02,945 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 23:28:02,945 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 23:28:03,478 - __main__ - INFO - Starting training...
2024-12-04 23:28:03,479 - training.trainer - INFO - 
================================================================================
2024-12-04 23:28:03,480 - training.trainer - INFO - Starting Training
2024-12-04 23:28:03,480 - training.trainer - INFO - ================================================================================
2024-12-04 23:28:13,359 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:28:13,360 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:28:13,360 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:28:13,361 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:28:13,714 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:28:13,714 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:28:18,143 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.1539 | Learning Rate: 0.000002
================================================================================
2024-12-04 23:28:22,192 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.0244 | Learning Rate: 0.000003
================================================================================
2024-12-04 23:28:25,938 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0362 | Learning Rate: 0.000005
================================================================================
2024-12-04 23:28:29,419 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0360 | Learning Rate: 0.000006
================================================================================
2024-12-04 23:28:33,291 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.0571 | Learning Rate: 0.000008
================================================================================
2024-12-04 23:28:36,658 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0526 | Learning Rate: 0.000010
================================================================================
2024-12-04 23:28:40,355 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9931 | Learning Rate: 0.000011
================================================================================
2024-12-04 23:28:43,941 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9832 | Learning Rate: 0.000013
================================================================================
2024-12-04 23:28:47,418 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0327 | Learning Rate: 0.000014
================================================================================
2024-12-04 23:28:50,849 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0348 | Learning Rate: 0.000016
================================================================================
2024-12-04 23:28:54,377 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 1.0981 | Learning Rate: 0.000017
================================================================================
2024-12-04 23:28:57,843 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9281 | Learning Rate: 0.000019
================================================================================
2024-12-04 23:29:01,335 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9971 | Learning Rate: 0.000021
================================================================================
2024-12-04 23:29:04,837 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9681 | Learning Rate: 0.000022
================================================================================
2024-12-04 23:29:08,361 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9862 | Learning Rate: 0.000024
================================================================================
2024-12-04 23:29:11,918 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9798 | Learning Rate: 0.000025
================================================================================
2024-12-04 23:29:15,361 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9881 | Learning Rate: 0.000027
================================================================================
2024-12-04 23:29:18,868 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9747 | Learning Rate: 0.000029
================================================================================
2024-12-04 23:29:22,442 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9415 | Learning Rate: 0.000030
================================================================================
2024-12-04 23:29:25,910 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9253 | Learning Rate: 0.000032
================================================================================
2024-12-04 23:29:29,908 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9602 | Learning Rate: 0.000033
================================================================================
2024-12-04 23:30:04,939 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0072
Val Loss: 0.8829
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 23:30:04,939 - training.trainer - INFO - New best model saved! (Val Loss: 0.8829)
2024-12-04 23:30:05,030 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:30:14,300 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:30:14,300 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:30:14,300 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:30:14,300 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:30:14,475 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:30:14,475 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:30:17,970 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.0203 | Learning Rate: 0.000035
================================================================================
2024-12-04 23:30:21,376 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8775 | Learning Rate: 0.000037
================================================================================
2024-12-04 23:30:24,745 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9420 | Learning Rate: 0.000038
================================================================================
2024-12-04 23:30:28,082 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0296 | Learning Rate: 0.000040
================================================================================
2024-12-04 23:30:31,415 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9560 | Learning Rate: 0.000041
================================================================================
2024-12-04 23:30:34,891 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8865 | Learning Rate: 0.000043
================================================================================
2024-12-04 23:30:38,155 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9464 | Learning Rate: 0.000044
================================================================================
2024-12-04 23:30:41,437 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9696 | Learning Rate: 0.000046
================================================================================
2024-12-04 23:30:44,825 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0034 | Learning Rate: 0.000048
================================================================================
2024-12-04 23:30:48,203 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0078 | Learning Rate: 0.000049
================================================================================
2024-12-04 23:30:51,576 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8985 | Learning Rate: 0.000051
================================================================================
2024-12-04 23:30:54,789 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9281 | Learning Rate: 0.000052
================================================================================
2024-12-04 23:30:58,045 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0542 | Learning Rate: 0.000054
================================================================================
2024-12-04 23:31:01,342 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9544 | Learning Rate: 0.000056
================================================================================
2024-12-04 23:31:04,695 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9592 | Learning Rate: 0.000057
================================================================================
2024-12-04 23:31:08,141 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9873 | Learning Rate: 0.000059
================================================================================
2024-12-04 23:31:11,614 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9216 | Learning Rate: 0.000060
================================================================================
2024-12-04 23:31:15,377 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9269 | Learning Rate: 0.000062
================================================================================
2024-12-04 23:31:18,724 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9063 | Learning Rate: 0.000063
================================================================================
2024-12-04 23:31:22,125 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 1.0063 | Learning Rate: 0.000065
================================================================================
2024-12-04 23:31:25,585 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9912 | Learning Rate: 0.000067
================================================================================
2024-12-04 23:32:11,357 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9606
Val Loss: 0.8791
Best Val Loss: 0.8829
Learning Rate: 0.000067
================================================================================
2024-12-04 23:32:11,357 - training.trainer - INFO - New best model saved! (Val Loss: 0.8791)
2024-12-04 23:32:11,477 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:32:23,469 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:32:23,482 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:32:23,482 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:32:23,482 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:32:24,395 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:32:24,395 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:32:30,676 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9244 | Learning Rate: 0.000068
================================================================================
2024-12-04 23:32:35,961 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9565 | Learning Rate: 0.000070
================================================================================
2024-12-04 23:32:40,897 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9815 | Learning Rate: 0.000071
================================================================================
2024-12-04 23:32:45,260 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0188 | Learning Rate: 0.000073
================================================================================
2024-12-04 23:32:49,710 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9526 | Learning Rate: 0.000075
================================================================================
2024-12-04 23:32:54,841 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9879 | Learning Rate: 0.000076
================================================================================
2024-12-04 23:33:04,589 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0036 | Learning Rate: 0.000078
================================================================================
2024-12-04 23:33:11,435 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8672 | Learning Rate: 0.000079
================================================================================
2024-12-04 23:33:17,803 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9107 | Learning Rate: 0.000081
================================================================================
2024-12-04 23:33:23,738 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8986 | Learning Rate: 0.000083
================================================================================
2024-12-04 23:33:29,754 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9741 | Learning Rate: 0.000084
================================================================================
2024-12-04 23:33:35,083 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9622 | Learning Rate: 0.000086
================================================================================
2024-12-04 23:33:40,897 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8665 | Learning Rate: 0.000087
================================================================================
2024-12-04 23:33:46,837 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9432 | Learning Rate: 0.000089
================================================================================
2024-12-04 23:33:51,747 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9468 | Learning Rate: 0.000090
================================================================================
2024-12-04 23:33:56,534 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8994 | Learning Rate: 0.000092
================================================================================
2024-12-04 23:34:04,907 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9126 | Learning Rate: 0.000094
================================================================================
2024-12-04 23:34:14,064 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9880 | Learning Rate: 0.000095
================================================================================
2024-12-04 23:34:25,529 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9311 | Learning Rate: 0.000097
================================================================================
2024-12-04 23:34:35,327 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9432 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:34:43,215 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9037 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:35:50,347 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9415
Val Loss: 0.9039
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:04,373 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:36:04,373 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:36:04,373 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:36:04,373 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:36:05,254 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:36:05,256 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:36:11,978 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9216 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:20,339 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8569 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:26,773 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9653 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:33,087 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9595 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:40,645 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9822 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:47,324 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8508 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:55,776 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8755 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:07,480 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8667 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:12,873 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9852 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:19,009 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9679 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:26,988 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9122 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:33,377 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9627 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:39,574 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9171 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:47,804 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9320 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:54,962 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9108 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:00,810 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8981 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:08,931 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8965 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:15,842 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9768 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:24,620 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9143 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:32,341 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8951 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:40,643 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8984 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:39:46,745 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9212
Val Loss: 0.9153
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:01,932 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:40:01,932 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:40:01,932 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:40:01,932 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:40:02,895 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:40:02,895 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:40:09,601 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8936 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:17,300 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9221 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:29,134 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9519 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:35,105 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8828 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:42,669 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9377 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:48,571 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8940 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:57,310 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9300 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:06,096 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8888 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:11,908 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9533 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:20,885 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9241 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:28,668 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8829 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:35,763 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8822 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:42,639 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8278 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:49,658 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8776 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:57,190 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9200 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:03,327 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8375 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:11,655 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9153 | Learning Rate: 0.000100
================================================================================
