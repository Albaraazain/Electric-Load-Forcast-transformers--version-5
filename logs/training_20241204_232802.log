2024-12-04 23:28:02,945 - __main__ - INFO - CUDA Environment:
2024-12-04 23:28:02,945 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 23:28:02,945 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 23:28:03,478 - __main__ - INFO - Starting training...
2024-12-04 23:28:03,479 - training.trainer - INFO - 
================================================================================
2024-12-04 23:28:03,480 - training.trainer - INFO - Starting Training
2024-12-04 23:28:03,480 - training.trainer - INFO - ================================================================================
2024-12-04 23:28:13,359 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:28:13,360 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:28:13,360 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:28:13,361 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:28:13,714 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:28:13,714 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:28:18,143 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.1539 | Learning Rate: 0.000002
================================================================================
2024-12-04 23:28:22,192 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.0244 | Learning Rate: 0.000003
================================================================================
2024-12-04 23:28:25,938 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0362 | Learning Rate: 0.000005
================================================================================
2024-12-04 23:28:29,419 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0360 | Learning Rate: 0.000006
================================================================================
2024-12-04 23:28:33,291 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.0571 | Learning Rate: 0.000008
================================================================================
2024-12-04 23:28:36,658 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0526 | Learning Rate: 0.000010
================================================================================
2024-12-04 23:28:40,355 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9931 | Learning Rate: 0.000011
================================================================================
2024-12-04 23:28:43,941 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9832 | Learning Rate: 0.000013
================================================================================
2024-12-04 23:28:47,418 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0327 | Learning Rate: 0.000014
================================================================================
2024-12-04 23:28:50,849 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0348 | Learning Rate: 0.000016
================================================================================
2024-12-04 23:28:54,377 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 1.0981 | Learning Rate: 0.000017
================================================================================
2024-12-04 23:28:57,843 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9281 | Learning Rate: 0.000019
================================================================================
2024-12-04 23:29:01,335 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9971 | Learning Rate: 0.000021
================================================================================
2024-12-04 23:29:04,837 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9681 | Learning Rate: 0.000022
================================================================================
2024-12-04 23:29:08,361 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9862 | Learning Rate: 0.000024
================================================================================
2024-12-04 23:29:11,918 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9798 | Learning Rate: 0.000025
================================================================================
2024-12-04 23:29:15,361 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9881 | Learning Rate: 0.000027
================================================================================
2024-12-04 23:29:18,868 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9747 | Learning Rate: 0.000029
================================================================================
2024-12-04 23:29:22,442 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9415 | Learning Rate: 0.000030
================================================================================
2024-12-04 23:29:25,910 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9253 | Learning Rate: 0.000032
================================================================================
2024-12-04 23:29:29,908 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9602 | Learning Rate: 0.000033
================================================================================
2024-12-04 23:30:04,939 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0072
Val Loss: 0.8829
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 23:30:04,939 - training.trainer - INFO - New best model saved! (Val Loss: 0.8829)
2024-12-04 23:30:05,030 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:30:14,300 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:30:14,300 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:30:14,300 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:30:14,300 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:30:14,475 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:30:14,475 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:30:17,970 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.0203 | Learning Rate: 0.000035
================================================================================
2024-12-04 23:30:21,376 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8775 | Learning Rate: 0.000037
================================================================================
2024-12-04 23:30:24,745 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9420 | Learning Rate: 0.000038
================================================================================
2024-12-04 23:30:28,082 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0296 | Learning Rate: 0.000040
================================================================================
2024-12-04 23:30:31,415 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9560 | Learning Rate: 0.000041
================================================================================
2024-12-04 23:30:34,891 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8865 | Learning Rate: 0.000043
================================================================================
2024-12-04 23:30:38,155 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9464 | Learning Rate: 0.000044
================================================================================
2024-12-04 23:30:41,437 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9696 | Learning Rate: 0.000046
================================================================================
2024-12-04 23:30:44,825 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0034 | Learning Rate: 0.000048
================================================================================
2024-12-04 23:30:48,203 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0078 | Learning Rate: 0.000049
================================================================================
2024-12-04 23:30:51,576 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8985 | Learning Rate: 0.000051
================================================================================
2024-12-04 23:30:54,789 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9281 | Learning Rate: 0.000052
================================================================================
2024-12-04 23:30:58,045 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0542 | Learning Rate: 0.000054
================================================================================
2024-12-04 23:31:01,342 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9544 | Learning Rate: 0.000056
================================================================================
2024-12-04 23:31:04,695 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9592 | Learning Rate: 0.000057
================================================================================
2024-12-04 23:31:08,141 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9873 | Learning Rate: 0.000059
================================================================================
2024-12-04 23:31:11,614 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9216 | Learning Rate: 0.000060
================================================================================
2024-12-04 23:31:15,377 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9269 | Learning Rate: 0.000062
================================================================================
2024-12-04 23:31:18,724 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9063 | Learning Rate: 0.000063
================================================================================
2024-12-04 23:31:22,125 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 1.0063 | Learning Rate: 0.000065
================================================================================
2024-12-04 23:31:25,585 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9912 | Learning Rate: 0.000067
================================================================================
2024-12-04 23:32:11,357 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9606
Val Loss: 0.8791
Best Val Loss: 0.8829
Learning Rate: 0.000067
================================================================================
2024-12-04 23:32:11,357 - training.trainer - INFO - New best model saved! (Val Loss: 0.8791)
2024-12-04 23:32:11,477 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:32:23,469 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:32:23,482 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:32:23,482 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:32:23,482 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:32:24,395 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:32:24,395 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:32:30,676 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9244 | Learning Rate: 0.000068
================================================================================
2024-12-04 23:32:35,961 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9565 | Learning Rate: 0.000070
================================================================================
2024-12-04 23:32:40,897 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9815 | Learning Rate: 0.000071
================================================================================
2024-12-04 23:32:45,260 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0188 | Learning Rate: 0.000073
================================================================================
2024-12-04 23:32:49,710 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9526 | Learning Rate: 0.000075
================================================================================
2024-12-04 23:32:54,841 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9879 | Learning Rate: 0.000076
================================================================================
2024-12-04 23:33:04,589 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0036 | Learning Rate: 0.000078
================================================================================
2024-12-04 23:33:11,435 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8672 | Learning Rate: 0.000079
================================================================================
2024-12-04 23:33:17,803 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9107 | Learning Rate: 0.000081
================================================================================
2024-12-04 23:33:23,738 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8986 | Learning Rate: 0.000083
================================================================================
2024-12-04 23:33:29,754 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9741 | Learning Rate: 0.000084
================================================================================
2024-12-04 23:33:35,083 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9622 | Learning Rate: 0.000086
================================================================================
2024-12-04 23:33:40,897 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8665 | Learning Rate: 0.000087
================================================================================
2024-12-04 23:33:46,837 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9432 | Learning Rate: 0.000089
================================================================================
2024-12-04 23:33:51,747 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9468 | Learning Rate: 0.000090
================================================================================
2024-12-04 23:33:56,534 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8994 | Learning Rate: 0.000092
================================================================================
2024-12-04 23:34:04,907 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9126 | Learning Rate: 0.000094
================================================================================
2024-12-04 23:34:14,064 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9880 | Learning Rate: 0.000095
================================================================================
2024-12-04 23:34:25,529 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9311 | Learning Rate: 0.000097
================================================================================
2024-12-04 23:34:35,327 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9432 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:34:43,215 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9037 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:35:50,347 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9415
Val Loss: 0.9039
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:04,373 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:36:04,373 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:36:04,373 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:36:04,373 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:36:05,254 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:36:05,256 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:36:11,978 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9216 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:20,339 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8569 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:26,773 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9653 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:33,087 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9595 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:40,645 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9822 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:47,324 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8508 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:36:55,776 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8755 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:07,480 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8667 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:12,873 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9852 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:19,009 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9679 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:26,988 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9122 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:33,377 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9627 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:39,574 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9171 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:47,804 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9320 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:37:54,962 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9108 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:00,810 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8981 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:08,931 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8965 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:15,842 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9768 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:24,620 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9143 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:32,341 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8951 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:38:40,643 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8984 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:39:46,745 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9212
Val Loss: 0.9153
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:01,932 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:40:01,932 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:40:01,932 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:40:01,932 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:40:02,895 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:40:02,895 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:40:09,601 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8936 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:17,300 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9221 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:29,134 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9519 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:35,105 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8828 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:42,669 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9377 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:48,571 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8940 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:40:57,310 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9300 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:06,096 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8888 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:11,908 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9533 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:20,885 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9241 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:28,668 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8829 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:35,763 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8822 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:42,639 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8278 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:49,658 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8776 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:41:57,190 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9200 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:03,327 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8375 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:11,655 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9153 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:18,503 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8942 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:23,331 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9730 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:27,639 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8836 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:42:36,344 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8721 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:43:55,357 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.9021
Val Loss: 0.8923
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:10,227 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:44:10,227 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:44:10,227 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:44:10,227 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:44:11,180 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:44:11,180 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:44:20,753 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8783 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:26,340 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8719 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:31,932 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8680 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:37,453 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8920 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:43,458 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9148 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:48,768 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8704 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:44:55,872 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9022 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:01,650 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8884 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:08,912 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9197 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:15,064 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8994 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:20,444 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8618 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:26,021 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8163 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:31,735 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8188 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:36,769 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9131 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:42,671 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8823 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:48,412 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9061 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:53,940 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8958 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:45:58,905 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9126 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:46:04,460 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9949 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:46:11,112 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8481 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:46:14,886 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8672 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:46:47,172 - training.trainer - INFO - 
============================== Epoch 5 Summary ==============================
Train Loss: 0.8868
Val Loss: 0.9406
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:46:55,974 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:46:55,974 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:46:55,974 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:46:55,974 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:46:56,146 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:46:56,146 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:46:59,460 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8997 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:02,669 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8975 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:05,903 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8205 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:09,208 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8544 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:12,554 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8223 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:15,804 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8298 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:18,997 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8888 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:22,248 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8367 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:25,449 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9119 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:28,643 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8132 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:31,839 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8874 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:34,955 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8619 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:38,148 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8398 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:41,370 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9006 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:44,563 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9099 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:47,766 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8923 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:51,035 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8974 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:54,249 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8880 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:47:57,414 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8566 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:00,675 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8582 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:03,804 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8468 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:36,044 - training.trainer - INFO - 
============================== Epoch 6 Summary ==============================
Train Loss: 0.8673
Val Loss: 0.9227
Best Val Loss: 0.8791
Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:44,716 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:48:44,716 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:48:44,716 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:48:44,716 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:48:44,887 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:48:44,887 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:48:48,234 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9058 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:51,543 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8531 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:55,137 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8178 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:48:58,626 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8616 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:49:02,080 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8655 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:49:05,468 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8045 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:49:08,950 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8257 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:49:12,355 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8055 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:49:15,774 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9417 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:19,024 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8345 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:22,365 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8612 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:25,653 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8263 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:29,005 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8206 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:32,397 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8297 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:35,769 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8767 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:39,009 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8113 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:42,247 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8496 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:45,515 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8869 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:48,793 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8559 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:52,101 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9028 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:49:55,398 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7838 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:28,219 - training.trainer - INFO - 
============================== Epoch 7 Summary ==============================
Train Loss: 0.8486
Val Loss: 0.9244
Best Val Loss: 0.8791
Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:36,949 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:50:36,949 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:50:36,949 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:50:36,964 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:50:37,146 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:50:37,146 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:50:40,350 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8381 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:43,557 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8061 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:46,774 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8779 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:49,973 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8848 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:53,209 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9125 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:56,502 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9074 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:50:59,821 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8759 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:03,027 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7581 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:06,235 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8818 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:09,485 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7922 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:12,708 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8238 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:15,928 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8320 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:19,184 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8250 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:22,573 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8149 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:25,810 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7606 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:29,023 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7762 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:32,302 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8105 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:35,529 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8443 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:38,878 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7686 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:42,042 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8200 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:51:45,229 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8934 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:17,021 - training.trainer - INFO - 
============================== Epoch 8 Summary ==============================
Train Loss: 0.8335
Val Loss: 0.9307
Best Val Loss: 0.8791
Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:25,706 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:52:25,706 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:52:25,706 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:52:25,706 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:52:25,903 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:52:25,903 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:52:29,162 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8369 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:32,362 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8319 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:35,571 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8071 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:38,777 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9327 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:41,975 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8232 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:45,147 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8716 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:48,340 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7683 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:51,643 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7872 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:54,915 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8012 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:52:58,129 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7228 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:01,461 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7792 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:04,815 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7903 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:08,274 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8395 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:11,767 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8761 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:15,305 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8051 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:18,724 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8440 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:22,049 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8261 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:25,485 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7732 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:28,773 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7956 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:32,117 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7835 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:53:35,631 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8198 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:07,116 - training.trainer - INFO - 
============================== Epoch 9 Summary ==============================
Train Loss: 0.8150
Val Loss: 0.9675
Best Val Loss: 0.8791
Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:15,911 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:54:15,911 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:54:15,911 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:54:15,911 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:54:16,087 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:54:16,087 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:54:19,436 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8279 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:23,197 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.7517 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:26,490 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8526 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:29,691 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7883 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:32,866 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7862 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:36,169 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7080 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:39,405 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8352 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:42,686 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7816 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:45,870 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8108 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:49,128 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8605 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:52,285 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8090 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:55,520 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7696 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:54:58,828 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7815 | Learning Rate: 0.000099
================================================================================
2024-12-04 23:55:02,118 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7827 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:05,321 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8262 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:08,519 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7791 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:11,768 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8028 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:14,994 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7785 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:18,231 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7937 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:21,440 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7972 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:24,613 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7986 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:55:58,356 - training.trainer - INFO - 
============================== Epoch 10 Summary ==============================
Train Loss: 0.7963
Val Loss: 0.9611
Best Val Loss: 0.8791
Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:07,496 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:56:07,496 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:56:07,496 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:56:07,496 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:56:07,688 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:56:07,689 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:56:11,011 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7800 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:14,342 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8533 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:17,599 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.7595 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:20,794 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8133 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:24,055 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7570 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:27,260 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8019 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:30,494 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8339 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:33,673 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8073 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:36,995 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7859 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:40,320 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7455 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:43,999 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8559 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:47,476 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7530 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:50,687 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8047 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:53,996 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7696 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:56:57,220 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7374 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:00,440 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7378 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:03,759 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7725 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:07,003 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7176 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:10,351 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7696 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:13,688 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8042 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:17,011 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7993 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:47,913 - training.trainer - INFO - 
============================== Epoch 11 Summary ==============================
Train Loss: 0.7838
Val Loss: 0.9598
Best Val Loss: 0.8791
Learning Rate: 0.000098
================================================================================
2024-12-04 23:57:47,954 - training.trainer - INFO - 
============================== Early Stopping ==============================
No improvement for 10 epochs
Best Val Loss: 0.8791
================================================================================
2024-12-04 23:57:47,960 - training.trainer - INFO - 
================================================================================
2024-12-04 23:57:47,960 - training.trainer - INFO - Training Completed!
2024-12-04 23:57:47,960 - training.trainer - INFO - ================================================================================
2024-12-04 23:57:47,961 - __main__ - INFO - Evaluating model...
2024-12-04 23:58:06,873 - __main__ - INFO - Test Metrics:
2024-12-04 23:58:06,873 - __main__ - INFO - mse: 0.7076
2024-12-04 23:58:06,873 - __main__ - INFO - rmse: 0.8412
2024-12-04 23:58:06,873 - __main__ - INFO - mae: 0.4739
2024-12-04 23:58:06,873 - __main__ - INFO - mape: 270.0652
2024-12-04 23:58:06,873 - __main__ - INFO - smape: 108.7035
2024-12-04 23:58:09,097 - __main__ - INFO - Training completed successfully!
