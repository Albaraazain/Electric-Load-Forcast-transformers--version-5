2024-12-04 23:12:09,171 - __main__ - INFO - CUDA Environment:
2024-12-04 23:12:09,171 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 23:12:09,171 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 23:12:09,720 - __main__ - INFO - Starting training...
2024-12-04 23:12:09,720 - training.trainer - INFO - 
================================================================================
2024-12-04 23:12:09,720 - training.trainer - INFO - Starting Training
2024-12-04 23:12:09,720 - training.trainer - INFO - ================================================================================
2024-12-04 23:12:19,883 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:12:19,883 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:12:19,883 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:12:19,883 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:12:20,205 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:12:20,205 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:12:23,863 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.8228 | Learning Rate: 0.000002
================================================================================
2024-12-04 23:12:27,121 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.4711 | Learning Rate: 0.000003
================================================================================
2024-12-04 23:12:30,391 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.1698 | Learning Rate: 0.000005
================================================================================
2024-12-04 23:12:33,617 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.1403 | Learning Rate: 0.000006
================================================================================
2024-12-04 23:12:36,856 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.0633 | Learning Rate: 0.000008
================================================================================
2024-12-04 23:12:40,017 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.1205 | Learning Rate: 0.000010
================================================================================
2024-12-04 23:12:43,192 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0558 | Learning Rate: 0.000011
================================================================================
2024-12-04 23:12:46,358 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0448 | Learning Rate: 0.000013
================================================================================
2024-12-04 23:12:49,483 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0739 | Learning Rate: 0.000014
================================================================================
2024-12-04 23:12:52,658 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9036 | Learning Rate: 0.000016
================================================================================
2024-12-04 23:12:55,857 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9887 | Learning Rate: 0.000017
================================================================================
2024-12-04 23:12:59,064 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8998 | Learning Rate: 0.000019
================================================================================
2024-12-04 23:13:02,330 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 1.0056 | Learning Rate: 0.000021
================================================================================
2024-12-04 23:13:05,643 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9720 | Learning Rate: 0.000022
================================================================================
2024-12-04 23:13:08,853 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0094 | Learning Rate: 0.000024
================================================================================
2024-12-04 23:13:12,101 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9612 | Learning Rate: 0.000025
================================================================================
2024-12-04 23:13:15,361 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9555 | Learning Rate: 0.000027
================================================================================
2024-12-04 23:13:18,570 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 1.0247 | Learning Rate: 0.000029
================================================================================
2024-12-04 23:13:21,761 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9926 | Learning Rate: 0.000030
================================================================================
2024-12-04 23:13:24,910 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 1.0010 | Learning Rate: 0.000032
================================================================================
2024-12-04 23:13:28,507 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9674 | Learning Rate: 0.000033
================================================================================
2024-12-04 23:14:01,549 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0783
Val Loss: 0.8854
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 23:14:01,549 - training.trainer - INFO - New best model saved! (Val Loss: 0.8854)
2024-12-04 23:14:01,647 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:14:11,622 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:14:11,622 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:14:11,630 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:14:11,630 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:14:11,817 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:14:11,817 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:14:14,934 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9685 | Learning Rate: 0.000035
================================================================================
2024-12-04 23:14:17,979 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9737 | Learning Rate: 0.000037
================================================================================
2024-12-04 23:14:20,940 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9638 | Learning Rate: 0.000038
================================================================================
2024-12-04 23:14:24,056 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9142 | Learning Rate: 0.000040
================================================================================
2024-12-04 23:14:27,110 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9521 | Learning Rate: 0.000041
================================================================================
2024-12-04 23:14:30,253 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0215 | Learning Rate: 0.000043
================================================================================
2024-12-04 23:14:33,273 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9719 | Learning Rate: 0.000044
================================================================================
2024-12-04 23:14:36,264 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0311 | Learning Rate: 0.000046
================================================================================
2024-12-04 23:14:39,269 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8911 | Learning Rate: 0.000048
================================================================================
2024-12-04 23:14:42,370 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0125 | Learning Rate: 0.000049
================================================================================
2024-12-04 23:14:45,400 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9510 | Learning Rate: 0.000051
================================================================================
2024-12-04 23:14:48,430 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9780 | Learning Rate: 0.000052
================================================================================
2024-12-04 23:14:51,360 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8956 | Learning Rate: 0.000054
================================================================================
2024-12-04 23:14:54,461 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9587 | Learning Rate: 0.000056
================================================================================
2024-12-04 23:14:57,472 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0301 | Learning Rate: 0.000057
================================================================================
2024-12-04 23:15:00,476 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9291 | Learning Rate: 0.000059
================================================================================
2024-12-04 23:15:03,506 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 1.0055 | Learning Rate: 0.000060
================================================================================
2024-12-04 23:15:06,619 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 1.0155 | Learning Rate: 0.000062
================================================================================
2024-12-04 23:15:09,670 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9624 | Learning Rate: 0.000063
================================================================================
2024-12-04 23:15:12,611 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8896 | Learning Rate: 0.000065
================================================================================
2024-12-04 23:15:15,686 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9728 | Learning Rate: 0.000067
================================================================================
2024-12-04 23:15:47,305 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9661
Val Loss: 0.8979
Best Val Loss: 0.8854
Learning Rate: 0.000067
================================================================================
2024-12-04 23:15:58,813 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:15:58,814 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:15:58,814 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:15:58,815 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:15:59,000 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:15:59,000 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:16:02,502 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8626 | Learning Rate: 0.000068
================================================================================
2024-12-04 23:16:06,290 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9008 | Learning Rate: 0.000070
================================================================================
2024-12-04 23:16:10,425 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9730 | Learning Rate: 0.000071
================================================================================
2024-12-04 23:16:14,399 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9487 | Learning Rate: 0.000073
================================================================================
2024-12-04 23:16:18,921 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9303 | Learning Rate: 0.000075
================================================================================
2024-12-04 23:16:23,576 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9343 | Learning Rate: 0.000076
================================================================================
2024-12-04 23:16:27,771 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9316 | Learning Rate: 0.000078
================================================================================
2024-12-04 23:16:31,895 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9709 | Learning Rate: 0.000079
================================================================================
2024-12-04 23:16:35,552 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9159 | Learning Rate: 0.000081
================================================================================
2024-12-04 23:16:38,708 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0027 | Learning Rate: 0.000083
================================================================================
2024-12-04 23:16:42,157 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9242 | Learning Rate: 0.000084
================================================================================
2024-12-04 23:16:45,476 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8566 | Learning Rate: 0.000086
================================================================================
2024-12-04 23:16:48,753 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9966 | Learning Rate: 0.000087
================================================================================
2024-12-04 23:16:51,885 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9413 | Learning Rate: 0.000089
================================================================================
2024-12-04 23:16:55,386 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9514 | Learning Rate: 0.000090
================================================================================
2024-12-04 23:16:58,551 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9789 | Learning Rate: 0.000092
================================================================================
2024-12-04 23:17:01,927 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9230 | Learning Rate: 0.000094
================================================================================
2024-12-04 23:17:05,121 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9671 | Learning Rate: 0.000095
================================================================================
2024-12-04 23:17:08,379 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9673 | Learning Rate: 0.000097
================================================================================
2024-12-04 23:17:11,535 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 1.0257 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:17:14,957 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9547 | Learning Rate: 0.000100
================================================================================
