2024-12-04 23:17:44,654 - __main__ - INFO - CUDA Environment:
2024-12-04 23:17:44,654 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 23:17:44,654 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 23:17:45,191 - __main__ - INFO - Starting training...
2024-12-04 23:17:45,200 - training.trainer - INFO - 
================================================================================
2024-12-04 23:17:45,201 - training.trainer - INFO - Starting Training
2024-12-04 23:17:45,201 - training.trainer - INFO - ================================================================================
2024-12-04 23:17:54,684 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:17:54,684 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:17:54,685 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:17:54,685 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:17:55,084 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:17:55,084 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:17:58,791 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.3119 | Learning Rate: 0.000002
================================================================================
2024-12-04 23:18:02,081 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.1228 | Learning Rate: 0.000003
================================================================================
2024-12-04 23:18:05,311 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.1331 | Learning Rate: 0.000005
================================================================================
2024-12-04 23:18:08,473 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0038 | Learning Rate: 0.000006
================================================================================
2024-12-04 23:18:11,692 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9464 | Learning Rate: 0.000008
================================================================================
2024-12-04 23:18:14,822 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0248 | Learning Rate: 0.000010
================================================================================
2024-12-04 23:18:18,020 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9630 | Learning Rate: 0.000011
================================================================================
2024-12-04 23:18:21,157 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 1.0215 | Learning Rate: 0.000013
================================================================================
2024-12-04 23:18:24,333 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9854 | Learning Rate: 0.000014
================================================================================
2024-12-04 23:18:27,430 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0102 | Learning Rate: 0.000016
================================================================================
2024-12-04 23:18:30,725 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 1.0367 | Learning Rate: 0.000017
================================================================================
2024-12-04 23:18:33,977 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.1112 | Learning Rate: 0.000019
================================================================================
2024-12-04 23:18:37,393 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9873 | Learning Rate: 0.000021
================================================================================
2024-12-04 23:18:40,771 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9150 | Learning Rate: 0.000022
================================================================================
2024-12-04 23:18:43,919 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0812 | Learning Rate: 0.000024
================================================================================
2024-12-04 23:18:47,182 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9453 | Learning Rate: 0.000025
================================================================================
2024-12-04 23:18:50,563 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9977 | Learning Rate: 0.000027
================================================================================
2024-12-04 23:18:53,645 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9456 | Learning Rate: 0.000029
================================================================================
2024-12-04 23:18:56,878 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9578 | Learning Rate: 0.000030
================================================================================
2024-12-04 23:19:00,288 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9254 | Learning Rate: 0.000032
================================================================================
2024-12-04 23:19:04,911 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 1.0604 | Learning Rate: 0.000033
================================================================================
2024-12-04 23:19:38,556 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0232
Val Loss: 0.8859
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 23:19:38,556 - training.trainer - INFO - New best model saved! (Val Loss: 0.8859)
2024-12-04 23:19:38,642 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:19:47,954 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:19:47,955 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:19:47,955 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:19:47,955 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:19:48,136 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:19:48,136 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:19:51,647 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9556 | Learning Rate: 0.000035
================================================================================
2024-12-04 23:19:54,774 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9253 | Learning Rate: 0.000037
================================================================================
2024-12-04 23:19:57,829 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8961 | Learning Rate: 0.000038
================================================================================
2024-12-04 23:20:00,912 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9230 | Learning Rate: 0.000040
================================================================================
2024-12-04 23:20:03,920 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9790 | Learning Rate: 0.000041
================================================================================
2024-12-04 23:20:06,939 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0133 | Learning Rate: 0.000043
================================================================================
2024-12-04 23:20:09,974 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9683 | Learning Rate: 0.000044
================================================================================
2024-12-04 23:20:13,030 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9251 | Learning Rate: 0.000046
================================================================================
2024-12-04 23:20:16,071 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9937 | Learning Rate: 0.000048
================================================================================
2024-12-04 23:20:19,105 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9055 | Learning Rate: 0.000049
================================================================================
2024-12-04 23:20:22,166 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 1.0239 | Learning Rate: 0.000051
================================================================================
2024-12-04 23:20:25,240 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9914 | Learning Rate: 0.000052
================================================================================
2024-12-04 23:20:28,366 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9067 | Learning Rate: 0.000054
================================================================================
2024-12-04 23:20:31,371 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 1.0096 | Learning Rate: 0.000056
================================================================================
2024-12-04 23:20:34,472 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9794 | Learning Rate: 0.000057
================================================================================
2024-12-04 23:20:37,746 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9767 | Learning Rate: 0.000059
================================================================================
2024-12-04 23:20:41,338 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9068 | Learning Rate: 0.000060
================================================================================
2024-12-04 23:20:45,425 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9203 | Learning Rate: 0.000062
================================================================================
2024-12-04 23:20:50,215 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 1.0023 | Learning Rate: 0.000063
================================================================================
2024-12-04 23:20:55,241 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 1.0323 | Learning Rate: 0.000065
================================================================================
2024-12-04 23:20:58,893 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9423 | Learning Rate: 0.000067
================================================================================
2024-12-04 23:21:32,191 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9608
Val Loss: 0.8984
Best Val Loss: 0.8859
Learning Rate: 0.000067
================================================================================
2024-12-04 23:21:41,366 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:21:41,372 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:21:41,374 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:21:41,374 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:21:41,561 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:21:41,561 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:21:44,923 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9330 | Learning Rate: 0.000068
================================================================================
2024-12-04 23:21:48,356 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.0077 | Learning Rate: 0.000070
================================================================================
2024-12-04 23:21:52,223 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9633 | Learning Rate: 0.000071
================================================================================
2024-12-04 23:21:56,731 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9764 | Learning Rate: 0.000073
================================================================================
2024-12-04 23:22:00,661 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9422 | Learning Rate: 0.000075
================================================================================
2024-12-04 23:22:04,392 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9386 | Learning Rate: 0.000076
================================================================================
2024-12-04 23:22:08,877 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9458 | Learning Rate: 0.000078
================================================================================
2024-12-04 23:22:12,869 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9262 | Learning Rate: 0.000079
================================================================================
2024-12-04 23:22:16,788 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0296 | Learning Rate: 0.000081
================================================================================
2024-12-04 23:22:21,194 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8813 | Learning Rate: 0.000083
================================================================================
2024-12-04 23:22:25,692 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9488 | Learning Rate: 0.000084
================================================================================
2024-12-04 23:22:29,276 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9420 | Learning Rate: 0.000086
================================================================================
2024-12-04 23:22:33,567 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9552 | Learning Rate: 0.000087
================================================================================
2024-12-04 23:22:37,066 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9400 | Learning Rate: 0.000089
================================================================================
2024-12-04 23:22:41,657 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8929 | Learning Rate: 0.000090
================================================================================
2024-12-04 23:22:45,411 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9202 | Learning Rate: 0.000092
================================================================================
2024-12-04 23:22:49,940 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9244 | Learning Rate: 0.000094
================================================================================
2024-12-04 23:22:53,897 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9489 | Learning Rate: 0.000095
================================================================================
2024-12-04 23:22:57,014 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8623 | Learning Rate: 0.000097
================================================================================
2024-12-04 23:23:00,043 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9971 | Learning Rate: 0.000098
================================================================================
2024-12-04 23:23:03,116 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8933 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:23:32,621 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9414
Val Loss: 0.8824
Best Val Loss: 0.8859
Learning Rate: 0.000100
================================================================================
2024-12-04 23:23:32,621 - training.trainer - INFO - New best model saved! (Val Loss: 0.8824)
2024-12-04 23:23:32,715 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 23:23:41,784 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:23:41,787 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:23:41,787 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:23:41,787 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:23:41,972 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:23:41,972 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:23:45,071 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8766 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:23:48,135 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9869 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:23:51,207 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8643 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:23:54,421 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9961 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:23:57,494 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9754 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:00,589 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9857 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:03,709 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8885 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:06,790 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9443 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:09,879 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9187 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:13,000 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8833 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:16,080 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8932 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:19,165 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8589 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:22,274 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9004 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:25,688 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9503 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:29,232 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9917 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:32,521 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8748 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:35,860 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8534 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:39,195 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8862 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:42,372 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9035 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:45,512 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9423 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:24:48,646 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9686 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:19,131 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9211
Val Loss: 0.8884
Best Val Loss: 0.8824
Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:28,385 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:25:28,385 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:25:28,385 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:25:28,385 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:25:28,571 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:25:28,581 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:25:31,750 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9176 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:34,947 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8501 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:38,220 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8828 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:41,424 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9621 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:44,833 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9633 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:48,411 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8861 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:52,196 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8978 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:25:55,734 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8895 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:00,741 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9219 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:06,384 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9111 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:12,654 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9090 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:17,579 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9485 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:21,963 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8039 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:26,399 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8979 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:30,603 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9846 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:33,810 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8111 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:37,051 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8466 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:40,280 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8900 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:43,483 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8363 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:46,781 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9277 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:26:50,007 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9468 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:27:21,857 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.8993
Val Loss: 0.9483
Best Val Loss: 0.8824
Learning Rate: 0.000100
================================================================================
2024-12-04 23:27:32,129 - training.trainer - INFO - 
Batch shapes:
2024-12-04 23:27:32,130 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 23:27:32,131 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 23:27:32,131 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 23:27:32,324 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 23:27:32,324 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 23:27:35,651 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8510 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:27:38,755 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9051 | Learning Rate: 0.000100
================================================================================
2024-12-04 23:27:42,090 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8533 | Learning Rate: 0.000100
================================================================================
