2024-12-04 20:46:54,902 - __main__ - INFO - CUDA Environment:
2024-12-04 20:46:54,905 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 20:46:54,905 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 20:46:55,434 - __main__ - INFO - Starting training...
2024-12-04 20:46:55,450 - training.trainer - INFO - 
================================================================================
2024-12-04 20:46:55,450 - training.trainer - INFO - Starting Training
2024-12-04 20:46:55,450 - training.trainer - INFO - ================================================================================
2024-12-04 20:47:06,079 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:47:06,079 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:47:06,079 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:47:06,079 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:47:06,509 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:47:06,509 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:47:08,048 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 1.3706 | Learning Rate: 0.000002
================================================================================
2024-12-04 20:47:09,108 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 1.2179 | Learning Rate: 0.000003
================================================================================
2024-12-04 20:47:10,164 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.1206 | Learning Rate: 0.000005
================================================================================
2024-12-04 20:47:11,186 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 1.0829 | Learning Rate: 0.000006
================================================================================
2024-12-04 20:47:12,233 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 1.0242 | Learning Rate: 0.000008
================================================================================
2024-12-04 20:47:13,249 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 1.0311 | Learning Rate: 0.000010
================================================================================
2024-12-04 20:47:14,272 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 1.0291 | Learning Rate: 0.000011
================================================================================
2024-12-04 20:47:15,310 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9945 | Learning Rate: 0.000013
================================================================================
2024-12-04 20:47:16,406 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9805 | Learning Rate: 0.000014
================================================================================
2024-12-04 20:47:17,437 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 1.0762 | Learning Rate: 0.000016
================================================================================
2024-12-04 20:47:18,486 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9434 | Learning Rate: 0.000017
================================================================================
2024-12-04 20:47:19,498 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9757 | Learning Rate: 0.000019
================================================================================
2024-12-04 20:47:20,504 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9651 | Learning Rate: 0.000021
================================================================================
2024-12-04 20:47:21,554 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 1.0218 | Learning Rate: 0.000022
================================================================================
2024-12-04 20:47:22,620 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9566 | Learning Rate: 0.000024
================================================================================
2024-12-04 20:47:23,710 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9761 | Learning Rate: 0.000025
================================================================================
2024-12-04 20:47:24,776 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9792 | Learning Rate: 0.000027
================================================================================
2024-12-04 20:47:25,825 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9991 | Learning Rate: 0.000029
================================================================================
2024-12-04 20:47:26,869 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9637 | Learning Rate: 0.000030
================================================================================
2024-12-04 20:47:27,898 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 1.0382 | Learning Rate: 0.000032
================================================================================
2024-12-04 20:47:28,997 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9909 | Learning Rate: 0.000033
================================================================================
2024-12-04 20:47:42,801 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0351
Val Loss: 0.8913
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 20:47:42,801 - training.trainer - INFO - New best model saved! (Val Loss: 0.8913)
2024-12-04 20:47:42,874 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 20:47:54,567 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:47:54,584 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:47:54,584 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:47:54,585 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:47:54,654 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:47:54,655 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:47:56,750 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9797 | Learning Rate: 0.000035
================================================================================
2024-12-04 20:47:58,347 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9160 | Learning Rate: 0.000037
================================================================================
2024-12-04 20:47:59,924 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9329 | Learning Rate: 0.000038
================================================================================
2024-12-04 20:48:01,713 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9838 | Learning Rate: 0.000040
================================================================================
2024-12-04 20:48:03,258 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9746 | Learning Rate: 0.000041
================================================================================
2024-12-04 20:48:05,382 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9925 | Learning Rate: 0.000043
================================================================================
2024-12-04 20:48:06,582 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9460 | Learning Rate: 0.000044
================================================================================
2024-12-04 20:48:07,709 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9501 | Learning Rate: 0.000046
================================================================================
2024-12-04 20:48:08,756 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8737 | Learning Rate: 0.000048
================================================================================
2024-12-04 20:48:10,026 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9216 | Learning Rate: 0.000049
================================================================================
2024-12-04 20:48:11,147 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9761 | Learning Rate: 0.000051
================================================================================
2024-12-04 20:48:12,242 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 1.0253 | Learning Rate: 0.000052
================================================================================
2024-12-04 20:48:13,319 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9577 | Learning Rate: 0.000054
================================================================================
2024-12-04 20:48:14,382 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9912 | Learning Rate: 0.000056
================================================================================
2024-12-04 20:48:15,372 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0096 | Learning Rate: 0.000057
================================================================================
2024-12-04 20:48:16,457 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9201 | Learning Rate: 0.000059
================================================================================
2024-12-04 20:48:17,487 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9969 | Learning Rate: 0.000060
================================================================================
2024-12-04 20:48:18,624 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9445 | Learning Rate: 0.000062
================================================================================
2024-12-04 20:48:19,647 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9921 | Learning Rate: 0.000063
================================================================================
2024-12-04 20:48:20,678 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9049 | Learning Rate: 0.000065
================================================================================
2024-12-04 20:48:21,868 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9728 | Learning Rate: 0.000067
================================================================================
2024-12-04 20:48:36,896 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 0.9601
Val Loss: 0.8840
Best Val Loss: 0.8913
Learning Rate: 0.000067
================================================================================
2024-12-04 20:48:36,897 - training.trainer - INFO - New best model saved! (Val Loss: 0.8840)
2024-12-04 20:48:36,953 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 20:48:47,028 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:48:47,029 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:48:47,029 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:48:47,030 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:48:47,057 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:48:47,057 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:48:48,166 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9596 | Learning Rate: 0.000068
================================================================================
2024-12-04 20:48:49,300 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8778 | Learning Rate: 0.000070
================================================================================
2024-12-04 20:48:50,401 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 1.0131 | Learning Rate: 0.000071
================================================================================
2024-12-04 20:48:51,721 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8821 | Learning Rate: 0.000073
================================================================================
2024-12-04 20:48:52,887 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9142 | Learning Rate: 0.000075
================================================================================
2024-12-04 20:48:54,218 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8634 | Learning Rate: 0.000076
================================================================================
2024-12-04 20:48:55,362 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8949 | Learning Rate: 0.000078
================================================================================
2024-12-04 20:48:56,425 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9442 | Learning Rate: 0.000079
================================================================================
2024-12-04 20:48:57,456 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 1.0148 | Learning Rate: 0.000081
================================================================================
2024-12-04 20:48:58,870 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9752 | Learning Rate: 0.000083
================================================================================
2024-12-04 20:49:00,267 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9624 | Learning Rate: 0.000084
================================================================================
2024-12-04 20:49:01,541 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9311 | Learning Rate: 0.000086
================================================================================
2024-12-04 20:49:02,912 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9793 | Learning Rate: 0.000087
================================================================================
2024-12-04 20:49:03,918 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8885 | Learning Rate: 0.000089
================================================================================
2024-12-04 20:49:05,102 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 1.0022 | Learning Rate: 0.000090
================================================================================
2024-12-04 20:49:06,096 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.9933 | Learning Rate: 0.000092
================================================================================
2024-12-04 20:49:07,126 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9328 | Learning Rate: 0.000094
================================================================================
2024-12-04 20:49:08,132 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8967 | Learning Rate: 0.000095
================================================================================
2024-12-04 20:49:09,146 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9465 | Learning Rate: 0.000097
================================================================================
2024-12-04 20:49:10,208 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9029 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:49:11,222 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9529 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:24,139 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9394
Val Loss: 0.8829
Best Val Loss: 0.8840
Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:24,141 - training.trainer - INFO - New best model saved! (Val Loss: 0.8829)
2024-12-04 20:49:24,198 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 20:49:33,868 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:49:33,870 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:49:33,871 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:49:33,871 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:49:33,897 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:49:33,897 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:49:35,075 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8737 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:36,165 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.9544 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:37,197 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8609 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:38,235 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.9890 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:39,218 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9825 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:40,356 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9000 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:41,342 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9514 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:42,388 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9486 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:43,444 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9613 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:44,544 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9443 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:45,534 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9400 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:46,648 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8207 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:47,679 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9504 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:48,748 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9313 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:49,763 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8934 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:50,831 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8922 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:51,875 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9400 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:52,999 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8504 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:54,126 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.9517 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:55,117 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9004 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:49:56,166 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8790 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:09,765 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9198
Val Loss: 0.9011
Best Val Loss: 0.8829
Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:19,431 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:50:19,432 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:50:19,432 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:50:19,432 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:50:19,460 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:50:19,460 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:50:20,730 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9046 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:21,913 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8569 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:23,021 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.9122 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:24,227 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8878 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:25,224 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9446 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:26,317 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9105 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:27,402 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8640 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:28,600 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.9290 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:29,684 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9334 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:30,904 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.9623 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:31,947 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.9345 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:32,965 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8386 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:33,997 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9421 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:35,034 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8885 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:36,041 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.9063 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:37,089 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8471 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:38,183 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.9141 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:39,332 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8579 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:40,426 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8535 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:41,681 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9196 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:50:42,722 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9261 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:03,042 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.9016
Val Loss: 0.8944
Best Val Loss: 0.8829
Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:18,715 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:51:18,716 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:51:18,716 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:51:18,716 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:51:18,740 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:51:18,740 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:51:19,976 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9339 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:21,036 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8296 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:22,314 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8857 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:23,723 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8835 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:24,928 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9386 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:25,921 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9023 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:26,898 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.9079 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:27,882 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8233 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:28,860 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9960 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:29,795 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8391 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:30,753 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8902 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:31,697 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8455 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:32,688 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8853 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:33,629 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8753 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:34,579 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8738 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:35,489 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8432 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:36,429 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8704 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:37,327 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.9799 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:38,272 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7964 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:39,168 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9032 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:40,123 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.9024 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:51:52,897 - training.trainer - INFO - 
============================== Epoch 5 Summary ==============================
Train Loss: 0.8860
Val Loss: 0.9199
Best Val Loss: 0.8829
Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:04,151 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:52:04,152 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:52:04,152 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:52:04,152 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:52:04,182 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:52:04,182 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:52:05,394 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.9307 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:06,350 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8949 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:07,489 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8266 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:08,440 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7606 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:09,483 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9080 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:10,480 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8387 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:11,472 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8710 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:12,563 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8238 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:13,545 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.9053 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:14,540 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8857 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:15,586 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8435 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:16,676 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8759 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:17,615 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.9167 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:18,651 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.9402 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:19,642 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8389 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:20,570 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8604 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:21,521 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8466 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:22,469 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8699 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:23,437 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8123 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:24,384 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9062 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:25,416 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8621 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:37,978 - training.trainer - INFO - 
============================== Epoch 6 Summary ==============================
Train Loss: 0.8675
Val Loss: 0.9277
Best Val Loss: 0.8829
Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:47,096 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:52:47,097 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:52:47,097 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:52:47,097 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:52:47,122 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:52:47,123 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:52:48,266 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7898 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:49,197 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8104 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:50,117 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.7851 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:51,099 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8785 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:52,140 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.9343 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:53,166 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7907 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:54,160 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8595 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:55,105 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8438 | Learning Rate: 0.000100
================================================================================
2024-12-04 20:52:56,124 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8457 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:52:57,081 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8418 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:52:58,180 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8159 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:52:59,208 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.9365 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:00,290 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8860 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:01,289 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8716 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:02,266 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8081 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:03,268 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8732 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:04,211 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8737 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:05,319 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.7982 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:06,468 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7728 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:07,603 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.9316 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:08,856 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8935 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:21,342 - training.trainer - INFO - 
============================== Epoch 7 Summary ==============================
Train Loss: 0.8496
Val Loss: 0.9637
Best Val Loss: 0.8829
Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:30,541 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:53:30,545 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:53:30,556 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:53:30,556 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:53:30,586 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:53:30,587 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:53:31,723 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8503 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:32,660 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8124 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:33,603 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.8079 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:34,688 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8341 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:35,840 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8271 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:36,890 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9030 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:37,909 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8548 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:38,965 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8416 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:39,999 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8555 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:40,978 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7636 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:41,963 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.8349 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:42,947 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7591 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:43,912 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8278 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:44,901 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.8092 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:45,853 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8046 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:46,869 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.8005 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:47,794 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8430 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:48,776 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8581 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:49,808 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8253 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:50,763 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8823 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:53:51,777 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8645 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:04,261 - training.trainer - INFO - 
============================== Epoch 8 Summary ==============================
Train Loss: 0.8314
Val Loss: 0.9483
Best Val Loss: 0.8829
Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:13,559 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:54:13,562 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:54:13,562 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:54:13,563 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:54:13,591 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:54:13,591 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:54:14,710 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8102 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:15,655 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8321 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:16,611 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.7950 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:17,638 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8006 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:18,551 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7803 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:19,575 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.9000 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:20,530 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.8177 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:21,512 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8396 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:22,473 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8179 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:23,405 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.8566 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:24,442 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7617 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:25,364 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7988 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:26,404 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.8209 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:27,400 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7733 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:28,383 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8216 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:29,267 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7899 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:30,243 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8203 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:31,234 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8300 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:32,110 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7704 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:33,057 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.8427 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:34,054 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8118 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:46,579 - training.trainer - INFO - 
============================== Epoch 9 Summary ==============================
Train Loss: 0.8139
Val Loss: 0.9378
Best Val Loss: 0.8829
Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:55,826 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:54:55,830 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:54:55,830 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:54:55,830 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:54:55,860 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:54:55,860 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:54:57,017 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.8196 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:58,088 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8027 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:54:59,079 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.7838 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:00,182 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.8181 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:01,247 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.8208 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:02,189 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.7912 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:03,187 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7760 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:04,171 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7552 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:05,229 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8589 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:06,215 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7743 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:07,130 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7677 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:08,076 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8154 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:09,142 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7812 | Learning Rate: 0.000099
================================================================================
2024-12-04 20:55:10,084 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7750 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:11,079 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7757 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:12,051 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7547 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:13,058 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7985 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:14,103 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8081 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:15,044 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.8224 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:16,024 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7422 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:17,003 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.8083 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:29,317 - training.trainer - INFO - 
============================== Epoch 10 Summary ==============================
Train Loss: 0.7928
Val Loss: 0.9780
Best Val Loss: 0.8829
Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:38,516 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:55:38,516 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:55:38,517 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:55:38,517 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:55:38,543 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:55:38,543 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:55:39,616 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7330 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:40,592 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.8347 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:41,561 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.7127 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:42,534 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7117 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:43,525 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7493 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:44,446 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8283 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:45,390 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7727 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:46,323 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.7839 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:47,280 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.8042 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:48,212 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7732 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:49,189 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7762 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:50,094 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.8388 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:51,010 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7299 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:51,968 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7943 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:52,842 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.8129 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:53,821 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7567 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:54,844 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.8088 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:55,754 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8061 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:56,759 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7746 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:57,742 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7824 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:55:58,624 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7443 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:11,362 - training.trainer - INFO - 
============================== Epoch 11 Summary ==============================
Train Loss: 0.7776
Val Loss: 0.9810
Best Val Loss: 0.8829
Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:20,650 - training.trainer - INFO - 
Batch shapes:
2024-12-04 20:56:20,654 - training.trainer - INFO - Encoder inputs: torch.Size([32, 48, 8])
2024-12-04 20:56:20,654 - training.trainer - INFO - Decoder inputs: torch.Size([32, 24, 8])
2024-12-04 20:56:20,654 - training.trainer - INFO - Targets: torch.Size([32, 24, 1])
2024-12-04 20:56:20,682 - training.trainer - INFO - Model output shape: torch.Size([32, 24, 1])
2024-12-04 20:56:20,682 - training.trainer - INFO - Target shape for loss: torch.Size([32, 24, 1])
2024-12-04 20:56:21,732 - training.trainer - INFO - 
============================== Batch 50/1050 ==============================
Average Loss: 0.7619 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:22,687 - training.trainer - INFO - 
============================== Batch 100/1050 ==============================
Average Loss: 0.7125 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:23,625 - training.trainer - INFO - 
============================== Batch 150/1050 ==============================
Average Loss: 0.7476 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:24,615 - training.trainer - INFO - 
============================== Batch 200/1050 ==============================
Average Loss: 0.7659 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:25,597 - training.trainer - INFO - 
============================== Batch 250/1050 ==============================
Average Loss: 0.7180 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:26,679 - training.trainer - INFO - 
============================== Batch 300/1050 ==============================
Average Loss: 0.8078 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:27,708 - training.trainer - INFO - 
============================== Batch 350/1050 ==============================
Average Loss: 0.7307 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:28,672 - training.trainer - INFO - 
============================== Batch 400/1050 ==============================
Average Loss: 0.8780 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:29,587 - training.trainer - INFO - 
============================== Batch 450/1050 ==============================
Average Loss: 0.7371 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:30,512 - training.trainer - INFO - 
============================== Batch 500/1050 ==============================
Average Loss: 0.7870 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:31,418 - training.trainer - INFO - 
============================== Batch 550/1050 ==============================
Average Loss: 0.7316 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:32,403 - training.trainer - INFO - 
============================== Batch 600/1050 ==============================
Average Loss: 0.7998 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:33,350 - training.trainer - INFO - 
============================== Batch 650/1050 ==============================
Average Loss: 0.7522 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:34,297 - training.trainer - INFO - 
============================== Batch 700/1050 ==============================
Average Loss: 0.7460 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:35,298 - training.trainer - INFO - 
============================== Batch 750/1050 ==============================
Average Loss: 0.7997 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:36,328 - training.trainer - INFO - 
============================== Batch 800/1050 ==============================
Average Loss: 0.7355 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:37,671 - training.trainer - INFO - 
============================== Batch 850/1050 ==============================
Average Loss: 0.7381 | Learning Rate: 0.000098
================================================================================
2024-12-04 20:56:38,950 - training.trainer - INFO - 
============================== Batch 900/1050 ==============================
Average Loss: 0.8075 | Learning Rate: 0.000097
================================================================================
2024-12-04 20:56:40,207 - training.trainer - INFO - 
============================== Batch 950/1050 ==============================
Average Loss: 0.7764 | Learning Rate: 0.000097
================================================================================
2024-12-04 20:56:41,324 - training.trainer - INFO - 
============================== Batch 1000/1050 ==============================
Average Loss: 0.7565 | Learning Rate: 0.000097
================================================================================
2024-12-04 20:56:42,406 - training.trainer - INFO - 
============================== Batch 1050/1050 ==============================
Average Loss: 0.7092 | Learning Rate: 0.000097
================================================================================
2024-12-04 20:56:54,910 - training.trainer - INFO - 
============================== Epoch 12 Summary ==============================
Train Loss: 0.7619
Val Loss: 1.0146
Best Val Loss: 0.8829
Learning Rate: 0.000097
================================================================================
2024-12-04 20:56:54,951 - training.trainer - INFO - 
============================== Early Stopping ==============================
No improvement for 10 epochs
Best Val Loss: 0.8829
================================================================================
2024-12-04 20:56:54,952 - training.trainer - INFO - 
================================================================================
2024-12-04 20:56:54,952 - training.trainer - INFO - Training Completed!
2024-12-04 20:56:54,952 - training.trainer - INFO - ================================================================================
2024-12-04 20:56:54,952 - __main__ - INFO - Evaluating model...
