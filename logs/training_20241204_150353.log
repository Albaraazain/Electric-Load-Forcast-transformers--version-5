2024-12-04 15:03:53,796 - __main__ - INFO - CUDA Environment:
2024-12-04 15:03:53,796 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 15:03:53,796 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 15:03:54,613 - __main__ - INFO - Starting training...
2024-12-04 15:03:54,613 - training.trainer - INFO - 
================================================================================
2024-12-04 15:03:54,613 - training.trainer - INFO - Starting Training
2024-12-04 15:03:54,613 - training.trainer - INFO - ================================================================================
2024-12-04 15:04:22,439 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:04:22,439 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:04:22,439 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:04:22,439 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:04:28,200 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:04:28,200 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:04:42,808 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 1.1936 | Learning Rate: 0.000001
================================================================================
2024-12-04 15:05:00,418 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 1.1121 | Learning Rate: 0.000002
================================================================================
2024-12-04 15:05:13,594 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 1.1859 | Learning Rate: 0.000002
================================================================================
2024-12-04 15:05:26,260 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 1.1198 | Learning Rate: 0.000003
================================================================================
2024-12-04 15:05:38,059 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 1.0421 | Learning Rate: 0.000004
================================================================================
2024-12-04 15:05:49,398 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 1.2157 | Learning Rate: 0.000005
================================================================================
2024-12-04 15:06:00,905 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 1.0423 | Learning Rate: 0.000006
================================================================================
2024-12-04 15:06:08,303 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 1.0932 | Learning Rate: 0.000006
================================================================================
2024-12-04 15:06:09,688 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 1.0949 | Learning Rate: 0.000007
================================================================================
2024-12-04 15:06:10,935 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 1.1246 | Learning Rate: 0.000008
================================================================================
2024-12-04 15:06:12,215 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 1.0391 | Learning Rate: 0.000009
================================================================================
2024-12-04 15:06:13,504 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 1.0043 | Learning Rate: 0.000010
================================================================================
2024-12-04 15:06:14,790 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 1.0841 | Learning Rate: 0.000010
================================================================================
2024-12-04 15:06:15,907 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 1.0198 | Learning Rate: 0.000011
================================================================================
2024-12-04 15:06:17,067 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 1.0452 | Learning Rate: 0.000012
================================================================================
2024-12-04 15:06:18,304 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 1.1890 | Learning Rate: 0.000013
================================================================================
2024-12-04 15:06:19,574 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 1.1034 | Learning Rate: 0.000014
================================================================================
2024-12-04 15:06:20,798 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 1.1279 | Learning Rate: 0.000014
================================================================================
2024-12-04 15:06:22,016 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 1.0512 | Learning Rate: 0.000015
================================================================================
2024-12-04 15:06:23,245 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 1.0135 | Learning Rate: 0.000016
================================================================================
2024-12-04 15:06:24,427 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 1.0945 | Learning Rate: 0.000017
================================================================================
2024-12-04 15:06:25,675 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 1.0029 | Learning Rate: 0.000018
================================================================================
2024-12-04 15:06:26,983 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 1.0545 | Learning Rate: 0.000018
================================================================================
2024-12-04 15:06:28,348 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 1.0097 | Learning Rate: 0.000019
================================================================================
2024-12-04 15:06:29,650 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 1.0054 | Learning Rate: 0.000020
================================================================================
2024-12-04 15:06:31,016 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 1.1880 | Learning Rate: 0.000021
================================================================================
2024-12-04 15:06:32,231 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 1.2325 | Learning Rate: 0.000022
================================================================================
2024-12-04 15:06:33,452 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 1.1039 | Learning Rate: 0.000022
================================================================================
2024-12-04 15:06:34,867 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 1.0632 | Learning Rate: 0.000023
================================================================================
2024-12-04 15:06:36,124 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 1.0583 | Learning Rate: 0.000024
================================================================================
2024-12-04 15:06:37,316 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 1.0259 | Learning Rate: 0.000025
================================================================================
2024-12-04 15:06:38,572 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 1.1341 | Learning Rate: 0.000026
================================================================================
2024-12-04 15:06:39,794 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 1.0205 | Learning Rate: 0.000026
================================================================================
2024-12-04 15:06:40,996 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 1.0681 | Learning Rate: 0.000027
================================================================================
2024-12-04 15:06:42,220 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 1.1056 | Learning Rate: 0.000028
================================================================================
2024-12-04 15:06:43,395 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 1.0427 | Learning Rate: 0.000029
================================================================================
2024-12-04 15:06:44,589 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 1.0359 | Learning Rate: 0.000030
================================================================================
2024-12-04 15:06:45,736 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 1.1029 | Learning Rate: 0.000030
================================================================================
2024-12-04 15:06:47,091 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 1.0263 | Learning Rate: 0.000031
================================================================================
2024-12-04 15:06:48,253 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.9003 | Learning Rate: 0.000032
================================================================================
2024-12-04 15:06:49,402 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.9906 | Learning Rate: 0.000033
================================================================================
2024-12-04 15:07:08,818 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0775
Val Loss: 0.9440
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 15:07:08,820 - training.trainer - INFO - New best model saved! (Val Loss: 0.9440)
2024-12-04 15:07:08,911 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 15:07:26,080 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:07:26,080 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:07:26,081 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:07:26,082 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:07:26,146 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:07:26,146 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:07:27,607 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 1.1105 | Learning Rate: 0.000034
================================================================================
2024-12-04 15:07:28,901 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 1.0974 | Learning Rate: 0.000035
================================================================================
2024-12-04 15:07:30,028 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.9771 | Learning Rate: 0.000036
================================================================================
2024-12-04 15:07:31,391 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 1.1090 | Learning Rate: 0.000037
================================================================================
2024-12-04 15:07:32,769 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 1.0971 | Learning Rate: 0.000037
================================================================================
2024-12-04 15:07:34,164 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 1.0437 | Learning Rate: 0.000038
================================================================================
2024-12-04 15:07:35,570 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.9770 | Learning Rate: 0.000039
================================================================================
2024-12-04 15:07:36,837 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 1.0135 | Learning Rate: 0.000040
================================================================================
2024-12-04 15:07:37,733 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 1.0236 | Learning Rate: 0.000041
================================================================================
2024-12-04 15:07:38,659 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.9709 | Learning Rate: 0.000041
================================================================================
2024-12-04 15:07:39,574 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.9755 | Learning Rate: 0.000042
================================================================================
2024-12-04 15:07:40,602 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.9774 | Learning Rate: 0.000043
================================================================================
2024-12-04 15:07:41,670 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 1.0728 | Learning Rate: 0.000044
================================================================================
2024-12-04 15:07:42,719 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 1.0199 | Learning Rate: 0.000045
================================================================================
2024-12-04 15:07:43,678 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 1.0115 | Learning Rate: 0.000045
================================================================================
2024-12-04 15:07:44,637 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.9414 | Learning Rate: 0.000046
================================================================================
2024-12-04 15:07:45,455 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 1.0135 | Learning Rate: 0.000047
================================================================================
2024-12-04 15:07:46,440 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.9894 | Learning Rate: 0.000048
================================================================================
2024-12-04 15:07:47,324 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 1.0138 | Learning Rate: 0.000048
================================================================================
2024-12-04 15:07:48,220 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 1.0830 | Learning Rate: 0.000049
================================================================================
2024-12-04 15:07:49,191 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 1.0820 | Learning Rate: 0.000050
================================================================================
2024-12-04 15:07:50,266 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 1.0676 | Learning Rate: 0.000051
================================================================================
2024-12-04 15:07:51,488 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 1.0280 | Learning Rate: 0.000052
================================================================================
2024-12-04 15:07:52,627 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.9471 | Learning Rate: 0.000052
================================================================================
2024-12-04 15:07:54,006 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.9366 | Learning Rate: 0.000053
================================================================================
2024-12-04 15:07:55,734 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.9913 | Learning Rate: 0.000054
================================================================================
2024-12-04 15:07:57,082 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.9856 | Learning Rate: 0.000055
================================================================================
2024-12-04 15:07:58,178 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.9748 | Learning Rate: 0.000056
================================================================================
2024-12-04 15:07:59,083 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.9782 | Learning Rate: 0.000056
================================================================================
2024-12-04 15:08:00,093 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 1.0491 | Learning Rate: 0.000057
================================================================================
2024-12-04 15:08:01,005 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 1.0145 | Learning Rate: 0.000058
================================================================================
2024-12-04 15:08:02,021 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 1.0057 | Learning Rate: 0.000059
================================================================================
2024-12-04 15:08:02,966 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 1.0800 | Learning Rate: 0.000060
================================================================================
2024-12-04 15:08:03,945 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 1.0068 | Learning Rate: 0.000060
================================================================================
2024-12-04 15:08:04,959 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 1.0212 | Learning Rate: 0.000061
================================================================================
2024-12-04 15:08:05,995 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.9266 | Learning Rate: 0.000062
================================================================================
2024-12-04 15:08:07,061 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.9495 | Learning Rate: 0.000063
================================================================================
2024-12-04 15:08:08,288 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 1.0501 | Learning Rate: 0.000064
================================================================================
2024-12-04 15:08:09,338 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.9830 | Learning Rate: 0.000064
================================================================================
2024-12-04 15:08:10,468 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.9972 | Learning Rate: 0.000065
================================================================================
2024-12-04 15:08:11,743 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 1.0254 | Learning Rate: 0.000066
================================================================================
2024-12-04 15:08:31,805 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 1.0141
Val Loss: 0.9009
Best Val Loss: 0.9440
Learning Rate: 0.000067
================================================================================
2024-12-04 15:08:31,805 - training.trainer - INFO - New best model saved! (Val Loss: 0.9009)
2024-12-04 15:08:31,899 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 15:08:47,495 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:08:47,495 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:08:47,495 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:08:47,495 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:08:47,534 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:08:47,534 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:08:49,335 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.9379 | Learning Rate: 0.000067
================================================================================
2024-12-04 15:08:50,826 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 1.0055 | Learning Rate: 0.000068
================================================================================
2024-12-04 15:08:51,937 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 1.0515 | Learning Rate: 0.000069
================================================================================
2024-12-04 15:08:52,923 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.9458 | Learning Rate: 0.000070
================================================================================
2024-12-04 15:08:53,886 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.9760 | Learning Rate: 0.000071
================================================================================
2024-12-04 15:08:55,042 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 1.0403 | Learning Rate: 0.000071
================================================================================
2024-12-04 15:08:56,214 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 1.0690 | Learning Rate: 0.000072
================================================================================
2024-12-04 15:08:57,184 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.9731 | Learning Rate: 0.000073
================================================================================
2024-12-04 15:08:58,284 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.9189 | Learning Rate: 0.000074
================================================================================
2024-12-04 15:08:59,475 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.9952 | Learning Rate: 0.000075
================================================================================
2024-12-04 15:09:00,546 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.9970 | Learning Rate: 0.000075
================================================================================
2024-12-04 15:09:01,471 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.9191 | Learning Rate: 0.000076
================================================================================
2024-12-04 15:09:02,577 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 1.0432 | Learning Rate: 0.000077
================================================================================
2024-12-04 15:09:03,571 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 1.0055 | Learning Rate: 0.000078
================================================================================
2024-12-04 15:09:04,675 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.9969 | Learning Rate: 0.000079
================================================================================
2024-12-04 15:09:05,718 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 1.0337 | Learning Rate: 0.000079
================================================================================
2024-12-04 15:09:06,697 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.9782 | Learning Rate: 0.000080
================================================================================
2024-12-04 15:09:07,929 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 1.0366 | Learning Rate: 0.000081
================================================================================
2024-12-04 15:09:09,211 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 1.0284 | Learning Rate: 0.000082
================================================================================
2024-12-04 15:09:10,301 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.9445 | Learning Rate: 0.000083
================================================================================
2024-12-04 15:09:11,242 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.9479 | Learning Rate: 0.000083
================================================================================
2024-12-04 15:09:12,157 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 1.0239 | Learning Rate: 0.000084
================================================================================
2024-12-04 15:09:13,030 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.9332 | Learning Rate: 0.000085
================================================================================
2024-12-04 15:09:14,065 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.9637 | Learning Rate: 0.000086
================================================================================
2024-12-04 15:09:15,093 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 1.0010 | Learning Rate: 0.000087
================================================================================
2024-12-04 15:09:16,233 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.9354 | Learning Rate: 0.000087
================================================================================
2024-12-04 15:09:17,303 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.9178 | Learning Rate: 0.000088
================================================================================
2024-12-04 15:09:18,671 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.9742 | Learning Rate: 0.000089
================================================================================
2024-12-04 15:09:19,834 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.9543 | Learning Rate: 0.000090
================================================================================
2024-12-04 15:09:20,981 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 1.0095 | Learning Rate: 0.000091
================================================================================
2024-12-04 15:09:22,348 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.9393 | Learning Rate: 0.000091
================================================================================
2024-12-04 15:09:23,319 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.9504 | Learning Rate: 0.000092
================================================================================
2024-12-04 15:09:24,581 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.9860 | Learning Rate: 0.000093
================================================================================
2024-12-04 15:09:25,802 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 1.0575 | Learning Rate: 0.000094
================================================================================
2024-12-04 15:09:27,002 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 1.0257 | Learning Rate: 0.000095
================================================================================
2024-12-04 15:09:28,384 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 1.0531 | Learning Rate: 0.000095
================================================================================
2024-12-04 15:09:29,586 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 1.0204 | Learning Rate: 0.000096
================================================================================
2024-12-04 15:09:30,873 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.9355 | Learning Rate: 0.000097
================================================================================
2024-12-04 15:09:32,087 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.9389 | Learning Rate: 0.000098
================================================================================
2024-12-04 15:09:33,190 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.9659 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:09:34,131 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.9509 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:09:55,462 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9841
Val Loss: 0.9094
Best Val Loss: 0.9009
Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:08,025 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:10:08,025 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:10:08,025 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:10:08,025 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:10:08,075 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:10:08,076 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:10:09,371 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.9307 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:10,257 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 0.9777 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:11,177 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.9858 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:12,073 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 1.0351 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:12,995 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.9184 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:13,996 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.8248 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:14,954 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 1.0338 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:15,843 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.9482 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:16,689 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.9648 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:17,629 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.9582 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:18,646 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.9465 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:19,497 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.9688 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:20,398 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.9349 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:21,275 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 0.9972 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:22,113 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.9703 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:23,038 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.9846 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:23,925 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.9216 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:24,847 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.8727 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:25,708 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 1.0698 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:26,526 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 1.0111 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:27,450 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.9440 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:28,272 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.9162 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:29,215 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 1.0254 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:30,080 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.9185 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:30,919 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.9368 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:31,833 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 1.0527 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:32,681 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.8722 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:33,593 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.9178 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:34,516 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 1.0237 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:35,360 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.9223 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:36,312 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 1.0038 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:37,166 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.9384 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:38,051 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 1.0177 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:38,940 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.9289 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:39,843 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.9254 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:40,736 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.8507 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:41,593 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.9229 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:42,547 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.9264 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:43,501 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.9464 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:44,426 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.9122 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:10:45,302 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.9633 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:04,600 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9542
Val Loss: 0.9441
Best Val Loss: 0.9009
Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:18,140 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:11:18,145 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:11:18,145 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:11:18,145 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:11:18,191 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:11:18,192 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:11:19,369 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.9860 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:20,382 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 1.0268 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:21,465 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.8700 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:22,598 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.9453 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:23,668 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.9458 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:24,775 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.9169 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:25,779 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.9029 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:26,845 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.9086 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:27,920 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.9044 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:28,966 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.8921 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:30,020 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.8862 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:31,128 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.9232 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:32,173 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.8954 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:33,261 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 1.0080 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:34,400 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.9586 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:35,487 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.8853 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:36,571 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.9339 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:37,628 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.9309 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:38,724 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 0.9012 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:39,768 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.9611 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:40,794 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.9640 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:41,847 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.9369 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:42,920 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.9573 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:43,998 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.9106 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:45,071 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.8172 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:46,330 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.8641 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:47,499 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.9623 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:48,794 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.9764 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:50,008 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.8654 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:51,170 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.8742 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:52,355 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.9636 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:53,501 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.9855 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:54,672 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.8999 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:55,929 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.9206 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:57,097 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.8245 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:58,414 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.9311 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:11:59,800 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 1.0596 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:01,148 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.9015 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:02,546 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.8963 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:03,936 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.9345 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:05,906 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.9246 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:26,567 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.9236
Val Loss: 0.9967
Best Val Loss: 0.9009
Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:46,045 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:12:46,045 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:12:46,045 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:12:46,046 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:12:46,090 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:12:46,090 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:12:47,725 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.8704 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:49,152 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 0.9048 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:51,043 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.8770 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:53,120 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.8358 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:55,063 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.9034 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:56,683 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.9206 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:58,094 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.9220 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:12:59,692 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.9079 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:01,292 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.8600 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:02,837 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.8981 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:04,627 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.9907 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:06,202 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.9217 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:07,728 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.9719 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:09,437 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 0.9010 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:10,869 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.8473 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:12,408 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.8985 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:14,054 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.8386 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:15,713 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.8932 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:17,355 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 0.9730 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:18,972 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.8859 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:20,626 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.8498 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:22,249 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.9074 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:24,083 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.9913 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:26,493 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.8389 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:29,075 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.8820 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:31,057 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.9070 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:32,596 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.8547 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:34,184 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.8506 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:35,730 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.9833 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:37,294 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.8738 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:38,748 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.8731 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:40,357 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.8252 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:42,412 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.9294 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:44,340 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.8871 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:46,346 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.9004 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:48,187 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.9011 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:49,898 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.8860 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:51,558 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.8965 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:53,182 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.9071 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:54,832 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.8645 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:13:56,628 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.9415 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:19,012 - training.trainer - INFO - 
============================== Epoch 5 Summary ==============================
Train Loss: 0.8968
Val Loss: 1.0628
Best Val Loss: 0.9009
Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:34,283 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:14:34,285 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:14:34,286 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:14:34,286 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:14:34,335 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:14:34,336 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:14:35,855 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.8663 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:37,245 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 0.9487 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:38,790 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.8042 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:40,439 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.8198 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:42,086 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.8276 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:43,787 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.8440 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:45,537 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.9520 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:47,300 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.8618 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:48,949 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.8466 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:50,669 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.8908 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:52,432 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.8885 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:54,198 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.8365 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:55,855 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.8774 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:57,610 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 0.8988 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:14:59,659 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.9211 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:01,464 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.9357 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:04,656 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.9015 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:06,413 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.9199 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:09,028 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 0.8839 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:11,732 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.8683 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:13,572 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.9002 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:15,548 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.7953 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:17,422 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.9245 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:19,128 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.8988 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:20,932 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.9109 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:22,443 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.9006 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:23,577 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.8865 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:24,694 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.8933 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:25,798 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.9090 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:26,868 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.8601 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:28,031 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.8691 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:29,303 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.9066 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:30,729 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.7750 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:31,914 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.7858 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:33,131 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.8537 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:34,371 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.8627 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:35,461 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.8940 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:36,693 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.9119 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:38,040 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.8836 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:39,341 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.8524 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:15:40,496 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.8634 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:01,992 - training.trainer - INFO - 
============================== Epoch 6 Summary ==============================
Train Loss: 0.8773
Val Loss: 1.1029
Best Val Loss: 0.9009
Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:18,938 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:16:18,940 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:16:18,945 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:16:18,945 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:16:18,990 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:16:18,990 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:16:22,122 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.8874 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:23,460 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 0.9003 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:25,149 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.8327 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:26,975 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.8868 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:28,670 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.8547 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:30,397 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.8779 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:31,916 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.8259 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:33,551 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.8944 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:35,220 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.9022 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:36,797 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.8982 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:38,194 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.9243 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:39,741 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.8020 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:41,173 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.9027 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:43,085 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 0.8396 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:44,834 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.7643 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:46,261 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.8923 | Learning Rate: 0.000100
================================================================================
2024-12-04 15:16:47,660 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.8471 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:16:49,386 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.8152 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:16:51,155 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 0.8769 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:16:53,596 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.8839 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:16:56,402 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.9275 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:16:59,077 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.8743 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:00,845 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.8156 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:02,466 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.8559 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:04,200 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.8677 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:05,764 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.8137 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:07,730 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.8013 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:09,685 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.8499 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:11,185 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.8431 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:12,681 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.8476 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:14,406 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.8553 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:15,884 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.8700 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:16,981 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.8533 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:18,355 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.8386 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:19,537 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.8375 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:20,642 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.8766 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:21,757 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.8635 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:22,821 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.8864 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:23,962 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.9012 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:25,175 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.8423 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:26,409 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.8734 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:17:47,153 - training.trainer - INFO - 
============================== Epoch 7 Summary ==============================
Train Loss: 0.8599
Val Loss: 1.0970
Best Val Loss: 0.9009
Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:01,888 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:18:01,891 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:18:01,892 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:18:01,892 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:18:01,938 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:18:01,939 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:18:03,393 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.9571 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:04,583 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 0.8858 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:05,957 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.8176 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:07,498 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.8437 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:09,212 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.8231 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:10,663 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.8909 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:12,448 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.8240 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:14,318 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.8746 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:16,309 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.8851 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:18,132 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.8139 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:19,596 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.8540 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:21,179 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.8232 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:22,835 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.8761 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:24,119 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 0.8486 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:26,303 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.7943 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:28,879 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.8502 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:32,017 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.8596 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:35,150 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.8216 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:37,641 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 0.8453 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:40,472 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.8447 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:42,018 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.7830 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:43,328 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.8880 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:45,483 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.8971 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:47,219 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.8324 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:49,018 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.8125 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:50,345 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.8780 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:51,651 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.8179 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:53,001 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.8696 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:54,391 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.8208 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:55,705 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.8710 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:56,926 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.7673 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:58,257 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.8630 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:18:59,657 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.8457 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:00,962 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.8413 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:02,490 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.8633 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:03,727 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.8402 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:04,936 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.8135 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:06,108 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.8182 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:07,372 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.7951 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:08,507 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.8422 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:09,909 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.8739 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:29,953 - training.trainer - INFO - 
============================== Epoch 8 Summary ==============================
Train Loss: 0.8455
Val Loss: 1.0738
Best Val Loss: 0.9009
Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:44,713 - training.trainer - INFO - 
Batch shapes:
2024-12-04 15:19:44,716 - training.trainer - INFO - Encoder inputs: torch.Size([16, 192, 8])
2024-12-04 15:19:44,716 - training.trainer - INFO - Decoder inputs: torch.Size([16, 48, 8])
2024-12-04 15:19:44,717 - training.trainer - INFO - Targets: torch.Size([16, 48, 1])
2024-12-04 15:19:44,763 - training.trainer - INFO - Model output shape: torch.Size([16, 48, 1])
2024-12-04 15:19:44,764 - training.trainer - INFO - Target shape for loss: torch.Size([16, 48, 1])
2024-12-04 15:19:46,156 - training.trainer - INFO - 
============================== Batch 50/2089 ==============================
Average Loss: 0.8735 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:47,238 - training.trainer - INFO - 
============================== Batch 100/2089 ==============================
Average Loss: 0.9516 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:48,562 - training.trainer - INFO - 
============================== Batch 150/2089 ==============================
Average Loss: 0.8137 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:49,946 - training.trainer - INFO - 
============================== Batch 200/2089 ==============================
Average Loss: 0.8566 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:51,203 - training.trainer - INFO - 
============================== Batch 250/2089 ==============================
Average Loss: 0.8629 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:52,447 - training.trainer - INFO - 
============================== Batch 300/2089 ==============================
Average Loss: 0.8633 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:53,822 - training.trainer - INFO - 
============================== Batch 350/2089 ==============================
Average Loss: 0.8227 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:55,155 - training.trainer - INFO - 
============================== Batch 400/2089 ==============================
Average Loss: 0.8116 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:56,373 - training.trainer - INFO - 
============================== Batch 450/2089 ==============================
Average Loss: 0.8017 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:57,597 - training.trainer - INFO - 
============================== Batch 500/2089 ==============================
Average Loss: 0.7693 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:19:58,858 - training.trainer - INFO - 
============================== Batch 550/2089 ==============================
Average Loss: 0.8746 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:00,125 - training.trainer - INFO - 
============================== Batch 600/2089 ==============================
Average Loss: 0.8421 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:01,396 - training.trainer - INFO - 
============================== Batch 650/2089 ==============================
Average Loss: 0.8429 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:02,623 - training.trainer - INFO - 
============================== Batch 700/2089 ==============================
Average Loss: 0.8436 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:03,930 - training.trainer - INFO - 
============================== Batch 750/2089 ==============================
Average Loss: 0.8883 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:05,047 - training.trainer - INFO - 
============================== Batch 800/2089 ==============================
Average Loss: 0.8511 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:06,308 - training.trainer - INFO - 
============================== Batch 850/2089 ==============================
Average Loss: 0.7720 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:07,558 - training.trainer - INFO - 
============================== Batch 900/2089 ==============================
Average Loss: 0.9445 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:09,003 - training.trainer - INFO - 
============================== Batch 950/2089 ==============================
Average Loss: 0.7633 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:10,337 - training.trainer - INFO - 
============================== Batch 1000/2089 ==============================
Average Loss: 0.8195 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:11,607 - training.trainer - INFO - 
============================== Batch 1050/2089 ==============================
Average Loss: 0.8597 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:12,850 - training.trainer - INFO - 
============================== Batch 1100/2089 ==============================
Average Loss: 0.8815 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:14,303 - training.trainer - INFO - 
============================== Batch 1150/2089 ==============================
Average Loss: 0.7914 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:15,837 - training.trainer - INFO - 
============================== Batch 1200/2089 ==============================
Average Loss: 0.7809 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:17,080 - training.trainer - INFO - 
============================== Batch 1250/2089 ==============================
Average Loss: 0.9031 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:18,362 - training.trainer - INFO - 
============================== Batch 1300/2089 ==============================
Average Loss: 0.7744 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:19,775 - training.trainer - INFO - 
============================== Batch 1350/2089 ==============================
Average Loss: 0.8553 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:21,198 - training.trainer - INFO - 
============================== Batch 1400/2089 ==============================
Average Loss: 0.7794 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:22,466 - training.trainer - INFO - 
============================== Batch 1450/2089 ==============================
Average Loss: 0.8623 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:23,745 - training.trainer - INFO - 
============================== Batch 1500/2089 ==============================
Average Loss: 0.8296 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:25,185 - training.trainer - INFO - 
============================== Batch 1550/2089 ==============================
Average Loss: 0.8592 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:26,615 - training.trainer - INFO - 
============================== Batch 1600/2089 ==============================
Average Loss: 0.8604 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:28,015 - training.trainer - INFO - 
============================== Batch 1650/2089 ==============================
Average Loss: 0.7867 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:29,357 - training.trainer - INFO - 
============================== Batch 1700/2089 ==============================
Average Loss: 0.7783 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:30,831 - training.trainer - INFO - 
============================== Batch 1750/2089 ==============================
Average Loss: 0.8420 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:32,436 - training.trainer - INFO - 
============================== Batch 1800/2089 ==============================
Average Loss: 0.8102 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:34,028 - training.trainer - INFO - 
============================== Batch 1850/2089 ==============================
Average Loss: 0.7528 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:35,572 - training.trainer - INFO - 
============================== Batch 1900/2089 ==============================
Average Loss: 0.7937 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:38,360 - training.trainer - INFO - 
============================== Batch 1950/2089 ==============================
Average Loss: 0.8377 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:41,268 - training.trainer - INFO - 
============================== Batch 2000/2089 ==============================
Average Loss: 0.8918 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:20:44,655 - training.trainer - INFO - 
============================== Batch 2050/2089 ==============================
Average Loss: 0.8448 | Learning Rate: 0.000099
================================================================================
2024-12-04 15:21:08,348 - training.trainer - INFO - 
============================== Epoch 9 Summary ==============================
Train Loss: 0.8335
Val Loss: 1.0987
Best Val Loss: 0.9009
Learning Rate: 0.000099
================================================================================
