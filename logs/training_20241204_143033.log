2024-12-04 14:30:35,369 - __main__ - INFO - CUDA Environment:
2024-12-04 14:30:35,369 - __main__ - INFO - {'cuda_available': True, 'cuda_version': '11.8', 'gpu_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'pytorch_version': '2.0.1', 'python_version': '3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:15:49) [MSC v.1941 64 bit (AMD64)]', 'gpu_count': 1, 'current_device': 0, 'memory_allocated': '0.00 GB', 'memory_cached': '0.00 GB'}
2024-12-04 14:30:35,369 - __main__ - INFO - Loading and preprocessing data...
2024-12-04 14:30:36,716 - __main__ - INFO - Starting training...
2024-12-04 14:30:36,733 - training.trainer - INFO - 
================================================================================
2024-12-04 14:30:36,733 - training.trainer - INFO - Starting Training
2024-12-04 14:30:36,734 - training.trainer - INFO - ================================================================================
2024-12-04 14:30:49,561 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:30:49,561 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:30:49,561 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:30:49,576 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:30:49,889 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:30:49,889 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:30:51,941 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 1.3272 | Learning Rate: 0.000002
================================================================================
2024-12-04 14:30:57,731 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 1.0932 | Learning Rate: 0.000003
================================================================================
2024-12-04 14:30:59,269 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 1.0843 | Learning Rate: 0.000005
================================================================================
2024-12-04 14:31:00,813 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 1.1545 | Learning Rate: 0.000006
================================================================================
2024-12-04 14:31:02,291 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 1.0884 | Learning Rate: 0.000008
================================================================================
2024-12-04 14:31:03,856 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 1.0727 | Learning Rate: 0.000010
================================================================================
2024-12-04 14:31:05,347 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 1.1085 | Learning Rate: 0.000011
================================================================================
2024-12-04 14:31:06,875 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 1.1210 | Learning Rate: 0.000013
================================================================================
2024-12-04 14:31:08,434 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 1.0562 | Learning Rate: 0.000014
================================================================================
2024-12-04 14:31:10,062 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 1.1172 | Learning Rate: 0.000016
================================================================================
2024-12-04 14:31:11,661 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 1.0709 | Learning Rate: 0.000018
================================================================================
2024-12-04 14:31:13,159 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 1.0930 | Learning Rate: 0.000019
================================================================================
2024-12-04 14:31:14,681 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 1.1140 | Learning Rate: 0.000021
================================================================================
2024-12-04 14:31:16,157 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 1.0086 | Learning Rate: 0.000022
================================================================================
2024-12-04 14:31:17,717 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 1.0672 | Learning Rate: 0.000024
================================================================================
2024-12-04 14:31:19,256 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 1.0491 | Learning Rate: 0.000026
================================================================================
2024-12-04 14:31:20,760 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 1.0400 | Learning Rate: 0.000027
================================================================================
2024-12-04 14:31:22,268 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 1.0548 | Learning Rate: 0.000029
================================================================================
2024-12-04 14:31:23,803 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 1.1011 | Learning Rate: 0.000030
================================================================================
2024-12-04 14:31:25,318 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 1.0525 | Learning Rate: 0.000032
================================================================================
2024-12-04 14:31:49,762 - training.trainer - INFO - 
============================== Epoch 0 Summary ==============================
Train Loss: 1.0928
Val Loss: 0.9573
Best Val Loss: inf
Learning Rate: 0.000033
================================================================================
2024-12-04 14:31:49,762 - training.trainer - INFO - New best model saved! (Val Loss: 0.9573)
2024-12-04 14:31:49,864 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 14:32:03,124 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:32:03,124 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:32:03,124 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:32:03,124 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:32:03,722 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:32:03,722 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:32:06,169 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 1.0704 | Learning Rate: 0.000035
================================================================================
2024-12-04 14:32:11,400 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 1.0865 | Learning Rate: 0.000037
================================================================================
2024-12-04 14:32:12,865 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 1.0255 | Learning Rate: 0.000038
================================================================================
2024-12-04 14:32:14,469 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 1.0519 | Learning Rate: 0.000040
================================================================================
2024-12-04 14:32:15,996 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 1.0533 | Learning Rate: 0.000041
================================================================================
2024-12-04 14:32:17,564 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 1.0342 | Learning Rate: 0.000043
================================================================================
2024-12-04 14:32:18,994 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.9913 | Learning Rate: 0.000044
================================================================================
2024-12-04 14:32:20,490 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 1.0170 | Learning Rate: 0.000046
================================================================================
2024-12-04 14:32:22,064 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 1.0643 | Learning Rate: 0.000048
================================================================================
2024-12-04 14:32:23,517 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.9960 | Learning Rate: 0.000049
================================================================================
2024-12-04 14:32:25,019 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 1.0059 | Learning Rate: 0.000051
================================================================================
2024-12-04 14:32:26,537 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 1.0505 | Learning Rate: 0.000052
================================================================================
2024-12-04 14:32:28,071 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 1.0739 | Learning Rate: 0.000054
================================================================================
2024-12-04 14:32:29,564 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 1.0114 | Learning Rate: 0.000056
================================================================================
2024-12-04 14:32:31,099 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 1.0029 | Learning Rate: 0.000057
================================================================================
2024-12-04 14:32:32,693 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 1.0728 | Learning Rate: 0.000059
================================================================================
2024-12-04 14:32:34,226 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 1.0137 | Learning Rate: 0.000060
================================================================================
2024-12-04 14:32:35,764 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.9503 | Learning Rate: 0.000062
================================================================================
2024-12-04 14:32:37,314 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.9899 | Learning Rate: 0.000064
================================================================================
2024-12-04 14:32:39,033 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.9533 | Learning Rate: 0.000065
================================================================================
2024-12-04 14:33:04,747 - training.trainer - INFO - 
============================== Epoch 1 Summary ==============================
Train Loss: 1.0276
Val Loss: 0.9115
Best Val Loss: 0.9573
Learning Rate: 0.000067
================================================================================
2024-12-04 14:33:04,747 - training.trainer - INFO - New best model saved! (Val Loss: 0.9115)
2024-12-04 14:33:04,852 - training.trainer - INFO - Saved best model checkpoint to checkpoints\best_model.pth
2024-12-04 14:33:20,400 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:33:20,400 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:33:20,400 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:33:20,400 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:33:21,179 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:33:21,179 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:33:24,189 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 1.0044 | Learning Rate: 0.000068
================================================================================
2024-12-04 14:33:29,983 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 1.0331 | Learning Rate: 0.000070
================================================================================
2024-12-04 14:33:31,504 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.9964 | Learning Rate: 0.000071
================================================================================
2024-12-04 14:33:32,978 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.9752 | Learning Rate: 0.000073
================================================================================
2024-12-04 14:33:34,717 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 1.0141 | Learning Rate: 0.000075
================================================================================
2024-12-04 14:33:36,269 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.9941 | Learning Rate: 0.000076
================================================================================
2024-12-04 14:33:37,771 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.9642 | Learning Rate: 0.000078
================================================================================
2024-12-04 14:33:39,310 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.9612 | Learning Rate: 0.000079
================================================================================
2024-12-04 14:33:40,944 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.9829 | Learning Rate: 0.000081
================================================================================
2024-12-04 14:33:42,471 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.9945 | Learning Rate: 0.000083
================================================================================
2024-12-04 14:33:44,071 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.9689 | Learning Rate: 0.000084
================================================================================
2024-12-04 14:33:45,750 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.9963 | Learning Rate: 0.000086
================================================================================
2024-12-04 14:33:47,296 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 1.0247 | Learning Rate: 0.000087
================================================================================
2024-12-04 14:33:48,864 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.9777 | Learning Rate: 0.000089
================================================================================
2024-12-04 14:33:50,476 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.9645 | Learning Rate: 0.000091
================================================================================
2024-12-04 14:33:52,003 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.9943 | Learning Rate: 0.000092
================================================================================
2024-12-04 14:33:53,517 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.9626 | Learning Rate: 0.000094
================================================================================
2024-12-04 14:33:55,136 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.9590 | Learning Rate: 0.000095
================================================================================
2024-12-04 14:33:56,671 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.9577 | Learning Rate: 0.000097
================================================================================
2024-12-04 14:33:58,210 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 1.0319 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:34:22,023 - training.trainer - INFO - 
============================== Epoch 2 Summary ==============================
Train Loss: 0.9895
Val Loss: 0.9263
Best Val Loss: 0.9115
Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:35,155 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:34:35,155 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:34:35,155 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:34:35,155 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:34:35,705 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:34:35,705 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:34:38,110 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 1.0142 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:39,589 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.9948 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:41,086 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 1.0216 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:42,607 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.9882 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:44,086 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 1.0312 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:45,690 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.9451 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:47,510 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.9514 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:49,206 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.9867 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:50,797 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.9043 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:52,634 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 1.0183 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:54,379 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.9575 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:56,091 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.9280 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:57,736 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8868 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:34:59,378 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8872 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:00,991 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 1.0153 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:02,607 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.9798 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:04,320 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.9496 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:06,020 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.9928 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:07,626 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.9602 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:09,222 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.9456 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:35,707 - training.trainer - INFO - 
============================== Epoch 3 Summary ==============================
Train Loss: 0.9667
Val Loss: 0.9439
Best Val Loss: 0.9115
Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:48,429 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:35:48,429 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:35:48,429 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:35:48,429 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:35:48,514 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:35:48,514 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:35:50,792 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.9905 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:56,603 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.9875 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:58,174 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.9373 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:35:59,715 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.9729 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:01,325 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.8995 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:02,875 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.9445 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:04,429 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.9356 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:05,936 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.9533 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:07,527 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.9448 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:09,171 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.9583 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:10,701 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8813 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:12,260 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.9437 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:13,872 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.9265 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:15,482 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8952 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:17,038 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.9552 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:18,634 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.9232 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:20,344 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.9292 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:21,981 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.9266 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:23,544 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.9544 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:25,134 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.8884 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:36:48,675 - training.trainer - INFO - 
============================== Epoch 4 Summary ==============================
Train Loss: 0.9382
Val Loss: 0.9414
Best Val Loss: 0.9115
Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:01,387 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:37:01,403 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:37:01,403 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:37:01,403 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:37:01,485 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:37:01,485 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:37:04,107 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.9319 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:09,996 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.9237 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:11,896 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.9012 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:13,629 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.8890 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:15,396 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.9578 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:17,091 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.9458 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:18,788 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8987 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:20,427 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.9075 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:22,089 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.8773 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:23,894 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8944 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:25,610 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8539 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:27,324 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.9164 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:29,097 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8926 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:30,767 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8620 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:32,428 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.9448 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:34,130 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.8836 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:35,821 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.9542 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:37,548 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.9110 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:39,250 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.9117 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:37:40,981 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.9542 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:03,637 - training.trainer - INFO - 
============================== Epoch 5 Summary ==============================
Train Loss: 0.9086
Val Loss: 0.9851
Best Val Loss: 0.9115
Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:16,274 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:38:16,281 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:38:16,281 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:38:16,281 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:38:16,853 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:38:16,854 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:38:19,558 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.8843 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:25,450 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.8602 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:27,221 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.9323 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:28,972 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.8804 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:30,712 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.8958 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:32,325 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.8320 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:33,922 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8498 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:35,579 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.8947 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:37,231 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.9039 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:38,978 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8585 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:40,600 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8703 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:42,265 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.8855 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:43,890 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8993 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:45,598 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.9138 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:47,321 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.9154 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:49,084 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.9107 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:50,827 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.8550 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:52,576 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.8555 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:54,258 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.8689 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:38:55,879 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.8946 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:19,829 - training.trainer - INFO - 
============================== Epoch 6 Summary ==============================
Train Loss: 0.8850
Val Loss: 1.0863
Best Val Loss: 0.9115
Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:32,961 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:39:32,961 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:39:32,961 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:39:32,961 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:39:33,524 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:39:33,524 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:39:35,511 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.8298 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:40,707 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.8833 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:42,217 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.8782 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:43,813 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.8942 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:45,325 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.8889 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:46,931 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.8584 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:48,502 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8718 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:50,152 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.8296 | Learning Rate: 0.000100
================================================================================
2024-12-04 14:39:51,893 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.8987 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:39:53,493 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8907 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:39:55,056 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8564 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:39:56,643 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.8759 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:39:58,290 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8507 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:39:59,824 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8394 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:01,531 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.8884 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:03,198 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.8374 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:04,874 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.8541 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:06,477 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.8761 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:08,183 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.8824 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:09,801 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.9048 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:29,482 - training.trainer - INFO - 
============================== Epoch 7 Summary ==============================
Train Loss: 0.8673
Val Loss: 1.0320
Best Val Loss: 0.9115
Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:41,876 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:40:41,876 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:40:41,876 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:40:41,876 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:40:42,444 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:40:42,444 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:40:45,029 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.8890 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:50,274 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.8801 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:51,821 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.8526 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:53,420 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.8562 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:55,065 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.7717 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:56,724 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.8337 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:40:58,405 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8597 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:00,149 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.8752 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:01,870 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.8622 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:03,592 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8747 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:05,246 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8647 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:06,954 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.8468 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:08,593 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8290 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:10,200 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8456 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:11,772 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.8364 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:13,424 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.8402 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:15,102 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.8518 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:16,711 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.9042 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:18,298 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.8257 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:19,986 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.8372 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:43,740 - training.trainer - INFO - 
============================== Epoch 8 Summary ==============================
Train Loss: 0.8513
Val Loss: 1.0166
Best Val Loss: 0.9115
Learning Rate: 0.000099
================================================================================
2024-12-04 14:41:55,975 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:41:55,980 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:41:55,980 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:41:55,980 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:41:56,049 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:41:56,049 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:41:59,876 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.8379 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:04,028 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.8527 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:05,623 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.8409 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:07,283 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.7971 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:08,818 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.8239 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:10,395 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.8581 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:12,041 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8293 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:13,547 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.8633 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:15,161 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.8105 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:16,749 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8614 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:18,255 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8508 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:19,859 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.8322 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:21,498 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8707 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:23,060 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8461 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:24,578 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.8165 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:26,151 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.8317 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:27,720 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.8394 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:29,206 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.8382 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:30,740 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.8429 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:32,325 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.8525 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:42:54,307 - training.trainer - INFO - 
============================== Epoch 9 Summary ==============================
Train Loss: 0.8400
Val Loss: 1.0137
Best Val Loss: 0.9115
Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:08,185 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:43:08,185 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:43:08,185 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:43:08,185 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:43:08,721 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:43:08,721 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:43:14,909 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.7694 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:17,733 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.8501 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:23,401 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.8084 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:26,287 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.7724 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:29,036 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.8168 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:31,641 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.8342 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:35,862 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8454 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:41,523 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.8223 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:44,437 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.8047 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:47,011 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8510 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:49,706 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8596 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:55,101 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.8302 | Learning Rate: 0.000099
================================================================================
2024-12-04 14:43:57,691 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8665 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:00,485 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.8379 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:02,941 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.8243 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:09,263 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.8399 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:12,157 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.7993 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:15,700 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.8127 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:18,220 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.8510 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:24,080 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.8243 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:44:51,989 - training.trainer - INFO - 
============================== Epoch 10 Summary ==============================
Train Loss: 0.8269
Val Loss: 0.9770
Best Val Loss: 0.9115
Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:04,123 - training.trainer - INFO - 
Batch shapes:
2024-12-04 14:45:04,123 - training.trainer - INFO - Encoder inputs: torch.Size([32, 192, 8])
2024-12-04 14:45:04,123 - training.trainer - INFO - Decoder inputs: torch.Size([32, 48, 8])
2024-12-04 14:45:04,123 - training.trainer - INFO - Targets: torch.Size([32, 48, 1])
2024-12-04 14:45:04,676 - training.trainer - INFO - Model output shape: torch.Size([32, 48, 1])
2024-12-04 14:45:04,676 - training.trainer - INFO - Target shape for loss: torch.Size([32, 48, 1])
2024-12-04 14:45:07,192 - training.trainer - INFO - 
============================== Batch 50/1045 ==============================
Average Loss: 0.8280 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:12,854 - training.trainer - INFO - 
============================== Batch 100/1045 ==============================
Average Loss: 0.8029 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:16,192 - training.trainer - INFO - 
============================== Batch 150/1045 ==============================
Average Loss: 0.8484 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:18,819 - training.trainer - INFO - 
============================== Batch 200/1045 ==============================
Average Loss: 0.8182 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:21,874 - training.trainer - INFO - 
============================== Batch 250/1045 ==============================
Average Loss: 0.8136 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:24,563 - training.trainer - INFO - 
============================== Batch 300/1045 ==============================
Average Loss: 0.8376 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:30,958 - training.trainer - INFO - 
============================== Batch 350/1045 ==============================
Average Loss: 0.8113 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:33,546 - training.trainer - INFO - 
============================== Batch 400/1045 ==============================
Average Loss: 0.8404 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:39,130 - training.trainer - INFO - 
============================== Batch 450/1045 ==============================
Average Loss: 0.7923 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:41,940 - training.trainer - INFO - 
============================== Batch 500/1045 ==============================
Average Loss: 0.8025 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:44,787 - training.trainer - INFO - 
============================== Batch 550/1045 ==============================
Average Loss: 0.8768 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:48,003 - training.trainer - INFO - 
============================== Batch 600/1045 ==============================
Average Loss: 0.8378 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:52,183 - training.trainer - INFO - 
============================== Batch 650/1045 ==============================
Average Loss: 0.8351 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:56,568 - training.trainer - INFO - 
============================== Batch 700/1045 ==============================
Average Loss: 0.7509 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:45:59,479 - training.trainer - INFO - 
============================== Batch 750/1045 ==============================
Average Loss: 0.8408 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:02,152 - training.trainer - INFO - 
============================== Batch 800/1045 ==============================
Average Loss: 0.7566 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:04,417 - training.trainer - INFO - 
============================== Batch 850/1045 ==============================
Average Loss: 0.8005 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:10,686 - training.trainer - INFO - 
============================== Batch 900/1045 ==============================
Average Loss: 0.7836 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:13,499 - training.trainer - INFO - 
============================== Batch 950/1045 ==============================
Average Loss: 0.7666 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:19,865 - training.trainer - INFO - 
============================== Batch 1000/1045 ==============================
Average Loss: 0.8922 | Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:50,292 - training.trainer - INFO - 
============================== Epoch 11 Summary ==============================
Train Loss: 0.8168
Val Loss: 1.0515
Best Val Loss: 0.9115
Learning Rate: 0.000098
================================================================================
2024-12-04 14:46:50,406 - training.trainer - INFO - 
============================== Early Stopping ==============================
No improvement for 10 epochs
Best Val Loss: 0.9115
================================================================================
2024-12-04 14:46:50,416 - training.trainer - INFO - 
================================================================================
2024-12-04 14:46:50,416 - training.trainer - INFO - Training Completed!
2024-12-04 14:46:50,416 - training.trainer - INFO - ================================================================================
2024-12-04 14:46:50,416 - __main__ - INFO - Evaluating model...
