{
    "sourceFile": "src/utils/visualization.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1733311728768,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733311945717,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,190 @@\n+\"\"\"\r\n+Visualization utilities for time series forecasting.\r\n+\r\n+Dependencies:\r\n+- matplotlib>=3.7.2\r\n+- seaborn>=0.12.2\r\n+\"\"\"\r\n+\r\n+import matplotlib.pyplot as plt\r\n+import seaborn as sns\r\n+import numpy as np\r\n+from typing import List, Optional, Tuple\r\n+import torch\r\n+\r\n+class TimeSeriesVisualizer:\r\n+    \"\"\"Visualization tools for time series data and model results\"\"\"\r\n+    \r\n+    def __init__(self, figsize: Tuple[int, int] = (12, 6)):\r\n+        \"\"\"\r\n+        Initialize visualizer\r\n+        \r\n+        Args:\r\n+            figsize: Default figure size for plots\r\n+        \"\"\"\r\n+        self.figsize = figsize\r\n+        # Set style\r\n+        sns.set_style(\"whitegrid\")\r\n+        plt.rcParams['figure.figsize'] = figsize\r\n+        \r\n+    def plot_training_history(\r\n+        self,\r\n+        train_losses: List[float],\r\n+        val_losses: List[float],\r\n+        title: str = \"Training History\"\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot training and validation losses\r\n+        \r\n+        Args:\r\n+            train_losses: List of training losses\r\n+            val_losses: List of validation losses\r\n+            title: Plot title\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        fig, ax = plt.subplots(figsize=self.figsize)\r\n+        epochs = range(1, len(train_losses) + 1)\r\n+        \r\n+        ax.plot(epochs, train_losses, 'b-', label='Training Loss')\r\n+        ax.plot(epochs, val_losses, 'r-', label='Validation Loss')\r\n+        \r\n+        ax.set_title(title)\r\n+        ax.set_xlabel('Epoch')\r\n+        ax.set_ylabel('Loss')\r\n+        ax.legend()\r\n+        ax.grid(True)\r\n+        \r\n+        return fig\r\n+    \r\n+    def plot_predictions(\r\n+        self,\r\n+        true_values: torch.Tensor,\r\n+        predictions: torch.Tensor,\r\n+        timestamps: Optional[List] = None,\r\n+        title: str = \"Predictions vs Actual\"\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot predictions against actual values\r\n+        \r\n+        Args:\r\n+            true_values: Ground truth values\r\n+            predictions: Model predictions\r\n+            timestamps: Optional list of timestamps\r\n+            title: Plot title\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        # Convert to numpy if tensors\r\n+        if isinstance(true_values, torch.Tensor):\r\n+            true_values = true_values.cpu().numpy()\r\n+        if isinstance(predictions, torch.Tensor):\r\n+            predictions = predictions.cpu().numpy()\r\n+            \r\n+        fig, ax = plt.subplots(figsize=self.figsize)\r\n+        \r\n+        x_values = timestamps if timestamps is not None else range(len(true_values))\r\n+        \r\n+        ax.plot(x_values, true_values, 'b-', label='Actual', alpha=0.7)\r\n+        ax.plot(x_values, predictions, 'r--', label='Predicted', alpha=0.7)\r\n+        \r\n+        ax.set_title(title)\r\n+        ax.set_xlabel('Time' if timestamps is None else 'Timestamp')\r\n+        ax.set_ylabel('Value')\r\n+        ax.legend()\r\n+        ax.grid(True)\r\n+        \r\n+        # Rotate x-axis labels if timestamps are provided\r\n+        if timestamps is not None:\r\n+            plt.xticks(rotation=45)\r\n+            \r\n+        plt.tight_layout()\r\n+        return fig\r\n+    \r\n+    def plot_attention_weights(\r\n+        self,\r\n+        attention_weights: torch.Tensor,\r\n+        index: int = 0\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot attention weights heatmap\r\n+        \r\n+        Args:\r\n+            attention_weights: Attention weights tensor [batch, heads, seq_len, seq_len]\r\n+            index: Batch index to plot\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        # Get weights for specified batch\r\n+        weights = attention_weights[index].cpu().numpy()\r\n+        \r\n+        # Create subplot for each attention head\r\n+        n_heads = weights.shape[0]\r\n+        fig, axes = plt.subplots(\r\n+            1, n_heads,\r\n+            figsize=(4 * n_heads, 4),\r\n+            squeeze=False\r\n+        )\r\n+        \r\n+        for i, ax in enumerate(axes[0]):\r\n+            sns.heatmap(\r\n+                weights[i],\r\n+                ax=ax,\r\n+                cmap='viridis',\r\n+                cbar=True\r\n+            )\r\n+            ax.set_title(f'Head {i+1}')\r\n+            ax.set_xlabel('Key')\r\n+            ax.set_ylabel('Query')\r\n+            \r\n+        plt.tight_layout()\r\n+        return fig\r\n+    \r\n+    def plot_feature_importance(\r\n+        self,\r\n+        importance_scores: np.ndarray,\r\n+        feature_names: List[str]\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot feature importance scores\r\n+        \r\n+        Args:\r\n+            importance_scores: Array of importance scores\r\n+            feature_names: List of feature names\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        fig, ax = plt.subplots(figsize=self.figsize)\r\n+        \r\n+        # Sort by importance\r\n+        sorted_idx = np.argsort(importance_scores)\r\n+        pos = np.arange(sorted_idx.shape[0]) + .5\r\n+        \r\n+        ax.barh(pos, importance_scores[sorted_idx])\r\n+        ax.set_yticks(pos)\r\n+        ax.set_yticklabels(np.array(feature_names)[sorted_idx])\r\n+        ax.set_xlabel('Importance Score')\r\n+        ax.set_title('Feature Importance')\r\n+        \r\n+        plt.tight_layout()\r\n+        return fig\r\n+    \r\n+    @staticmethod\r\n+    def save_figure(\r\n+        fig: plt.Figure,\r\n+        filename: str,\r\n+        dpi: int = 300\r\n+    ):\r\n+        \"\"\"\r\n+        Save figure to file\r\n+        \r\n+        Args:\r\n+            fig: matplotlib figure\r\n+            filename: Output filename\r\n+            dpi: Resolution in dots per inch\r\n+        \"\"\"\r\n+        fig.savefig(filename, dpi=dpi, bbox_inches='tight')\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733343947813,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,195 +186,283 @@\n             fig: matplotlib figure\r\n             filename: Output filename\r\n             dpi: Resolution in dots per inch\r\n         \"\"\"\r\n-        fig.savefig(filename, dpi=dpi, bbox_inches='tight')\n-\"\"\"\r\n-Visualization utilities for time series forecasting.\r\n+        fig.savefig(filename, dpi=dpi, bbox_inches='tight')\r\n+        \r\n+        \r\n \r\n-Dependencies:\r\n-- matplotlib>=3.7.2\r\n-- seaborn>=0.12.2\r\n-\"\"\"\r\n-\r\n+import os\r\n+import torch\r\n+import numpy as np\r\n import matplotlib.pyplot as plt\r\n-import seaborn as sns\r\n-import numpy as np\r\n-from typing import List, Optional, Tuple\r\n-import torch\r\n+from datetime import datetime, timedelta\r\n+import pandas as pd\r\n+from typing import Tuple, Optional, List\r\n \r\n-class TimeSeriesVisualizer:\r\n-    \"\"\"Visualization tools for time series data and model results\"\"\"\r\n+class PredictionVisualizer:\r\n+    \"\"\"Handles visualization of model predictions with input windows\"\"\"\r\n     \r\n-    def __init__(self, figsize: Tuple[int, int] = (12, 6)):\r\n+    def __init__(\r\n+            self,\r\n+            output_dir: str = \"prediction_plots\",\r\n+            fig_size: Tuple[int, int] = (15, 7),\r\n+            dpi: int = 300\r\n+    ):\r\n         \"\"\"\r\n         Initialize visualizer\r\n         \r\n         Args:\r\n-            figsize: Default figure size for plots\r\n+            output_dir: Directory to save plots\r\n+            fig_size: Figure size for plots\r\n+            dpi: DPI for saved figures\r\n         \"\"\"\r\n-        self.figsize = figsize\r\n+        self.output_dir = output_dir\r\n+        self.fig_size = fig_size\r\n+        self.dpi = dpi\r\n+        \r\n+        # Create output directory if it doesn't exist\r\n+        os.makedirs(output_dir, exist_ok=True)\r\n+        \r\n         # Set style\r\n-        sns.set_style(\"whitegrid\")\r\n-        plt.rcParams['figure.figsize'] = figsize\r\n-        \r\n-    def plot_training_history(\r\n-        self,\r\n-        train_losses: List[float],\r\n-        val_losses: List[float],\r\n-        title: str = \"Training History\"\r\n-    ) -> plt.Figure:\r\n+        plt.style.use('seaborn')\r\n+    \r\n+    def _generate_timestamps(\r\n+            self,\r\n+            start_time: datetime,\r\n+            sequence_length: int,\r\n+            freq: str = 'H'\r\n+    ) -> List[datetime]:\r\n         \"\"\"\r\n-        Plot training and validation losses\r\n+        Generate timestamps for x-axis\r\n         \r\n         Args:\r\n-            train_losses: List of training losses\r\n-            val_losses: List of validation losses\r\n-            title: Plot title\r\n+            start_time: Starting timestamp\r\n+            sequence_length: Number of timestamps to generate\r\n+            freq: Frequency of timestamps ('H' for hourly)\r\n             \r\n         Returns:\r\n-            matplotlib figure\r\n+            List of timestamps\r\n         \"\"\"\r\n-        fig, ax = plt.subplots(figsize=self.figsize)\r\n-        epochs = range(1, len(train_losses) + 1)\r\n-        \r\n-        ax.plot(epochs, train_losses, 'b-', label='Training Loss')\r\n-        ax.plot(epochs, val_losses, 'r-', label='Validation Loss')\r\n-        \r\n-        ax.set_title(title)\r\n-        ax.set_xlabel('Epoch')\r\n-        ax.set_ylabel('Loss')\r\n-        ax.legend()\r\n-        ax.grid(True)\r\n-        \r\n-        return fig\r\n+        try:\r\n+            timestamps = [start_time + timedelta(hours=i) for i in range(sequence_length)]\r\n+            return timestamps\r\n+        except Exception as e:\r\n+            print(f\"Error generating timestamps: {str(e)}\")\r\n+            # Return fallback numeric x-axis\r\n+            return list(range(sequence_length))\r\n     \r\n-    def plot_predictions(\r\n-        self,\r\n-        true_values: torch.Tensor,\r\n-        predictions: torch.Tensor,\r\n-        timestamps: Optional[List] = None,\r\n-        title: str = \"Predictions vs Actual\"\r\n-    ) -> plt.Figure:\r\n+    def plot_prediction_sample(\r\n+            self,\r\n+            input_seq: torch.Tensor,\r\n+            actual_seq: torch.Tensor,\r\n+            predicted_seq: torch.Tensor,\r\n+            sample_id: int,\r\n+            start_time: Optional[datetime] = None,\r\n+            scaler = None\r\n+    ):\r\n         \"\"\"\r\n-        Plot predictions against actual values\r\n+        Create and save a plot showing input, actual and predicted values\r\n         \r\n         Args:\r\n-            true_values: Ground truth values\r\n-            predictions: Model predictions\r\n-            timestamps: Optional list of timestamps\r\n-            title: Plot title\r\n-            \r\n-        Returns:\r\n-            matplotlib figure\r\n+            input_seq: Input sequence tensor\r\n+            actual_seq: Actual values tensor\r\n+            predicted_seq: Predicted values tensor\r\n+            sample_id: Sample identifier for filename\r\n+            start_time: Starting timestamp (optional)\r\n+            scaler: Scaler object for inverse transform (optional)\r\n         \"\"\"\r\n-        # Convert to numpy if tensors\r\n-        if isinstance(true_values, torch.Tensor):\r\n-            true_values = true_values.cpu().numpy()\r\n-        if isinstance(predictions, torch.Tensor):\r\n-            predictions = predictions.cpu().numpy()\r\n+        try:\r\n+            # Move tensors to CPU and convert to numpy\r\n+            input_seq = input_seq.cpu().numpy()\r\n+            actual_seq = actual_seq.cpu().numpy()\r\n+            predicted_seq = predicted_seq.cpu().numpy()\r\n             \r\n-        fig, ax = plt.subplots(figsize=self.figsize)\r\n-        \r\n-        x_values = timestamps if timestamps is not None else range(len(true_values))\r\n-        \r\n-        ax.plot(x_values, true_values, 'b-', label='Actual', alpha=0.7)\r\n-        ax.plot(x_values, predictions, 'r--', label='Predicted', alpha=0.7)\r\n-        \r\n-        ax.set_title(title)\r\n-        ax.set_xlabel('Time' if timestamps is None else 'Timestamp')\r\n-        ax.set_ylabel('Value')\r\n-        ax.legend()\r\n-        ax.grid(True)\r\n-        \r\n-        # Rotate x-axis labels if timestamps are provided\r\n-        if timestamps is not None:\r\n-            plt.xticks(rotation=45)\r\n+            # If we have a scaler, inverse transform the values\r\n+            if scaler is not None:\r\n+                try:\r\n+                    input_seq = scaler.inverse_transform(input_seq.reshape(-1, 1)).reshape(input_seq.shape)\r\n+                    actual_seq = scaler.inverse_transform(actual_seq.reshape(-1, 1)).reshape(actual_seq.shape)\r\n+                    predicted_seq = scaler.inverse_transform(predicted_seq.reshape(-1, 1)).reshape(predicted_seq.shape)\r\n+                except Exception as e:\r\n+                    print(f\"Warning: Could not apply inverse transform: {str(e)}\")\r\n             \r\n-        plt.tight_layout()\r\n-        return fig\r\n-    \r\n-    def plot_attention_weights(\r\n-        self,\r\n-        attention_weights: torch.Tensor,\r\n-        index: int = 0\r\n-    ) -> plt.Figure:\r\n-        \"\"\"\r\n-        Plot attention weights heatmap\r\n-        \r\n-        Args:\r\n\\ No newline at end of file\n-            attention_weights: Attention weights tensor [batch, heads, seq_len, seq_len]\r\n-            index: Batch index to plot\r\n+            # Get sequence lengths\r\n+            input_len = len(input_seq)\r\n+            prediction_len = len(predicted_seq)\r\n+            total_len = input_len + prediction_len\r\n             \r\n-        Returns:\r\n-            matplotlib figure\r\n-        \"\"\"\r\n-        # Get weights for specified batch\r\n-        weights = attention_weights[index].cpu().numpy()\r\n-        \r\n-        # Create subplot for each attention head\r\n-        n_heads = weights.shape[0]\r\n-        fig, axes = plt.subplots(\r\n-            1, n_heads,\r\n-            figsize=(4 * n_heads, 4),\r\n-            squeeze=False\r\n-        )\r\n-        \r\n-        for i, ax in enumerate(axes[0]):\r\n-            sns.heatmap(\r\n-                weights[i],\r\n-                ax=ax,\r\n-                cmap='viridis',\r\n-                cbar=True\r\n-            )\r\n-            ax.set_title(f'Head {i+1}')\r\n-            ax.set_xlabel('Key')\r\n-            ax.set_ylabel('Query')\r\n+            # Generate x-axis values\r\n+            if start_time is not None:\r\n+                x_values = self._generate_timestamps(start_time, total_len)\r\n+            else:\r\n+                x_values = list(range(total_len))\r\n             \r\n-        plt.tight_layout()\r\n-        return fig\r\n+            # Create figure\r\n+            plt.figure(figsize=self.fig_size)\r\n+            \r\n+            # Plot input sequence\r\n+            plt.plot(x_values[:input_len], \r\n+                    input_seq[:, 0], \r\n+                    'b-', \r\n+                    label='Input', \r\n+                    alpha=0.5)\r\n+            \r\n+            # Plot actual values\r\n+            plt.plot(x_values[input_len:], \r\n+                    actual_seq[:, 0], \r\n+                    'g-', \r\n+                    label='Actual', \r\n+                    linewidth=2)\r\n+            \r\n+            # Plot predictions\r\n+            plt.plot(x_values[input_len:], \r\n+                    predicted_seq[:, 0], \r\n+                    'r--', \r\n+                    label='Predicted', \r\n+                    linewidth=2)\r\n+            \r\n+            # Add vertical line separating input and prediction\r\n+            plt.axvline(x=x_values[input_len-1], \r\n+                       color='gray', \r\n+                       linestyle='--', \r\n+                       alpha=0.5)\r\n+            \r\n+            # Customize plot\r\n+            plt.title('Input Sequence and Predictions', pad=20)\r\n+            plt.xlabel('Time' if isinstance(x_values[0], datetime) else 'Time Step')\r\n+            plt.ylabel('Value')\r\n+            plt.legend()\r\n+            plt.grid(True, alpha=0.3)\r\n+            \r\n+            # Rotate x-axis labels if using timestamps\r\n+            if isinstance(x_values[0], datetime):\r\n+                plt.xticks(rotation=45)\r\n+            \r\n+            # Tight layout to prevent label cutoff\r\n+            plt.tight_layout()\r\n+            \r\n+            # Save plot\r\n+            filename = os.path.join(self.output_dir, f'prediction_sample_{sample_id}.png')\r\n+            plt.savefig(filename, dpi=self.dpi, bbox_inches='tight')\r\n+            plt.close()\r\n+            \r\n+            print(f\"Saved prediction plot to {filename}\")\r\n+            \r\n+        except Exception as e:\r\n+            print(f\"Error creating prediction plot: {str(e)}\")\r\n+            # Ensure figure is closed even if error occurs\r\n+            plt.close()\r\n     \r\n-    def plot_feature_importance(\r\n-        self,\r\n-        importance_scores: np.ndarray,\r\n-        feature_names: List[str]\r\n-    ) -> plt.Figure:\r\n+    def plot_multiple_samples(\r\n+            self,\r\n+            model: torch.nn.Module,\r\n+            dataset,\r\n+            num_samples: int = 5,\r\n+            scaler = None,\r\n+            device: torch.device = torch.device('cpu')\r\n+    ):\r\n         \"\"\"\r\n-        Plot feature importance scores\r\n+        Create plots for multiple random samples from dataset\r\n         \r\n         Args:\r\n-            importance_scores: Array of importance scores\r\n-            feature_names: List of feature names\r\n+            model: Trained model\r\n+            dataset: Dataset containing samples\r\n+            num_samples: Number of samples to plot\r\n+            scaler: Scaler object for inverse transform\r\n+            device: Device to run model on\r\n+        \"\"\"\r\n+        try:\r\n+            model.eval()  # Set model to evaluation mode\r\n             \r\n-        Returns:\r\n-            matplotlib figure\r\n-        \"\"\"\r\n-        fig, ax = plt.subplots(figsize=self.figsize)\r\n-        \r\n-        # Sort by importance\r\n-        sorted_idx = np.argsort(importance_scores)\r\n-        pos = np.arange(sorted_idx.shape[0]) + .5\r\n-        \r\n-        ax.barh(pos, importance_scores[sorted_idx])\r\n-        ax.set_yticks(pos)\r\n-        ax.set_yticklabels(np.array(feature_names)[sorted_idx])\r\n-        ax.set_xlabel('Importance Score')\r\n-        ax.set_title('Feature Importance')\r\n-        \r\n-        plt.tight_layout()\r\n-        return fig\r\n-    \r\n-    @staticmethod\r\n-    def save_figure(\r\n-        fig: plt.Figure,\r\n-        filename: str,\r\n-        dpi: int = 300\r\n+            # Generate random indices\r\n+            total_samples = len(dataset)\r\n+            indices = np.random.choice(total_samples, min(num_samples, total_samples), replace=False)\r\n+            \r\n+            with torch.no_grad():\r\n+                for idx in indices:\r\n+                    # Get sample from dataset\r\n+                    input_seq, decoder_input, target = dataset[idx]\r\n+                    \r\n+                    # Prepare input for model\r\n+                    input_batch = input_seq.unsqueeze(0).to(device)\r\n+                    \r\n+                    # Get prediction\r\n+                    prediction, _ = model(input_batch)\r\n+                    \r\n+                    # Remove batch dimension\r\n+                    prediction = prediction.squeeze(0)\r\n+                    \r\n+                    # Plot sample\r\n+                    self.plot_prediction_sample(\r\n+                        input_seq=input_seq,\r\n+                        actual_seq=target,\r\n+                        predicted_seq=prediction,\r\n+                        sample_id=idx,\r\n+                        scaler=scaler\r\n+                    )\r\n+                    \r\n+        except Exception as e:\r\n+            print(f\"Error plotting multiple samples: {str(e)}\")\r\n+\r\n+    def create_error_analysis_plots(\r\n+            self,\r\n+            actual_values: torch.Tensor,\r\n+            predicted_values: torch.Tensor,\r\n+            scaler = None\r\n     ):\r\n         \"\"\"\r\n-        Save figure to file\r\n+        Create error analysis plots (error distribution, scatter plot)\r\n         \r\n         Args:\r\n-            fig: matplotlib figure\r\n-            filename: Output filename\r\n-            dpi: Resolution in dots per inch\r\n+            actual_values: Tensor of actual values\r\n+            predicted_values: Tensor of predicted values\r\n+            scaler: Scaler object for inverse transform\r\n         \"\"\"\r\n-        fig.savefig(filename, dpi=dpi, bbox_inches='tight')\n+        try:\r\n+            # Convert to numpy and reshape\r\n+            actual = actual_values.cpu().numpy().reshape(-1)\r\n+            predicted = predicted_values.cpu().numpy().reshape(-1)\r\n+            \r\n+            if scaler is not None:\r\n+                try:\r\n+                    actual = scaler.inverse_transform(actual.reshape(-1, 1)).reshape(-1)\r\n+                    predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(-1)\r\n+                except Exception as e:\r\n+                    print(f\"Warning: Could not apply inverse transform: {str(e)}\")\r\n+            \r\n+            # Calculate errors\r\n+            errors = predicted - actual\r\n+            \r\n+            # Error distribution plot\r\n+            plt.figure(figsize=self.fig_size)\r\n+            plt.hist(errors, bins=50, edgecolor='black')\r\n+            plt.title('Error Distribution')\r\n+            plt.xlabel('Prediction Error')\r\n+            plt.ylabel('Frequency')\r\n+            plt.tight_layout()\r\n+            plt.savefig(os.path.join(self.output_dir, 'error_distribution.png'), \r\n+                       dpi=self.dpi, \r\n+                       bbox_inches='tight')\r\n+            plt.close()\r\n+            \r\n+            # Scatter plot\r\n+            plt.figure(figsize=self.fig_size)\r\n+            plt.scatter(actual, predicted, alpha=0.5)\r\n+            \r\n+            # Add diagonal line\r\n+            min_val = min(actual.min(), predicted.min())\r\n+            max_val = max(actual.max(), predicted.max())\r\n+            plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\r\n+            \r\n+            plt.title('Actual vs Predicted Values')\r\n+            plt.xlabel('Actual Values')\r\n+            plt.ylabel('Predicted Values')\r\n+            plt.tight_layout()\r\n+            plt.savefig(os.path.join(self.output_dir, 'actual_vs_predicted.png'), \r\n+                       dpi=self.dpi, \r\n+                       bbox_inches='tight')\r\n+            plt.close()\r\n+            \r\n+        except Exception as e:\r\n+            print(f\"Error creating analysis plots: {str(e)}\")\r\n+            plt.close()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733343963788,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -447,9 +447,9 @@\n             \r\n             # Scatter plot\r\n             plt.figure(figsize=self.fig_size)\r\n             plt.scatter(actual, predicted, alpha=0.5)\r\n-            \r\n+            z\r\n             # Add diagonal line\r\n             min_val = min(actual.min(), predicted.min())\r\n             max_val = max(actual.max(), predicted.max())\r\n             plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\r\n"
                },
                {
                    "date": 1733344009859,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,467 @@\n+\"\"\"\r\n+Visualization utilities for time series forecasting.\r\n+\r\n+Dependencies:\r\n+- matplotlib>=3.7.2\r\n+- seaborn>=0.12.2\r\n+\"\"\"\r\n+\r\n+import matplotlib.pyplot as plt\r\n+import seaborn as sns\r\n+import numpy as np\r\n+from typing import List, Optional, Tuple\r\n+import torch\r\n+\r\n+class TimeSeriesVisualizer:\r\n+    \"\"\"Visualization tools for time series data and model results\"\"\"\r\n+    \r\n+    def __init__(self, figsize: Tuple[int, int] = (12, 6)):\r\n+        \"\"\"\r\n+        Initialize visualizer\r\n+        \r\n+        Args:\r\n+            figsize: Default figure size for plots\r\n+        \"\"\"\r\n+        self.figsize = figsize\r\n+        # Set style\r\n+        sns.set_style(\"whitegrid\")\r\n+        plt.rcParams['figure.figsize'] = figsize\r\n+        \r\n+    def plot_training_history(\r\n+        self,\r\n+        train_losses: List[float],\r\n+        val_losses: List[float],\r\n+        title: str = \"Training History\"\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot training and validation losses\r\n+        \r\n+        Args:\r\n+            train_losses: List of training losses\r\n+            val_losses: List of validation losses\r\n+            title: Plot title\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        fig, ax = plt.subplots(figsize=self.figsize)\r\n+        epochs = range(1, len(train_losses) + 1)\r\n+        \r\n+        ax.plot(epochs, train_losses, 'b-', label='Training Loss')\r\n+        ax.plot(epochs, val_losses, 'r-', label='Validation Loss')\r\n+        \r\n+        ax.set_title(title)\r\n+        ax.set_xlabel('Epoch')\r\n+        ax.set_ylabel('Loss')\r\n+        ax.legend()\r\n+        ax.grid(True)\r\n+        \r\n+        return fig\r\n+    \r\n+    def plot_predictions(\r\n+        self,\r\n+        true_values: torch.Tensor,\r\n+        predictions: torch.Tensor,\r\n+        timestamps: Optional[List] = None,\r\n+        title: str = \"Predictions vs Actual\"\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot predictions against actual values\r\n+        \r\n+        Args:\r\n+            true_values: Ground truth values\r\n+            predictions: Model predictions\r\n+            timestamps: Optional list of timestamps\r\n+            title: Plot title\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        # Convert to numpy if tensors\r\n+        if isinstance(true_values, torch.Tensor):\r\n+            true_values = true_values.cpu().numpy()\r\n+        if isinstance(predictions, torch.Tensor):\r\n+            predictions = predictions.cpu().numpy()\r\n+            \r\n+        fig, ax = plt.subplots(figsize=self.figsize)\r\n+        \r\n+        x_values = timestamps if timestamps is not None else range(len(true_values))\r\n+        \r\n+        ax.plot(x_values, true_values, 'b-', label='Actual', alpha=0.7)\r\n+        ax.plot(x_values, predictions, 'r--', label='Predicted', alpha=0.7)\r\n+        \r\n+        ax.set_title(title)\r\n+        ax.set_xlabel('Time' if timestamps is None else 'Timestamp')\r\n+        ax.set_ylabel('Value')\r\n+        ax.legend()\r\n+        ax.grid(True)\r\n+        \r\n+        # Rotate x-axis labels if timestamps are provided\r\n+        if timestamps is not None:\r\n+            plt.xticks(rotation=45)\r\n+            \r\n+        plt.tight_layout()\r\n+        return fig\r\n+    \r\n+    def plot_attention_weights(\r\n+        self,\r\n+        attention_weights: torch.Tensor,\r\n+        index: int = 0\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot attention weights heatmap\r\n+        \r\n+        Args:\r\n+            attention_weights: Attention weights tensor [batch, heads, seq_len, seq_len]\r\n+            index: Batch index to plot\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        # Get weights for specified batch\r\n+        weights = attention_weights[index].cpu().numpy()\r\n+        \r\n+        # Create subplot for each attention head\r\n+        n_heads = weights.shape[0]\r\n+        fig, axes = plt.subplots(\r\n+            1, n_heads,\r\n+            figsize=(4 * n_heads, 4),\r\n+            squeeze=False\r\n+        )\r\n+        \r\n+        for i, ax in enumerate(axes[0]):\r\n+            sns.heatmap(\r\n+                weights[i],\r\n+                ax=ax,\r\n+                cmap='viridis',\r\n+                cbar=True\r\n+            )\r\n+            ax.set_title(f'Head {i+1}')\r\n+            ax.set_xlabel('Key')\r\n+            ax.set_ylabel('Query')\r\n+            \r\n+        plt.tight_layout()\r\n+        return fig\r\n+    \r\n+    def plot_feature_importance(\r\n+        self,\r\n+        importance_scores: np.ndarray,\r\n+        feature_names: List[str]\r\n+    ) -> plt.Figure:\r\n+        \"\"\"\r\n+        Plot feature importance scores\r\n+        \r\n+        Args:\r\n+            importance_scores: Array of importance scores\r\n+            feature_names: List of feature names\r\n+            \r\n+        Returns:\r\n+            matplotlib figure\r\n+        \"\"\"\r\n+        fig, ax = plt.subplots(figsize=self.figsize)\r\n+        \r\n+        # Sort by importance\r\n+        sorted_idx = np.argsort(importance_scores)\r\n+        pos = np.arange(sorted_idx.shape[0]) + .5\r\n+        \r\n+        ax.barh(pos, importance_scores[sorted_idx])\r\n+        ax.set_yticks(pos)\r\n+        ax.set_yticklabels(np.array(feature_names)[sorted_idx])\r\n+        ax.set_xlabel('Importance Score')\r\n+        ax.set_title('Feature Importance')\r\n+        \r\n+        plt.tight_layout()\r\n+        return fig\r\n+    \r\n+    @staticmethod\r\n+    def save_figure(\r\n+        fig: plt.Figure,\r\n+        filename: str,\r\n+        dpi: int = 300\r\n+    ):\r\n+        \"\"\"\r\n+        Save figure to file\r\n+        \r\n+        Args:\r\n+            fig: matplotlib figure\r\n+            filename: Output filename\r\n+            dpi: Resolution in dots per inch\r\n+        \"\"\"\r\n+        fig.savefig(filename, dpi=dpi, bbox_inches='tight')\r\n+        \r\n+        \r\n+\r\n+import os\r\n+import torch\r\n+import numpy as np\r\n+import matplotlib.pyplot as plt\r\n+from datetime import datetime, timedelta\r\n+import pandas as pd\r\n+from typing import Tuple, Optional, List\r\n+\r\n+class PredictionVisualizer:\r\n+    \"\"\"Handles visualization of model predictions with input windows\"\"\"\r\n+    \r\n+    def __init__(\r\n+            self,\r\n+            output_dir: str = \"prediction_plots\",\r\n+            fig_size: Tuple[int, int] = (15, 7),\r\n+            dpi: int = 300\r\n+    ):\r\n+        \"\"\"\r\n+        Initialize visualizer\r\n+        \r\n+        Args:\r\n+            output_dir: Directory to save plots\r\n+            fig_size: Figure size for plots\r\n+            dpi: DPI for saved figures\r\n+        \"\"\"\r\n+        self.output_dir = output_dir\r\n+        self.fig_size = fig_size\r\n+        self.dpi = dpi\r\n+        \r\n+        # Create output directory if it doesn't exist\r\n+        os.makedirs(output_dir, exist_ok=True)\r\n+        \r\n+        # Set style\r\n+        plt.style.use('seaborn')\r\n+    \r\n+    def _generate_timestamps(\r\n+            self,\r\n+            start_time: datetime,\r\n+            sequence_length: int,\r\n+            freq: str = 'H'\r\n+    ) -> List[datetime]:\r\n+        \"\"\"\r\n+        Generate timestamps for x-axis\r\n+        \r\n+        Args:\r\n+            start_time: Starting timestamp\r\n+            sequence_length: Number of timestamps to generate\r\n+            freq: Frequency of timestamps ('H' for hourly)\r\n+            \r\n+        Returns:\r\n+            List of timestamps\r\n+        \"\"\"\r\n+        try:\r\n+            return [start_time + timedelta(hours=i) for i in range(sequence_length)]\r\n+        except Exception as e:\r\n+            print(f\"Error generating timestamps: {str(e)}\")\r\n+            # Return fallback numeric x-axis\r\n+            return [start_time + timedelta(hours=i) for i in range(sequence_length)]\r\n+    \r\n+    def plot_prediction_sample(\r\n+            self,\r\n+            input_seq: torch.Tensor,\r\n+            actual_seq: torch.Tensor,\r\n+            predicted_seq: torch.Tensor,\r\n+            sample_id: int,\r\n+            start_time: Optional[datetime] = None,\r\n+            scaler = None\r\n+    ):\r\n+        \"\"\"\r\n+        Create and save a plot showing input, actual and predicted values\r\n+        \r\n+        Args:\r\n+            input_seq: Input sequence tensor\r\n+            actual_seq: Actual values tensor\r\n+            predicted_seq: Predicted values tensor\r\n+            sample_id: Sample identifier for filename\r\n+            start_time: Starting timestamp (optional)\r\n+            scaler: Scaler object for inverse transform (optional)\r\n+        \"\"\"\r\n+        try:\r\n+            # Move tensors to CPU and convert to numpy\r\n+            input_seq = input_seq.cpu().numpy()\r\n+            actual_seq = actual_seq.cpu().numpy()\r\n+            predicted_seq = predicted_seq.cpu().numpy()\r\n+            \r\n+            # If we have a scaler, inverse transform the values\r\n+            if scaler is not None:\r\n+                try:\r\n+                    input_seq = scaler.inverse_transform(input_seq.reshape(-1, 1)).reshape(input_seq.shape)\r\n+                    actual_seq = scaler.inverse_transform(actual_seq.reshape(-1, 1)).reshape(actual_seq.shape)\r\n+                    predicted_seq = scaler.inverse_transform(predicted_seq.reshape(-1, 1)).reshape(predicted_seq.shape)\r\n+                except Exception as e:\r\n+                    print(f\"Warning: Could not apply inverse transform: {str(e)}\")\r\n+            \r\n+            # Get sequence lengths\r\n+            input_len = len(input_seq)\r\n+            prediction_len = len(predicted_seq)\r\n+            total_len = input_len + prediction_len\r\n+            \r\n+            # Generate x-axis values\r\n+            if start_time is not None:\r\n+                x_values = self._generate_timestamps(start_time, total_len)\r\n+            else:\r\n+                x_values = list(range(total_len))\r\n+            \r\n+            # Create figure\r\n+            plt.figure(figsize=self.fig_size)\r\n+            \r\n+            # Plot input sequence\r\n+            plt.plot(x_values[:input_len], \r\n+                    input_seq[:, 0], \r\n+                    'b-', \r\n+                    label='Input', \r\n+                    alpha=0.5)\r\n+            \r\n+            # Plot actual values\r\n+            plt.plot(x_values[input_len:], \r\n+                    actual_seq[:, 0], \r\n+                    'g-', \r\n+                    label='Actual', \r\n+                    linewidth=2)\r\n+            \r\n+            # Plot predictions\r\n+            plt.plot(x_values[input_len:], \r\n+                    predicted_seq[:, 0], \r\n+                    'r--', \r\n+                    label='Predicted', \r\n+                    linewidth=2)\r\n+            \r\n+            # Add vertical line separating input and prediction\r\n+            plt.axvline(x=x_values[input_len-1], \r\n+                       color='gray', \r\n+                       linestyle='--', \r\n+                       alpha=0.5)\r\n+            \r\n+            # Customize plot\r\n+            plt.title('Input Sequence and Predictions', pad=20)\r\n+            plt.xlabel('Time' if isinstance(x_values[0], datetime) else 'Time Step')\r\n+            plt.ylabel('Value')\r\n+            plt.legend()\r\n+            plt.grid(True, alpha=0.3)\r\n+            \r\n+            # Rotate x-axis labels if using timestamps\r\n+            if isinstance(x_values[0], datetime):\r\n+                plt.xticks(rotation=45)\r\n+            \r\n+            # Tight layout to prevent label cutoff\r\n+            plt.tight_layout()\r\n+            \r\n+            # Save plot\r\n+            filename = os.path.join(self.output_dir, f'prediction_sample_{sample_id}.png')\r\n+            plt.savefig(filename, dpi=self.dpi, bbox_inches='tight')\r\n+            plt.close()\r\n+            \r\n+            print(f\"Saved prediction plot to {filename}\")\r\n+            \r\n+        except Exception as e:\r\n+            print(f\"Error creating prediction plot: {str(e)}\")\r\n+            # Ensure figure is closed even if error occurs\r\n+            plt.close()\r\n+    \r\n+    def plot_multiple_samples(\r\n+            self,\r\n+            model: torch.nn.Module,\r\n+            dataset,\r\n+            num_samples: int = 5,\r\n+            scaler = None,\r\n+            device: torch.device = torch.device('cpu')\r\n+    ):\r\n+        \"\"\"\r\n+        Create plots for multiple random samples from dataset\r\n+        \r\n+        Args:\r\n+            model: Trained model\r\n+            dataset: Dataset containing samples\r\n+            num_samples: Number of samples to plot\r\n+            scaler: Scaler object for inverse transform\r\n+            device: Device to run model on\r\n+        \"\"\"\r\n+        try:\r\n+            model.eval()  # Set model to evaluation mode\r\n+            \r\n+            # Generate random indices\r\n+            total_samples = len(dataset)\r\n+            indices = np.random.choice(total_samples, min(num_samples, total_samples), replace=False)\r\n+            \r\n+            with torch.no_grad():\r\n+                for idx in indices:\r\n+                    # Get sample from dataset\r\n+                    input_seq, decoder_input, target = dataset[idx]\r\n+                    \r\n+                    # Prepare input for model\r\n+                    input_batch = input_seq.unsqueeze(0).to(device)\r\n+                    \r\n+                    # Get prediction\r\n+                    prediction, _ = model(input_batch)\r\n+                    \r\n+                    # Remove batch dimension\r\n+                    prediction = prediction.squeeze(0)\r\n+                    \r\n+                    # Plot sample\r\n+                    self.plot_prediction_sample(\r\n+                        input_seq=input_seq,\r\n+                        actual_seq=target,\r\n+                        predicted_seq=prediction,\r\n+                        sample_id=idx,\r\n+                        scaler=scaler\r\n+                    )\r\n+                    \r\n+        except Exception as e:\r\n+            print(f\"Error plotting multiple samples: {str(e)}\")\r\n+\r\n+    def create_error_analysis_plots(\r\n+            self,\r\n+            actual_values: torch.Tensor,\r\n+            predicted_values: torch.Tensor,\r\n+            scaler = None\r\n+    ):\r\n+        \"\"\"\r\n+        Create error analysis plots (error distribution, scatter plot)\r\n+        \r\n+        Args:\r\n+            actual_values: Tensor of actual values\r\n+            predicted_values: Tensor of predicted values\r\n+            scaler: Scaler object for inverse transform\r\n+        \"\"\"\r\n+        try:\r\n+            # Convert to numpy and reshape\r\n+            actual = actual_values.cpu().numpy().reshape(-1)\r\n+            predicted = predicted_values.cpu().numpy().reshape(-1)\r\n+            \r\n+            if scaler is not None:\r\n+                try:\r\n+                    actual = scaler.inverse_transform(actual.reshape(-1, 1)).reshape(-1)\r\n+                    predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(-1)\r\n+                except Exception as e:\r\n+                    print(f\"Warning: Could not apply inverse transform: {str(e)}\")\r\n+            \r\n+            # Calculate errors\r\n+            errors = predicted - actual\r\n+            \r\n+            # Error distribution plot\r\n+            plt.figure(figsize=self.fig_size)\r\n+            plt.hist(errors, bins=50, edgecolor='black')\r\n+            plt.title('Error Distribution')\r\n+            plt.xlabel('Prediction Error')\r\n+            plt.ylabel('Frequency')\r\n+            plt.tight_layout()\r\n+            plt.savefig(os.path.join(self.output_dir, 'error_distribution.png'), \r\n+                       dpi=self.dpi, \r\n+                       bbox_inches='tight')\r\n+            plt.close()\r\n+            \r\n+            # Scatter plot\r\n+            plt.figure(figsize=self.fig_size)\r\n+            plt.scatter(actual, predicted, alpha=0.5)\r\n+            z\r\n+            # Add diagonal line\r\n+            min_val = min(actual.min(), predicted.min())\r\n+            max_val = max(actual.max(), predicted.max())\r\n+            plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\r\n+            \r\n+            plt.title('Actual vs Predicted Values')\r\n+            plt.xlabel('Actual Values')\r\n+            plt.ylabel('Predicted Values')\r\n+            plt.tight_layout()\r\n+            plt.savefig(os.path.join(self.output_dir, 'actual_vs_predicted.png'), \r\n+                       dpi=self.dpi, \r\n+                       bbox_inches='tight')\r\n+            plt.close()\r\n+            \r\n+        except Exception as e:\r\n+            print(f\"Error creating analysis plots: {str(e)}\")\r\n+            plt.close()\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733311728768,
            "name": "Commit-0",
            "content": "\"\"\"\r\nVisualization utilities for time series forecasting.\r\n\r\nDependencies:\r\n- matplotlib>=3.7.2\r\n- seaborn>=0.12.2\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nfrom typing import List, Optional, Tuple\r\nimport torch\r\n\r\nclass TimeSeriesVisualizer:\r\n    \"\"\"Visualization tools for time series data and model results\"\"\"\r\n    \r\n    def __init__(self, figsize: Tuple[int, int] = (12, 6)):\r\n        \"\"\"\r\n        Initialize visualizer\r\n        \r\n        Args:\r\n            figsize: Default figure size for plots\r\n        \"\"\"\r\n        self.figsize = figsize\r\n        # Set style\r\n        sns.set_style(\"whitegrid\")\r\n        plt.rcParams['figure.figsize'] = figsize\r\n        \r\n    def plot_training_history(\r\n        self,\r\n        train_losses: List[float],\r\n        val_losses: List[float],\r\n        title: str = \"Training History\"\r\n    ) -> plt.Figure:\r\n        \"\"\"\r\n        Plot training and validation losses\r\n        \r\n        Args:\r\n            train_losses: List of training losses\r\n            val_losses: List of validation losses\r\n            title: Plot title\r\n            \r\n        Returns:\r\n            matplotlib figure\r\n        \"\"\"\r\n        fig, ax = plt.subplots(figsize=self.figsize)\r\n        epochs = range(1, len(train_losses) + 1)\r\n        \r\n        ax.plot(epochs, train_losses, 'b-', label='Training Loss')\r\n        ax.plot(epochs, val_losses, 'r-', label='Validation Loss')\r\n        \r\n        ax.set_title(title)\r\n        ax.set_xlabel('Epoch')\r\n        ax.set_ylabel('Loss')\r\n        ax.legend()\r\n        ax.grid(True)\r\n        \r\n        return fig\r\n    \r\n    def plot_predictions(\r\n        self,\r\n        true_values: torch.Tensor,\r\n        predictions: torch.Tensor,\r\n        timestamps: Optional[List] = None,\r\n        title: str = \"Predictions vs Actual\"\r\n    ) -> plt.Figure:\r\n        \"\"\"\r\n        Plot predictions against actual values\r\n        \r\n        Args:\r\n            true_values: Ground truth values\r\n            predictions: Model predictions\r\n            timestamps: Optional list of timestamps\r\n            title: Plot title\r\n            \r\n        Returns:\r\n            matplotlib figure\r\n        \"\"\"\r\n        # Convert to numpy if tensors\r\n        if isinstance(true_values, torch.Tensor):\r\n            true_values = true_values.cpu().numpy()\r\n        if isinstance(predictions, torch.Tensor):\r\n            predictions = predictions.cpu().numpy()\r\n            \r\n        fig, ax = plt.subplots(figsize=self.figsize)\r\n        \r\n        x_values = timestamps if timestamps is not None else range(len(true_values))\r\n        \r\n        ax.plot(x_values, true_values, 'b-', label='Actual', alpha=0.7)\r\n        ax.plot(x_values, predictions, 'r--', label='Predicted', alpha=0.7)\r\n        \r\n        ax.set_title(title)\r\n        ax.set_xlabel('Time' if timestamps is None else 'Timestamp')\r\n        ax.set_ylabel('Value')\r\n        ax.legend()\r\n        ax.grid(True)\r\n        \r\n        # Rotate x-axis labels if timestamps are provided\r\n        if timestamps is not None:\r\n            plt.xticks(rotation=45)\r\n            \r\n        plt.tight_layout()\r\n        return fig\r\n    \r\n    def plot_attention_weights(\r\n        self,\r\n        attention_weights: torch.Tensor,\r\n        index: int = 0\r\n    ) -> plt.Figure:\r\n        \"\"\"\r\n        Plot attention weights heatmap\r\n        \r\n        Args:\r\n            attention_weights: Attention weights tensor [batch, heads, seq_len, seq_len]\r\n            index: Batch index to plot\r\n            \r\n        Returns:\r\n            matplotlib figure\r\n        \"\"\"\r\n        # Get weights for specified batch\r\n        weights = attention_weights[index].cpu().numpy()\r\n        \r\n        # Create subplot for each attention head\r\n        n_heads = weights.shape[0]\r\n        fig, axes = plt.subplots(\r\n            1, n_heads,\r\n            figsize=(4 * n_heads, 4),\r\n            squeeze=False\r\n        )\r\n        \r\n        for i, ax in enumerate(axes[0]):\r\n            sns.heatmap(\r\n                weights[i],\r\n                ax=ax,\r\n                cmap='viridis',\r\n                cbar=True\r\n            )\r\n            ax.set_title(f'Head {i+1}')\r\n            ax.set_xlabel('Key')\r\n            ax.set_ylabel('Query')\r\n            \r\n        plt.tight_layout()\r\n        return fig\r\n    \r\n    def plot_feature_importance(\r\n        self,\r\n        importance_scores: np.ndarray,\r\n        feature_names: List[str]\r\n    ) -> plt.Figure:\r\n        \"\"\"\r\n        Plot feature importance scores\r\n        \r\n        Args:\r\n            importance_scores: Array of importance scores\r\n            feature_names: List of feature names\r\n            \r\n        Returns:\r\n            matplotlib figure\r\n        \"\"\"\r\n        fig, ax = plt.subplots(figsize=self.figsize)\r\n        \r\n        # Sort by importance\r\n        sorted_idx = np.argsort(importance_scores)\r\n        pos = np.arange(sorted_idx.shape[0]) + .5\r\n        \r\n        ax.barh(pos, importance_scores[sorted_idx])\r\n        ax.set_yticks(pos)\r\n        ax.set_yticklabels(np.array(feature_names)[sorted_idx])\r\n        ax.set_xlabel('Importance Score')\r\n        ax.set_title('Feature Importance')\r\n        \r\n        plt.tight_layout()\r\n        return fig\r\n    \r\n    @staticmethod\r\n    def save_figure(\r\n        fig: plt.Figure,\r\n        filename: str,\r\n        dpi: int = 300\r\n    ):\r\n        \"\"\"\r\n        Save figure to file\r\n        \r\n        Args:\r\n            fig: matplotlib figure\r\n            filename: Output filename\r\n            dpi: Resolution in dots per inch\r\n        \"\"\"\r\n        fig.savefig(filename, dpi=dpi, bbox_inches='tight')"
        }
    ]
}