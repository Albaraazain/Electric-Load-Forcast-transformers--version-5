{
    "sourceFile": "src/training/trainer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 10,
            "patches": [
                {
                    "date": 1733312110572,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733312196495,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -134,8 +134,10 @@\n \r\n                 # Mixed precision forward pass\r\n                 with autocast():\r\n                     outputs, _ = self.model(encoder_inputs)\r\n+                    # Ensure the output and target dimensions match\r\n+                    outputs = outputs[:, :targets.size(1), :]\r\n                     loss = self.criterion(outputs, targets)\r\n \r\n                 # Mixed precision backward pass\r\n                 self.scaler.scale(loss).backward()\r\n"
                },
                {
                    "date": 1733312463077,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,14 +117,20 @@\n \r\n         self.logger.info(f\"Loaded checkpoint from epoch {self.current_epoch}\")\r\n \r\n     def train_epoch(self) -> float:\r\n-        \"\"\"Train one epoch\"\"\"\r\n+        \"\"\"Train one epoch with enhanced debugging\"\"\"\r\n         self.model.train()\r\n         total_loss = 0\r\n \r\n         with tqdm(self.train_loader, desc=f'Epoch {self.current_epoch}') as pbar:\r\n             for batch_idx, (encoder_inputs, decoder_inputs, targets) in enumerate(pbar):\r\n+                # Log shapes before moving to device\r\n+                self.logger.debug(f\"Batch {batch_idx} shapes:\")\r\n+                self.logger.debug(f\"encoder_inputs shape: {encoder_inputs.shape}\")\r\n+                self.logger.debug(f\"decoder_inputs shape: {decoder_inputs.shape}\")\r\n+                self.logger.debug(f\"targets shape: {targets.shape}\")\r\n+\r\n                 # Move data to device\r\n                 encoder_inputs = encoder_inputs.to(self.device)\r\n                 decoder_inputs = decoder_inputs.to(self.device)\r\n                 targets = targets.to(self.device)\r\n@@ -134,10 +140,20 @@\n \r\n                 # Mixed precision forward pass\r\n                 with autocast():\r\n                     outputs, _ = self.model(encoder_inputs)\r\n+                    self.logger.debug(f\"Model outputs shape: {outputs.shape}\")\r\n+                    \r\n                     # Ensure the output and target dimensions match\r\n-                    outputs = outputs[:, :targets.size(1), :]\r\n+                    # We need to adjust either the outputs or targets here\r\n+                    if outputs.size(1) != targets.size(1):\r\n+                        self.logger.warning(\r\n+                            f\"Dimension mismatch - outputs: {outputs.size()}, targets: {targets.size()}\"\r\n+                        )\r\n+                        # Adjust targets to match output sequence length\r\n+                        targets = targets[:, :outputs.size(1), :]\r\n+                        self.logger.debug(f\"Adjusted targets shape: {targets.shape}\")\r\n+\r\n                     loss = self.criterion(outputs, targets)\r\n \r\n                 # Mixed precision backward pass\r\n                 self.scaler.scale(loss).backward()\r\n@@ -160,11 +176,22 @@\n                 # Update metrics\r\n                 total_loss += loss.item()\r\n                 self.global_step += 1\r\n \r\n-                # Update progress bar\r\n-                pbar.set_postfix({'loss': loss.item()})\r\n+                # Update progress bar with more detailed info\r\n+                pbar.set_postfix({\r\n+                    'loss': f\"{loss.item():.4f}\",\r\n+                    'output_shape': str(list(outputs.shape)),\r\n+                    'target_shape': str(list(targets.shape))\r\n+                })\r\n \r\n+                # Early batch debugging\r\n+                if batch_idx < 3:\r\n+                    self.logger.info(f\"Detailed batch {batch_idx} info:\")\r\n+                    self.logger.info(f\"Input sequence length: {encoder_inputs.size(1)}\")\r\n+                    self.logger.info(f\"Output sequence length: {outputs.size(1)}\")\r\n+                    self.logger.info(f\"Target sequence length: {targets.size(1)}\")\r\n+\r\n         return total_loss / len(self.train_loader)\r\n \r\n     @torch.no_grad()\r\n     def validate(self) -> float:\r\n"
                },
                {
                    "date": 1733312574654,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,8 +61,17 @@\n \r\n         # Setup logging\r\n         self.logger = logging.getLogger(__name__)\r\n         self._setup_logging()\r\n+        \r\n+        logging.basicConfig(\r\n+            level=logging.DEBUG,  # Change to DEBUG level\r\n+            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\r\n+            handlers=[\r\n+                logging.FileHandler('debug.log'),\r\n+                logging.StreamHandler()\r\n+            ]\r\n+        )\r\n \r\n         # Training state\r\n         self.current_epoch = 0\r\n         self.global_step = 0\r\n"
                },
                {
                    "date": 1733312605105,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,9 +57,19 @@\n         )\r\n \r\n         # Initialize mixed precision training\r\n         self.scaler = GradScaler()\r\n+        \r\n+        logging.basicConfig(\r\n+            level=logging.DEBUG,  # Change to DEBUG level\r\n+            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\r\n+            handlers=[\r\n+                logging.FileHandler('debug.log'),\r\n+                logging.StreamHandler()\r\n+            ]\r\n+        )\r\n \r\n+\r\n         # Setup logging\r\n         self.logger = logging.getLogger(__name__)\r\n         self._setup_logging()\r\n         \r\n"
                },
                {
                    "date": 1733312610347,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,16 +72,8 @@\n         # Setup logging\r\n         self.logger = logging.getLogger(__name__)\r\n         self._setup_logging()\r\n         \r\n-        logging.basicConfig(\r\n-            level=logging.DEBUG,  # Change to DEBUG level\r\n-            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\r\n-            handlers=[\r\n-                logging.FileHandler('debug.log'),\r\n-                logging.StreamHandler()\r\n-            ]\r\n-        )\r\n \r\n         # Training state\r\n         self.current_epoch = 0\r\n         self.global_step = 0\r\n"
                },
                {
                    "date": 1733314341264,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,20 +128,16 @@\n \r\n         self.logger.info(f\"Loaded checkpoint from epoch {self.current_epoch}\")\r\n \r\n     def train_epoch(self) -> float:\r\n-        \"\"\"Train one epoch with enhanced debugging\"\"\"\r\n+        \"\"\"Train one epoch with improved logging\"\"\"\r\n         self.model.train()\r\n         total_loss = 0\r\n+        log_interval = 50  # Log every 50 batches\r\n+        running_loss = 0.0\r\n \r\n         with tqdm(self.train_loader, desc=f'Epoch {self.current_epoch}') as pbar:\r\n             for batch_idx, (encoder_inputs, decoder_inputs, targets) in enumerate(pbar):\r\n-                # Log shapes before moving to device\r\n-                self.logger.debug(f\"Batch {batch_idx} shapes:\")\r\n-                self.logger.debug(f\"encoder_inputs shape: {encoder_inputs.shape}\")\r\n-                self.logger.debug(f\"decoder_inputs shape: {decoder_inputs.shape}\")\r\n-                self.logger.debug(f\"targets shape: {targets.shape}\")\r\n-\r\n                 # Move data to device\r\n                 encoder_inputs = encoder_inputs.to(self.device)\r\n                 decoder_inputs = decoder_inputs.to(self.device)\r\n                 targets = targets.to(self.device)\r\n@@ -151,25 +147,27 @@\n \r\n                 # Mixed precision forward pass\r\n                 with autocast():\r\n                     outputs, _ = self.model(encoder_inputs)\r\n-                    self.logger.debug(f\"Model outputs shape: {outputs.shape}\")\r\n+                    if batch_idx == 0:  # Log shapes only for first batch of epoch\r\n+                        self.logger.info(\r\n+                            f\"\\n{'='*50}\\n\"\r\n+                            f\"Epoch {self.current_epoch} - Batch Shapes:\\n\"\r\n+                            f\"Input: {encoder_inputs.shape}\\n\"\r\n+                            f\"Output: {outputs.shape}\\n\"\r\n+                            f\"Target: {targets.shape}\\n\"\r\n+                            f\"{'='*50}\"\r\n+                        )\r\n                     \r\n                     # Ensure the output and target dimensions match\r\n-                    # We need to adjust either the outputs or targets here\r\n                     if outputs.size(1) != targets.size(1):\r\n-                        self.logger.warning(\r\n-                            f\"Dimension mismatch - outputs: {outputs.size()}, targets: {targets.size()}\"\r\n-                        )\r\n-                        # Adjust targets to match output sequence length\r\n                         targets = targets[:, :outputs.size(1), :]\r\n-                        self.logger.debug(f\"Adjusted targets shape: {targets.shape}\")\r\n-\r\n+                    \r\n                     loss = self.criterion(outputs, targets)\r\n \r\n                 # Mixed precision backward pass\r\n                 self.scaler.scale(loss).backward()\r\n-\r\n+                \r\n                 # Gradient clipping\r\n                 if self.config.get('grad_clip'):\r\n                     self.scaler.unscale_(self.optimizer)\r\n                     torch.nn.utils.clip_grad_norm_(\r\n@@ -184,81 +182,101 @@\n                 # Scheduler step\r\n                 self.scheduler.step()\r\n \r\n                 # Update metrics\r\n+                running_loss += loss.item()\r\n                 total_loss += loss.item()\r\n                 self.global_step += 1\r\n \r\n-                # Update progress bar with more detailed info\r\n+                # Update progress bar with minimal info\r\n                 pbar.set_postfix({\r\n-                    'loss': f\"{loss.item():.4f}\",\r\n-                    'output_shape': str(list(outputs.shape)),\r\n-                    'target_shape': str(list(targets.shape))\r\n+                    'loss': f\"{loss.item():.4f}\"\r\n                 })\r\n \r\n-                # Early batch debugging\r\n-                if batch_idx < 3:\r\n-                    self.logger.info(f\"Detailed batch {batch_idx} info:\")\r\n-                    self.logger.info(f\"Input sequence length: {encoder_inputs.size(1)}\")\r\n-                    self.logger.info(f\"Output sequence length: {outputs.size(1)}\")\r\n-                    self.logger.info(f\"Target sequence length: {targets.size(1)}\")\r\n+                # Periodic detailed logging\r\n+                if (batch_idx + 1) % log_interval == 0:\r\n+                    avg_loss = running_loss / log_interval\r\n+                    lr = self.optimizer.param_groups[0]['lr']\r\n+                    self.logger.info(\r\n+                        f\"\\n{'='*30} Batch {batch_idx + 1}/{len(self.train_loader)} {'='*30}\\n\"\r\n+                        f\"Average Loss: {avg_loss:.4f} | Learning Rate: {lr:.6f}\\n\"\r\n+                        f\"{'='*80}\"\r\n+                    )\r\n+                    running_loss = 0.0\r\n \r\n         return total_loss / len(self.train_loader)\r\n \r\n-    @torch.no_grad()\r\n-    def validate(self) -> float:\r\n-        \"\"\"Validate model\"\"\"\r\n-        self.model.eval()\r\n-        total_loss = 0\r\n-\r\n-        for encoder_inputs, decoder_inputs, targets in self.val_loader:\r\n-            # Move data to device\r\n-            encoder_inputs = encoder_inputs.to(self.device)\r\n-            decoder_inputs = decoder_inputs.to(self.device)\r\n-            targets = targets.to(self.device)\r\n-\r\n-            # Forward pass\r\n-            outputs, _ = self.model(encoder_inputs)\r\n-            loss = self.criterion(outputs, targets)\r\n-\r\n-            total_loss += loss.item()\r\n-\r\n-        return total_loss / len(self.val_loader)\r\n-\r\n     def train(self):\r\n-        \"\"\"Main training loop\"\"\"\r\n-        self.logger.info(\"Starting training...\")\r\n+        \"\"\"Main training loop with improved logging\"\"\"\r\n+        self.logger.info(\"\\n\" + \"=\"*80)\r\n+        self.logger.info(\"Starting Training\")\r\n+        self.logger.info(\"=\"*80)\r\n \r\n         for epoch in range(self.current_epoch, self.config['max_epochs']):\r\n             self.current_epoch = epoch\r\n-\r\n+            \r\n             # Training phase\r\n             train_loss = self.train_epoch()\r\n             self.train_losses.append(train_loss)\r\n \r\n             # Validation phase\r\n             val_loss = self.validate()\r\n             self.val_losses.append(val_loss)\r\n \r\n-            # Logging\r\n+            # Epoch summary\r\n             self.logger.info(\r\n-                f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\r\n+                f\"\\n{'='*30} Epoch {epoch} Summary {'='*30}\\n\"\r\n\\ No newline at end of file\n+                f\"Train Loss: {train_loss:.4f}\\n\"\r\n+                f\"Val Loss: {val_loss:.4f}\\n\"\r\n+                f\"Best Val Loss: {self.best_val_loss:.4f}\\n\"\r\n+                f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\\n\"\r\n+                f\"{'='*80}\"\r\n             )\r\n \r\n             # Check for best model\r\n             is_best = val_loss < self.best_val_loss\r\n             if is_best:\r\n                 self.best_val_loss = val_loss\r\n                 self.patience_counter = 0\r\n+                self.logger.info(f\"New best model saved! (Val Loss: {val_loss:.4f})\")\r\n             else:\r\n                 self.patience_counter += 1\r\n \r\n             # Save checkpoint\r\n             self.save_checkpoint(is_best)\r\n \r\n             # Early stopping\r\n             if self.patience_counter >= self.config['patience']:\r\n-                self.logger.info(\"Early stopping triggered\")\r\n+                self.logger.info(\r\n+                    f\"\\n{'='*30} Early Stopping {'='*30}\\n\"\r\n+                    f\"No improvement for {self.config['patience']} epochs\\n\"\r\n+                    f\"Best Val Loss: {self.best_val_loss:.4f}\\n\"\r\n+                    f\"{'='*80}\"\r\n+                )\r\n                 break\r\n \r\n-        self.logger.info(\"Training completed!\")\r\n-        return self.train_losses, self.val_losses\n+        self.logger.info(\"\\n\" + \"=\"*80)\r\n+        self.logger.info(\"Training Completed!\")\r\n+        self.logger.info(\"=\"*80)\r\n+        return self.train_losses, self.val_losses\r\n+\r\n+\r\n+    @torch.no_grad()\r\n+    def validate(self) -> float:\r\n+        \"\"\"Validate model\"\"\"\r\n+        self.model.eval()\r\n+        total_loss = 0\r\n+\r\n+        for encoder_inputs, decoder_inputs, targets in self.val_loader:\r\n+            # Move data to device\r\n+            encoder_inputs = encoder_inputs.to(self.device)\r\n+            decoder_inputs = decoder_inputs.to(self.device)\r\n+            targets = targets.to(self.device)\r\n+\r\n+            # Forward pass\r\n+            outputs, _ = self.model(encoder_inputs)\r\n+            loss = self.criterion(outputs, targets)\r\n+\r\n+            total_loss += loss.item()\r\n+\r\n+        return total_loss / len(self.val_loader)\r\n+\r\n"
                },
                {
                    "date": 1733314917233,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,43 +128,38 @@\n \r\n         self.logger.info(f\"Loaded checkpoint from epoch {self.current_epoch}\")\r\n \r\n     def train_epoch(self) -> float:\r\n-        \"\"\"Train one epoch with improved logging\"\"\"\r\n         self.model.train()\r\n         total_loss = 0\r\n-        log_interval = 50  # Log every 50 batches\r\n-        running_loss = 0.0\r\n-\r\n+        \r\n         with tqdm(self.train_loader, desc=f'Epoch {self.current_epoch}') as pbar:\r\n             for batch_idx, (encoder_inputs, decoder_inputs, targets) in enumerate(pbar):\r\n+                # Debug information\r\n+                if batch_idx == 0:\r\n+                    self.logger.info(f\"\\nBatch shapes:\")\r\n+                    self.logger.info(f\"Encoder inputs: {encoder_inputs.shape}\")\r\n+                    self.logger.info(f\"Decoder inputs: {decoder_inputs.shape}\")\r\n+                    self.logger.info(f\"Targets: {targets.shape}\")\r\n+                \r\n                 # Move data to device\r\n                 encoder_inputs = encoder_inputs.to(self.device)\r\n                 decoder_inputs = decoder_inputs.to(self.device)\r\n                 targets = targets.to(self.device)\r\n-\r\n+                \r\n                 # Clear gradients\r\n                 self.optimizer.zero_grad()\r\n+                \r\n+                # Forward pass\r\n+                outputs, _ = self.model(encoder_inputs)\r\n+                \r\n+                if batch_idx == 0:\r\n+                    self.logger.info(f\"Model output shape: {outputs.shape}\")\r\n+                    self.logger.info(f\"Target shape for loss: {targets.shape}\")\r\n+                \r\n+                # Compute loss\r\n+                loss = self.criterion(outputs, targets)\r\n \r\n-                # Mixed precision forward pass\r\n-                with autocast():\r\n-                    outputs, _ = self.model(encoder_inputs)\r\n-                    if batch_idx == 0:  # Log shapes only for first batch of epoch\r\n-                        self.logger.info(\r\n-                            f\"\\n{'='*50}\\n\"\r\n-                            f\"Epoch {self.current_epoch} - Batch Shapes:\\n\"\r\n-                            f\"Input: {encoder_inputs.shape}\\n\"\r\n-                            f\"Output: {outputs.shape}\\n\"\r\n-                            f\"Target: {targets.shape}\\n\"\r\n-                            f\"{'='*50}\"\r\n-                        )\r\n-                    \r\n-                    # Ensure the output and target dimensions match\r\n-                    if outputs.size(1) != targets.size(1):\r\n-                        targets = targets[:, :outputs.size(1), :]\r\n-                    \r\n-                    loss = self.criterion(outputs, targets)\r\n-\r\n                 # Mixed precision backward pass\r\n                 self.scaler.scale(loss).backward()\r\n                 \r\n                 # Gradient clipping\r\n@@ -277,5 +272,6 @@\n             loss = self.criterion(outputs, targets)\r\n \r\n             total_loss += loss.item()\r\n \r\n-        return total_loss / len(self.val_loader)\n\\ No newline at end of file\n+        return total_loss / len(self.val_loader)\r\n+\r\n"
                },
                {
                    "date": 1733315365506,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,11 +128,14 @@\n \r\n         self.logger.info(f\"Loaded checkpoint from epoch {self.current_epoch}\")\r\n \r\n     def train_epoch(self) -> float:\r\n+        \"\"\"Train one epoch with improved logging\"\"\"\r\n         self.model.train()\r\n         total_loss = 0\r\n-        \r\n+        running_loss = 0.0  # Add this line to initialize running_loss\r\n+        log_interval = 50\r\n+\r\n         with tqdm(self.train_loader, desc=f'Epoch {self.current_epoch}') as pbar:\r\n             for batch_idx, (encoder_inputs, decoder_inputs, targets) in enumerate(pbar):\r\n                 # Debug information\r\n                 if batch_idx == 0:\r\n@@ -144,22 +147,21 @@\n                 # Move data to device\r\n                 encoder_inputs = encoder_inputs.to(self.device)\r\n                 decoder_inputs = decoder_inputs.to(self.device)\r\n                 targets = targets.to(self.device)\r\n-                \r\n+\r\n                 # Clear gradients\r\n                 self.optimizer.zero_grad()\r\n-                \r\n-                # Forward pass\r\n-                outputs, _ = self.model(encoder_inputs)\r\n-                \r\n-                if batch_idx == 0:\r\n-                    self.logger.info(f\"Model output shape: {outputs.shape}\")\r\n-                    self.logger.info(f\"Target shape for loss: {targets.shape}\")\r\n-                \r\n-                # Compute loss\r\n-                loss = self.criterion(outputs, targets)\r\n \r\n+                # Mixed precision forward pass\r\n+                with autocast():\r\n+                    outputs, _ = self.model(encoder_inputs)\r\n+                    if batch_idx == 0:\r\n+                        self.logger.info(f\"Model output shape: {outputs.shape}\")\r\n+                        self.logger.info(f\"Target shape for loss: {targets.shape}\")\r\n+                        \r\n+                    loss = self.criterion(outputs, targets)\r\n+\r\n                 # Mixed precision backward pass\r\n                 self.scaler.scale(loss).backward()\r\n                 \r\n                 # Gradient clipping\r\n"
                },
                {
                    "date": 1733317342664,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,10 @@\n import time\r\n import torch\r\n import torch.nn as nn\r\n from torch.utils.data import DataLoader\r\n-from torch.cuda.amp import GradScaler, autocast\r\n+from torch.cuda.amp.grad_scaler import GradScaler\r\n+from torch.cuda.amp.autocast_mode import autocast\r\n from typing import Dict, Optional, Tuple, List\r\n import logging\r\n from tqdm import tqdm\r\n import numpy as np\r\n"
                },
                {
                    "date": 1733317370764,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,14 +162,14 @@\n                         \r\n                     loss = self.criterion(outputs, targets)\r\n \r\n                 # Mixed precision backward pass\r\n-                self.scaler.scale(loss).backward()\r\n+                self.scaler.scale(loss).backward() # type: ignore\r\n                 \r\n                 # Gradient clipping\r\n                 if self.config.get('grad_clip'):\r\n                     self.scaler.unscale_(self.optimizer)\r\n-                    torch.nn.utils.clip_grad_norm_(\r\n+                    torch.nn.utils.clip_grad_norm_( # type: ignore\r\n                         self.model.parameters(),\r\n                         self.config['grad_clip_value']\r\n                     )\r\n \r\n"
                }
            ],
            "date": 1733312110572,
            "name": "Commit-0",
            "content": "\"\"\"\r\nTrainer class for the Informer model.\r\nHandles training loop, validation, and model checkpointing.\r\n\"\"\"\r\n\r\nimport os\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.cuda.amp import GradScaler, autocast\r\nfrom typing import Dict, Optional, Tuple, List\r\nimport logging\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nfrom model.informer import Informer\r\nfrom training.scheduler import create_scheduler\r\n\r\nclass Trainer:\r\n    def __init__(\r\n            self,\r\n            model: Informer,\r\n            train_loader: DataLoader,\r\n            val_loader: DataLoader,\r\n            optimizer: torch.optim.Optimizer,\r\n            criterion: nn.Module,\r\n            config: dict,\r\n            device: torch.device\r\n    ):\r\n        \"\"\"\r\n        Initialize trainer\r\n\r\n        Args:\r\n            model: Informer model instance\r\n            train_loader: Training data loader\r\n            val_loader: Validation data loader\r\n            optimizer: Optimizer instance\r\n            criterion: Loss function\r\n            config: Training configuration\r\n            device: Device to train on\r\n        \"\"\"\r\n        self.model = model\r\n        self.train_loader = train_loader\r\n        self.val_loader = val_loader\r\n        self.optimizer = optimizer\r\n        self.criterion = criterion\r\n        self.config = config\r\n        self.device = device\r\n\r\n        # Initialize scheduler\r\n        self.scheduler = create_scheduler(\r\n            optimizer=optimizer,\r\n            num_epochs=config['max_epochs'],\r\n            steps_per_epoch=len(train_loader),\r\n            warmup_epochs=config.get('warmup_epochs', 3)\r\n        )\r\n\r\n        # Initialize mixed precision training\r\n        self.scaler = GradScaler()\r\n\r\n        # Setup logging\r\n        self.logger = logging.getLogger(__name__)\r\n        self._setup_logging()\r\n\r\n        # Training state\r\n        self.current_epoch = 0\r\n        self.global_step = 0\r\n        self.best_val_loss = float('inf')\r\n        self.patience_counter = 0\r\n\r\n        # Metrics tracking\r\n        self.train_losses = []\r\n        self.val_losses = []\r\n\r\n    def _setup_logging(self):\r\n        \"\"\"Setup logging configuration\"\"\"\r\n        logging.basicConfig(\r\n            level=logging.INFO,\r\n            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\r\n        )\r\n\r\n    def save_checkpoint(self, is_best: bool = False):\r\n        \"\"\"Save model checkpoint\"\"\"\r\n        checkpoint = {\r\n            'epoch': self.current_epoch,\r\n            'model_state_dict': self.model.state_dict(),\r\n            'optimizer_state_dict': self.optimizer.state_dict(),\r\n            'scheduler_state_dict': self.scheduler.state_dict(),\r\n            'best_val_loss': self.best_val_loss,\r\n            'train_losses': self.train_losses,\r\n            'val_losses': self.val_losses\r\n        }\r\n\r\n        # Save regular checkpoint\r\n        checkpoint_path = os.path.join(self.config['checkpoint_dir'], 'checkpoint.pth')\r\n        torch.save(checkpoint, checkpoint_path)\r\n\r\n        # Save best model\r\n        if is_best:\r\n            best_model_path = os.path.join(self.config['checkpoint_dir'], 'best_model.pth')\r\n            torch.save(checkpoint, best_model_path)\r\n            self.logger.info(f\"Saved best model checkpoint to {best_model_path}\")\r\n\r\n    def load_checkpoint(self, checkpoint_path: str):\r\n        \"\"\"Load model checkpoint\"\"\"\r\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\r\n\r\n        self.model.load_state_dict(checkpoint['model_state_dict'])\r\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\r\n\r\n        self.current_epoch = checkpoint['epoch']\r\n        self.best_val_loss = checkpoint['best_val_loss']\r\n        self.train_losses = checkpoint['train_losses']\r\n        self.val_losses = checkpoint['val_losses']\r\n\r\n        self.logger.info(f\"Loaded checkpoint from epoch {self.current_epoch}\")\r\n\r\n    def train_epoch(self) -> float:\r\n        \"\"\"Train one epoch\"\"\"\r\n        self.model.train()\r\n        total_loss = 0\r\n\r\n        with tqdm(self.train_loader, desc=f'Epoch {self.current_epoch}') as pbar:\r\n            for batch_idx, (encoder_inputs, decoder_inputs, targets) in enumerate(pbar):\r\n                # Move data to device\r\n                encoder_inputs = encoder_inputs.to(self.device)\r\n                decoder_inputs = decoder_inputs.to(self.device)\r\n                targets = targets.to(self.device)\r\n\r\n                # Clear gradients\r\n                self.optimizer.zero_grad()\r\n\r\n                # Mixed precision forward pass\r\n                with autocast():\r\n                    outputs, _ = self.model(encoder_inputs)\r\n                    loss = self.criterion(outputs, targets)\r\n\r\n                # Mixed precision backward pass\r\n                self.scaler.scale(loss).backward()\r\n\r\n                # Gradient clipping\r\n                if self.config.get('grad_clip'):\r\n                    self.scaler.unscale_(self.optimizer)\r\n                    torch.nn.utils.clip_grad_norm_(\r\n                        self.model.parameters(),\r\n                        self.config['grad_clip_value']\r\n                    )\r\n\r\n                # Optimizer step\r\n                self.scaler.step(self.optimizer)\r\n                self.scaler.update()\r\n\r\n                # Scheduler step\r\n                self.scheduler.step()\r\n\r\n                # Update metrics\r\n                total_loss += loss.item()\r\n                self.global_step += 1\r\n\r\n                # Update progress bar\r\n                pbar.set_postfix({'loss': loss.item()})\r\n\r\n        return total_loss / len(self.train_loader)\r\n\r\n    @torch.no_grad()\r\n    def validate(self) -> float:\r\n        \"\"\"Validate model\"\"\"\r\n        self.model.eval()\r\n        total_loss = 0\r\n\r\n        for encoder_inputs, decoder_inputs, targets in self.val_loader:\r\n            # Move data to device\r\n            encoder_inputs = encoder_inputs.to(self.device)\r\n            decoder_inputs = decoder_inputs.to(self.device)\r\n            targets = targets.to(self.device)\r\n\r\n            # Forward pass\r\n            outputs, _ = self.model(encoder_inputs)\r\n            loss = self.criterion(outputs, targets)\r\n\r\n            total_loss += loss.item()\r\n\r\n        return total_loss / len(self.val_loader)\r\n\r\n    def train(self):\r\n        \"\"\"Main training loop\"\"\"\r\n        self.logger.info(\"Starting training...\")\r\n\r\n        for epoch in range(self.current_epoch, self.config['max_epochs']):\r\n            self.current_epoch = epoch\r\n\r\n            # Training phase\r\n            train_loss = self.train_epoch()\r\n            self.train_losses.append(train_loss)\r\n\r\n            # Validation phase\r\n            val_loss = self.validate()\r\n            self.val_losses.append(val_loss)\r\n\r\n            # Logging\r\n            self.logger.info(\r\n                f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\r\n            )\r\n\r\n            # Check for best model\r\n            is_best = val_loss < self.best_val_loss\r\n            if is_best:\r\n                self.best_val_loss = val_loss\r\n                self.patience_counter = 0\r\n            else:\r\n                self.patience_counter += 1\r\n\r\n            # Save checkpoint\r\n            self.save_checkpoint(is_best)\r\n\r\n            # Early stopping\r\n            if self.patience_counter >= self.config['patience']:\r\n                self.logger.info(\"Early stopping triggered\")\r\n                break\r\n\r\n        self.logger.info(\"Training completed!\")\r\n        return self.train_losses, self.val_losses"
        }
    ]
}