{
    "sourceFile": "src/model/layers.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733312790705,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733339017641,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,36 +37,51 @@\n         \"\"\"Calculate the sparse attention using sampling with fixed memory handling\"\"\"\r\n         B, H, L_K, E = K.shape\r\n         _, _, L_Q, _ = Q.shape\r\n \r\n-        # Calculate the sampled Q_K\r\n-        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\r\n-        index_sample = torch.randint(L_K, (L_Q, sample_k))\r\n-        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\r\n-        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\r\n+        # Sample K for each query instead of expanding K\r\n+        K_expand = []\r\n+        for q_idx in range(L_Q):\r\n+            # Sample random indices for this query\r\n+            index_sample = torch.randint(L_K, (sample_k,), device=K.device)\r\n+            # Get sampled keys\r\n+            K_sample = K[:, :, index_sample, :]\r\n+            K_expand.append(K_sample)\r\n \r\n-        # Find the Top_k query with sparisty measurement\r\n+        # Stack sampled keys\r\n+        K_expand = torch.stack(K_expand, dim=2)  # B x H x L_Q x sample_k x E\r\n+\r\n+        # Calculate Q_K_sample more efficiently\r\n+        Q_expanded = Q.unsqueeze(-2)  # B x H x L_Q x 1 x E\r\n+        Q_K_sample = torch.matmul(Q_expanded, K_expand.transpose(-2, -1))  # B x H x L_Q x 1 x sample_k\r\n+        Q_K_sample = Q_K_sample.squeeze(-2)  # B x H x L_Q x sample_k\r\n+\r\n+        # Find Top_k queries with sparsity measurement\r\n         M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\r\n         M_top = M.topk(n_top, sorted=False)[1]\r\n \r\n         # Use the reduced Q to calculate Q_K\r\n-        # Clone tensors to avoid memory overlap\r\n-        Q_reduce = Q.clone()[torch.arange(B)[:, None, None],\r\n-                           torch.arange(H)[None, :, None],\r\n-                           M_top, :]\r\n+        Q_reduce = Q[torch.arange(B)[:, None, None],\r\n+                    torch.arange(H)[None, :, None],\r\n+                    M_top, :]\r\n         Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\r\n \r\n+        # Clean up to free memory\r\n+        del K_expand, Q_expanded, Q_K_sample, M\r\n+        torch.cuda.empty_cache()\r\n+\r\n         return Q_K, M_top\r\n \r\n     def _get_initial_context(self, V: torch.Tensor, L_Q: int) -> torch.Tensor:\r\n         B, H, L_V, D = V.shape\r\n         if not self.mask_flag:\r\n-            # Clone to avoid memory overlap\r\n-            V_sum = V.mean(dim=-2)\r\n-            context = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\r\n+            # More memory efficient mean calculation\r\n+            V_sum = torch.mean(V, dim=-2, keepdim=True)\r\n+            context = V_sum.expand(B, H, L_Q, D)\r\n         else:\r\n-            # Clone to avoid memory overlap\r\n-            context = V.cumsum(dim=-2).clone()\r\n+            # More memory efficient cumsum\r\n+            context = torch.cumsum(V, dim=-2)\r\n+            context = torch.div(context, torch.arange(1, L_V + 1, device=V.device).view(1, 1, -1, 1))\r\n         return context\r\n \r\n     def forward(self, queries, keys, values, attn_mask):\r\n         B, L_Q, H, D = queries.shape\r\n"
                }
            ],
            "date": 1733312790705,
            "name": "Commit-0",
            "content": "\"\"\"\r\nCustom layers for the Informer model implementation.\r\nReference: https://github.com/zhouhaoyi/Informer2020\r\n\r\nDependencies:\r\n- torch>=2.0.1\r\n- numpy>=1.24.3\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nfrom math import sqrt\r\nfrom typing import Optional, Tuple\r\n\r\nclass ProbAttention(nn.Module):\r\n    \"\"\"\r\n    Probability Attention mechanism with complexity reduction from O(L*L) to O(L*ln(L)).\r\n    \"\"\"\r\n    def __init__(\r\n            self,\r\n            mask_flag: bool = True,\r\n            factor: int = 5,\r\n            scale: Optional[float] = None,\r\n            attention_dropout: float = 0.1,\r\n            output_attention: bool = False\r\n    ):\r\n        super(ProbAttention, self).__init__()\r\n        self.factor = factor\r\n        self.scale = scale\r\n        self.mask_flag = mask_flag\r\n        self.output_attention = output_attention\r\n        self.dropout = nn.Dropout(attention_dropout)\r\n\r\n    def _prob_QK(self, Q: torch.Tensor, K: torch.Tensor, sample_k: int, n_top: int) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"Calculate the sparse attention using sampling with fixed memory handling\"\"\"\r\n        B, H, L_K, E = K.shape\r\n        _, _, L_Q, _ = Q.shape\r\n\r\n        # Calculate the sampled Q_K\r\n        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\r\n        index_sample = torch.randint(L_K, (L_Q, sample_k))\r\n        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\r\n        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\r\n\r\n        # Find the Top_k query with sparisty measurement\r\n        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\r\n        M_top = M.topk(n_top, sorted=False)[1]\r\n\r\n        # Use the reduced Q to calculate Q_K\r\n        # Clone tensors to avoid memory overlap\r\n        Q_reduce = Q.clone()[torch.arange(B)[:, None, None],\r\n                           torch.arange(H)[None, :, None],\r\n                           M_top, :]\r\n        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\r\n\r\n        return Q_K, M_top\r\n\r\n    def _get_initial_context(self, V: torch.Tensor, L_Q: int) -> torch.Tensor:\r\n        B, H, L_V, D = V.shape\r\n        if not self.mask_flag:\r\n            # Clone to avoid memory overlap\r\n            V_sum = V.mean(dim=-2)\r\n            context = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\r\n        else:\r\n            # Clone to avoid memory overlap\r\n            context = V.cumsum(dim=-2).clone()\r\n        return context\r\n\r\n    def forward(self, queries, keys, values, attn_mask):\r\n        B, L_Q, H, D = queries.shape\r\n        _, L_K, _, _ = keys.shape\r\n\r\n        queries = queries.transpose(2, 1)\r\n        keys = keys.transpose(2, 1)\r\n        values = values.transpose(2, 1)\r\n\r\n        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\r\n        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\r\n\r\n        U_part = U_part if U_part < L_K else L_K\r\n        u = u if u < L_Q else L_Q\r\n\r\n        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\r\n\r\n        # Add scale factor\r\n        scale = self.scale or 1./sqrt(D)\r\n        if scale is not None:\r\n            scores_top = scores_top * scale\r\n\r\n        # Get attention and context\r\n        context = self._get_initial_context(values, L_Q)\r\n\r\n        # Update context with selected top_k queries\r\n        scores_top = self.dropout(F.softmax(scores_top, dim=-1))\r\n        \r\n        # Clone context before updating to avoid memory overlap\r\n        context_clone = context.clone()\r\n        context_clone[torch.arange(B)[:, None, None],\r\n                     torch.arange(H)[None, :, None],\r\n                     index, :] = torch.matmul(scores_top, values)\r\n\r\n        return context_clone.transpose(2,1).contiguous(), scores_top if self.output_attention else None\r\n\r\nclass AttentionLayer(nn.Module):\r\n    \"\"\"\r\n    Attention Layer with ProbAttention mechanism.\r\n    \"\"\"\r\n    def __init__(\r\n            self,\r\n            attention: nn.Module,\r\n            d_model: int,\r\n            n_heads: int,\r\n            d_keys: Optional[int] = None,\r\n            d_values: Optional[int] = None\r\n    ):\r\n        super(AttentionLayer, self).__init__()\r\n\r\n        d_keys = d_keys or (d_model//n_heads)\r\n        d_values = d_values or (d_model//n_heads)\r\n\r\n        self.inner_attention = attention\r\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\r\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\r\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)\r\n        self.out_projection = nn.Linear(d_values * n_heads, d_model)\r\n        self.n_heads = n_heads\r\n\r\n    def forward(\r\n            self,\r\n            queries: torch.Tensor,\r\n            keys: torch.Tensor,\r\n            values: torch.Tensor,\r\n            attn_mask: Optional[torch.Tensor] = None\r\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\r\n        B, L, _ = queries.shape\r\n        _, S, _ = keys.shape\r\n        H = self.n_heads\r\n\r\n        queries = self.query_projection(queries).view(B, L, H, -1)\r\n        keys = self.key_projection(keys).view(B, S, H, -1)\r\n        values = self.value_projection(values).view(B, S, H, -1)\r\n\r\n        out, attn = self.inner_attention(\r\n            queries,\r\n            keys,\r\n            values,\r\n            attn_mask\r\n        )\r\n\r\n        out = out.view(B, L, -1)\r\n        return self.out_projection(out), attn"
        }
    ]
}